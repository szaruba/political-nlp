{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#deal with tensors\n",
    "import torch\n",
    "import spacy\n",
    "from torchtext.legacy import data\n",
    "import torchtext.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "#Reproducing same results\n",
    "SEED = 2019\n",
    "\n",
    "#Torch\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "#Cuda algorithms\n",
    "torch.backends.cudnn.deterministic = True"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"de_dep_news_trf\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefan/.virtualenvs/diplom-code-env/lib/python3.8/site-packages/torchtext/data/utils.py:123: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
      "  warnings.warn(f'Spacy model \"{language}\" could not be loaded, trying \"{OLD_MODEL_SHORTCUTS[language]}\" instead')\n"
     ]
    }
   ],
   "source": [
    "TEXT = data.Field(tokenize='spacy',batch_first=True,include_lengths=True)\n",
    "LABEL = data.LabelField(dtype = torch.float)\n",
    "fields = [(None, None),(None, None),(None, None),(None, None),(None, None),(None, None),\n",
    "          ('label', LABEL),\n",
    "          (None, None),(None, None),(None, None),(None, None),(None, None),(None, None),\n",
    "          ('text',TEXT)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "# loading custom dataset\n",
    "training_data=data.TabularDataset(path = 'protocols/labelled/only_lockdown.csv',format = 'csv',fields = fields,skip_header = True,\n",
    "                                  csv_reader_params={'delimiter': '\\t', 'quotechar': None})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "all_samples = [e for e in training_data.examples]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "# Pre-processing\n",
    "# to lower case to match with vector embeddings\n",
    "tmp = []\n",
    "for sample in all_samples:\n",
    "    text = getattr(sample, 'text')\n",
    "    text = [s.lower() for s in text]\n",
    "    setattr(sample, 'text', text)\n",
    "\n",
    "positive = [e for e in all_samples if getattr(e, 'label') == '+']\n",
    "negative = [e for e in all_samples if getattr(e, 'label') == '-']\n",
    "neutral = [e for e in all_samples if getattr(e, 'label') == 'o']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(positive)\n",
    "random.shuffle(negative)\n",
    "random.shuffle(neutral)\n",
    "train_data = data.Dataset(positive[:] + negative[:90] + neutral[:90], fields)\n",
    "valid_data = data.Dataset(negative[90:] + neutral[90:], fields)\n",
    "\n",
    "# train_data, valid_data = training_data.split(split_ratio=0.7, random_state = random.seed(SEED))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of TEXT vocabulary: 3384\n",
      "Size of LABEL vocabulary: 3\n",
      "[(',', 1345), ('die', 595), ('.', 508), ('der', 464), ('und', 457), ('lockdown', 380), ('\\xa0', 310), ('in', 251), ('–', 211), ('den', 201)]\n",
      "defaultdict(<bound method Vocab._default_unk_index of <torchtext.vocab.Vocab object at 0x7f8ae2dbc850>>, {'<unk>': 0, '<pad>': 1, ',': 2, 'die': 3, '.': 4, 'der': 5, 'und': 6, 'lockdown': 7, '\\xa0': 8, 'in': 9, '–': 10, 'den': 11, 'das': 12, 'es': 13, 'ist': 14, 'dass': 15, 'auch': 16, 'i': 17, 'm': 18, 'wir': 19, 'für': 20, 'des': 21, ':': 22, 'haben': 23, 'nicht': 24, 'hat': 25, 'sie': 26, 'zu': 27, 'von': 28, 'lockdowns': 29, 'sind': 30, 'dem': 31, 'ich': 32, 'mit': 33, '-': 34, 'zweiten': 35, 'eine': 36, 'ein': 37, 'einen': 38, 'noch': 39, 'jetzt': 40, 'wie': 41, 'aber': 42, 'wird': 43, 'sich': 44, 'vor': 45, 'werden': 46, 'auf': 47, 'um': 48, 'diese': 49, '„': 50, 'als': 51, 'so': 52, 'an': 53, 'durch': 54, 'am': 55, 'ja': 56, '“': 57, 'schon': 58, 'wieder': 59, 'wenn': 60, 'einem': 61, 'war': 62, 'da': 63, 'ersten': 64, 'sehr': 65, 'man': 66, 'was': 67, ';': 68, 'bei': 69, 'dieser': 70, 'aus': 71, 'nach': 72, 'österreich': 73, 'über': 74, 'uns': 75, 'weil': 76, 'zum': 77, 'maßnahmen': 78, 'während': 79, 'alle': 80, 'budget': 81, 'diesem': 82, 'er': 83, 'nur': 84, 'können': 85, 'heute': 86, 'bis': 87, 'bundesregierung': 88, 'diesen': 89, '!': 90, 'hier': 91, 'oder': 92, 'viele': 93, 'dann': 94, 'ganz': 95, 'zur': 96, 'damit': 97, 'märz': 98, 'regierung': 99, 'unternehmen': 100, 'wurde': 101, 'denn': 102, 'vom': 103, 'frauen': 104, 'gibt': 105, 'seit': 106, 'zeit': 107, '2020': 108, 'dieses': 109, 'einer': 110, 'euro': 111, 'herr': 112, 'immer': 113, 'menschen': 114, 'umsatzersatz': 115, 'mehr': 116, 'nämlich': 117, 'zweite': 118, 'allem': 119, 'aufgrund': 120, 'einmal': 121, 'möchte': 122, 'wochen': 123, 'worden': 124, '2021': 125, 'eines': 126, 'gar': 127, 'kommt': 128, 'lock\\xaddown': 129, 'machen': 130, 'unsere': 131, 'zahlen': 132, '?': 133, 'betriebe': 134, 'etwas': 135, 'gesagt': 136, 'keine': 137, 'krise': 138, 'mir': 139, 'natürlich': 140, 'prozent': 141, 'wirtschaft': 142, 'woche': 143, 'wäre': 144, 'würde': 145, 'betreffend': 146, 'bevölkerung': 147, 'daher': 148, 'hätte': 149, 'kickl': 150, 'kommen': 151, 'waren': 152, 'wissen': 153, ' ': 154, 'ende': 155, 'gefordert': 156, 'harten': 157, 'ihr': 158, 'könnte': 159, 'müssen': 160, 'sondern': 161, 'viel': 162, 'abgeordneten': 163, 'dafür': 164, 'kinder': 165, 'kollegen': 166, 'wirklich': 167, 'alles': 168, 'bereits': 169, 'gerade': 170, 'habe': 171, 'kann': 172, 'kolleginnen': 173, 'november': 174, 'phase': 175, 'sein': 176, 'sowie': 177, 'vielleicht': 178, 'wurden': 179, 'antrag': 180, 'auswirkungen': 181, 'beim': 182, 'beschließen': 183, 'corona': 184, 'eltern': 185, 'finanzminister': 186, 'geht': 187, 'gut': 188, 'ihnen': 189, 'jahr': 190, 'lassen': 191, 'muss': 192, 'möglich': 193, 'sagen': 194, 'situation': 195, 'sollen': 196, 'stehen': 197, 'unter': 198, 'wifo': 199, 'wollen': 200, 'zwei': 201, 'also': 202, 'arbeiten': 203, 'betroffen': 204, 'betroffenen': 205, 'dort': 206, 'fall': 207, 'frage': 208, 'wo': 209, '2': 210, 'anfang': 211, 'covid-19': 212, 'darf': 213, 'davon': 214, 'dazu': 215, 'einfach': 216, 'erst': 217, 'folgen': 218, 'folgenden': 219, 'frau': 220, 'gemacht': 221, 'harte': 222, 'ihre': 223, 'kosten': 224, 'letzten': 225, 'monaten': 226, 'nationalrat': 227, 'pandemie': 228, 'tage': 229, 'zahl': 230, 'allen': 231, 'anderen': 232, 'coronakrise': 233, 'damen': 234, 'dritten': 235, 'eingebracht': 236, 'gastronomie': 237, 'gegeben': 238, 'getan': 239, 'halten': 240, 'herren': 241, 'jene': 242, 'kollege': 243, 'mich': 244, 'milliarden': 245, 'mussten': 246, 'nichts': 247, 'notwendig': 248, 'nun': 249, 'oktober': 250, 'passiert': 251, 'sicherzustellen': 252, 'unser': 253, 'weiter': 254, 'wolle': 255, 'zeitpunkt': 256, 'zuge': 257, 'zwar': 258, '13': 259, 'ab': 260, 'aktuellen': 261, 'bereich': 262, 'bericht': 263, 'besonders': 264, 'braucht': 265, 'debatte': 266, 'erhalten': 267, 'familien': 268, 'frühjahr': 269, 'gab': 270, 'ihren': 271, 'insbesondere': 272, 'keinen': 273, 'kurz': 274, 'land': 275, 'laut': 276, 'lock\\xaddowns': 277, 'millionen': 278, 'nächsten': 279, 'ob': 280, 'offen': 281, 'ohne': 282, 'planungssicherheit': 283, 'schulen': 284, 'unternehmer': 285, 'verhindern': 286, 'wichtig': 287, 'ziel': 288, 'österreichischen': 289, 'övp': 290, '1': 291, 'andere': 292, 'befinden': 293, 'beginn': 294, 'bekommen': 295, 'branchen': 296, 'brauchen': 297, 'bundesgesetz': 298, 'bundeskanzler': 299, 'darüber': 300, 'deshalb': 301, 'dringend': 302, 'fpö': 303, 'gemeinden': 304, 'getroffen': 305, 'große': 306, 'indirekt': 307, 'klar': 308, 'leben': 309, 'light': 310, 'maßnahme': 311, 'meine': 312, 'meiner': 313, 'monate': 314, 'rasch': 315, 'richtig': 316, 'schulschließungen': 317, 'steht': 318, 'stellen': 319, 'tag': 320, 'unterstützung': 321, 'wegen': 322, 'weniger': 323, 'wären': 324, 'überhaupt': 325, '/': 326, '000': 327, '6': 328, 'abholstationen': 329, 'allein': 330, 'aufgefordert': 331, 'betrifft': 332, 'bevor': 333, 'dank': 334, 'drei': 335, 'eigentlich': 336, 'einigen': 337, 'geben': 338, 'gegangen': 339, 'gehabt': 340, 'gekommen': 341, 'genau': 342, 'gesamtwortlaut': 343, 'gesprochen': 344, 'gewesen': 345, 'gilt': 346, 'glaube': 347, 'hätten': 348, 'ihn': 349, 'ihrer': 350, 'ins': 351, 'kunst-': 352, 'leider': 353, 'mal': 354, 'massiv': 355, 'mittlerweile': 356, 'monat': 357, 'raum': 358, 'republik': 359, 'schwierigen': 360, 'sonst': 361, 'stark': 362, 'tatsächlich': 363, 'umgehend': 364, 'unserer': 365, 'vergleich': 366, 'verordnung': 367, 'warum': 368, 'wer': 369, 'zeiten': 370, 'zweiter': 371, 'österreichische': 372, '6,8': 373, '80': 374, 'all': 375, 'aller': 376, 'anders': 377, 'begonnen': 378, 'berücksichtigt': 379, 'besser': 380, 'bewusst': 381, 'branche': 382, 'budgetmittel': 383, 'coronavirus': 384, 'damals': 385, 'daran': 386, 'deswegen': 387, 'deutschland': 388, 'dezember': 389, 'diskussion': 390, 'down': 391, 'eben': 392, 'eingepreist': 393, 'einzige': 394, 'erste': 395, 'euch': 396, 'fixkostenzuschuss': 397, 'folgeschäden': 398, 'freitag': 399, 'ganze': 400, 'gehört': 401, 'geld': 402, 'gesamten': 403, 'gesellschaft': 404, 'gesundheitssystem': 405, 'gleichzeitig': 406, 'grund': 407, 'habt': 408, 'hause': 409, 'heißt': 410, 'helfen': 411, 'herbst': 412, 'hilfe': 413, 'ihrem': 414, 'ihs': 415, 'ii': 416, 'infektionen': 417, 'italien': 418, 'kein': 419, 'konnten': 420, 'kraft': 421, 'lange': 422, 'letzte': 423, 'lock': 424, 'macht': 425, 'minister': 426, 'ministerin': 427, 'nein': 428, 'neos': 429, 'neue': 430, 'neuen': 431, 'pressekonferenz': 432, 'punkt': 433, 'rede': 434, 'sagt': 435, 'schnell': 436, 'setzen': 437, 'soll': 438, 'sommer': 439, 'totalen': 440, 'tourismus': 441, 'trotzdem': 442, 'verfügung': 443, 'wahrscheinlich': 444, 'weg': 445, 'weise': 446, 'wirtschaftlich': 447, 'wirtschaftliche': 448, 'wirtschaftlichen': 449, 'zukunft': 450, 'zusammenhang': 451, '30': 452, '4': 453, '5': 454, '9': 455, 'abend': 456, 'abgebildet': 457, 'abgeordneter': 458, 'absolut': 459, 'angekündigt': 460, 'angerer': 461, 'anlagen': 462, 'arbeit': 463, 'ausmaß': 464, 'ausschließt': 465, 'begründung': 466, 'behauptet': 467, 'beispiel': 468, 'belastung': 469, 'beschlossen': 470, 'beschäftigten': 471, 'betrieb': 472, 'bewilligung': 473, 'bin': 474, 'bitte': 475, 'bleiben': 476, 'bringen': 477, 'budgetausschusses': 478, 'bürger': 479, 'dadurch': 480, 'darauf': 481, 'definitiv': 482, 'denen': 483, 'denke': 484, 'denken': 485, 'dennoch': 486, 'derzeit': 487, 'dessen': 488, 'deutlich': 489, 'distancelearning': 490, 'drozda': 491, 'einnahmen': 492, 'einrichtungen': 493, 'einzelnen': 494, 'entwicklung': 495, 'erinnere': 496, 'ermöglichung': 497, 'erwin': 498, 'folge': 499, 'freie': 500, 'freiheitlichen': 501, 'funktioniert': 502, 'geführt': 503, 'gesorgt': 504, 'gestern': 505, 'geworden': 506, 'grenzen': 507, 'gutes': 508, 'heimischen': 509, 'herinnen': 510, 'heuer': 511, 'hohen': 512, 'hotellerie': 513, 'instrument': 514, 'jahres': 515, 'jenen': 516, 'keiner': 517, 'kleinen': 518, 'klubobmann': 519, 'kollegin': 520, 'konsequenzen': 521, 'künstler': 522, 'künstlerinnen': 523, 'lage': 524, 'leisten': 525, 'leute': 526, 'länder': 527, 'manche': 528, 'medien': 529, 'mein': 530, 'mitte': 531, 'neuerliche': 532, 'neuerlicher': 533, 'nie': 534, 'notwendigen': 535, 'obwohl': 536, 'offensichtlich': 537, 'paar': 538, 'plus': 539, 'rahmen': 540, 'reden': 541, 'regierungsvorlage': 542, 'rund': 543, 'sage': 544, 'samt': 545, 'schaffen': 546, 'schließen': 547, 'schritt': 548, 'schule': 549, 'seine': 550, 'selber': 551, 'shutdown': 552, 'sinne': 553, 'sorgen': 554, 'soziale': 555, 'sozialen': 556, 'sperren': 557, 'staat': 558, 'stelle': 559, 'thema': 560, 'thomas': 561, 'treffen': 562, 'tun': 563, 'uhr': 564, 'umsatzersatzes': 565, 'umzusetzen': 566, 'unserem': 567, 'vergangenen': 568, 'verhängt': 569, 'verloren': 570, 'verluste': 571, 'verschlechtert': 572, 'vier': 573, 'vorgesehen': 574, 'wahrheit': 575, 'weiteren': 576, 'weiterhin': 577, 'weiß': 578, 'welche': 579, 'wien': 580, 'wirtschaftsministerin': 581, 'wohl': 582, 'wollten': 583, 'wort': 584, 'zurück': 585, 'zurückgegangen': 586, 'zwischen': 587, 'öffentlich': 588, 'öffentlichen': 589, 'österreicher': 590, 'österreichs': 591, 'übrigen': 592, '16': 593, '18': 594, '3': 595, '300': 596, '8': 597, 'aktuell': 598, 'alternativlos': 599, 'amazon': 600, 'anderes': 601, 'angesichts': 602, 'anhält': 603, 'april': 604, 'arbeitslosenzahlen': 605, 'aufge\\xadfordert': 606, 'aufholen': 607, 'aufrecht': 608, 'ausgegangen': 609, 'aussendung': 610, 'auswirkt': 611, 'auszubezahlen': 612, 'auszuschließen': 613, 'bedeutet': 614, 'befürchten': 615, 'beginnen': 616, 'beitrag': 617, 'bekannt': 618, 'beraten': 619, 'bereiche': 620, 'beschlos\\xadsen': 621, 'bezug': 622, 'bip': 623, 'bisschen': 624, 'blick': 625, 'blümel': 626, 'bringe': 627, 'buch': 628, 'bundesminister': 629, 'bundesvoranschlages': 630, 'bzw': 631, 'bücher': 632, 'coronalockdown': 633, 'covid': 634, 'damoklesschwert': 635, 'darum': 636, 'daten': 637, 'dauer': 638, 'dauern': 639, 'dies': 640, 'direkt': 641, 'diskutieren': 642, 'diskutiert': 643, 'doch': 644, 'dramatischen': 645, 'draußen': 646, 'drinnen': 647, 'durchgeführt': 648, 'ebenfalls': 649, 'eher': 650, 'einige': 651, 'eins': 652, 'entsprechenden': 653, 'entstanden': 654, 'erleben': 655, 'erreicht': 656, 'erschossen': 657, 'etwa': 658, 'europäischen': 659, 'familie': 660, 'februar': 661, 'finanzonline': 662, 'finanzämter': 663, 'finden': 664, 'flughafen': 665, 'freunden': 666, 'früher': 667, 'fünf': 668, 'gebraucht': 669, 'geehrte': 670, 'gehen': 671, 'geleistet': 672, 'gelungen': 673, 'gemeinsam': 674, 'genauso': 675, 'geregelt': 676, 'gering': 677, 'gesamte': 678, 'geschlossen': 679, 'gesehen': 680, 'gesetzt': 681, 'gesundheitssystems': 682, 'getreten': 683, 'gewusst': 684, 'gleich': 685, 'großen': 686, 'großes': 687, 'grundlage': 688, 'grüne': 689, 'gute': 690, 'hart': 691, 'hatten': 692, 'haus': 693, 'heimische': 694, 'hilfen': 695, 'hoffentlich': 696, 'ihm': 697, 'innerhalb': 698, 'investieren': 699, 'jedem': 700, 'jeden': 701, 'jedoch': 702, 'jetzigen': 703, 'job': 704, 'jänner': 705, 'kam': 706, 'kindern': 707, 'klarstellen': 708, 'kommunikation': 709, 'kulturschaffende': 710, 'kurzarbeit': 711, 'kümmern': 712, 'kürzlich': 713, 'lebensmitteln': 714, 'lesen': 715, 'liegen': 716, 'mahrer': 717, 'massive': 718, 'medial': 719, 'mitarbeiter': 720, 'musste': 721, 'möglichkeit': 722, 'möglichst': 723, 'nachhaltig': 724, 'neben': 725, 'neuerlichen': 726, 'normalität': 727, 'nächste': 728, 'oft': 729, 'online': 730, 'partei': 731, 'pflege': 732, 'planbarkeit': 733, 'profitieren': 734, 'prognose': 735, 'quartals': 736, 'rasche': 737, 'rechnen': 738, 'regelmäßige': 739, 'richtige': 740, 'schauen': 741, 'schaut': 742, 'schellhorn': 743, 'schließung': 744, 'schramböck': 745, 'schüler': 746, 'sehen': 747, 'sei': 748, 'seite': 749, 'sicher': 750, 'sofortige': 751, 'sogar': 752, 'sowohl': 753, 'spiel': 754, 'stattfinden': 755, 'steigen': 756, 'studien': 757, 'tagen': 758, 'teil': 759, 'teillockdown': 760, 'top': 761, 'tragen': 762, 'trifft': 763, 'ug': 764, 'umständen': 765, 'um\\xadsetzung': 766, 'ungefähr': 767, 'unseres': 768, 'unterfertigten': 769, 'unterschieden': 770, 'unterstützen': 771, 'veranstaltungen': 772, 'verantwortung': 773, 'vereinen': 774, 'vergessen': 775, 'verlassen': 776, 'vermeiden': 777, 'verordnete': 778, 'verordneten': 779, 'verschlafen': 780, 'versuchen': 781, 'verursacht': 782, 'vielen': 783, 'volksanwaltschaft': 784, 'vollen': 785, 'wann': 786, 'weit': 787, 'weitere': 788, 'weiterer': 789, 'welle': 790, 'wirtschaftsforscher': 791, 'wirtschaftsforschungsinstitut': 792, 'wirtschaftshilfen': 793, 'wirtschaftskammer': 794, 'wöginger': 795, 'zehn': 796, 'zuerst': 797, 'zulieferer': 798, 'zusammengeschossen': 799, 'zusätzlich': 800, 'zusätzliche': 801, 'österreicherinnen': 802, 'öster\\xadreich': 803, 'überlegt': 804, '‘': 805, '\"': 806, '%': 807, '...': 808, '0': 809, '09.okt': 810, '10': 811, '100': 812, '140': 813, '15': 814, '2,5': 815, '2.000': 816, '20': 817, '4,0': 818, '4,4': 819, '4,5': 820, '500.000': 821, '60': 822, '62': 823, '7': 824, '80prozentigen': 825, '[': 826, ']': 827, 'abgelehnt': 828, 'abgenommen': 829, 'absagen': 830, 'abschließend': 831, 'absehbar': 832, 'abstimmung': 833, 'abzusehen': 834, 'aktuelle': 835, 'andererseits': 836, 'angebot': 837, 'angefangen': 838, 'angenommen': 839, 'angstmacherei': 840, 'anlässlich': 841, 'anschober': 842, 'anspruch': 843, 'anstatt': 844, 'anträge': 845, 'antwort': 846, 'apa0140': 847, 'arbeitslose': 848, 'arbeitslosen': 849, 'arbeitslosigkeit': 850, 'arbeitsmarkpolitische': 851, 'argen': 852, 'aschbacher': 853, 'aufgabe': 854, 'aufgefangen': 855, 'aufgestellt': 856, 'aufholbar': 857, 'aufträge': 858, 'auf\\xadgrund': 859, 'ausgangslage': 860, 'ausgeführt': 861, 'ausgehen': 862, 'ausgelöst': 863, 'ausgeschlossen': 864, 'ausland': 865, 'ausschließen': 866, 'auszusenden': 867, 'badelt': 868, 'bauarbeiter': 869, 'bedanken': 870, 'bedenkt': 871, 'bedingt': 872, 'bedingungen': 873, 'beendet': 874, 'beendigung': 875, 'befindet': 876, 'befreundeter': 877, 'begleitende': 878, 'begleit\\xadmaß\\xadnahmen': 879, 'beispielhaft': 880, 'beisteuern': 881, 'bekämpfung': 882, 'bemühungen': 883, 'benötigen': 884, 'bereichen': 885, 'berichtige': 886, 'berichtigung': 887, 'beschluss': 888, 'beschränkungen': 889, 'beste': 890, 'bestehenden': 891, 'besteht': 892, 'bestellt': 893, 'bestimmt': 894, 'bestimmte': 895, 'besucher': 896, 'betretungsverbote': 897, 'betretungsverboten': 898, 'betrug': 899, 'bewirken': 900, 'bezeichnen': 901, 'beziehungsweise': 902, 'bildung': 903, 'bleibt': 904, 'branchenabhängigen': 905, 'brückl': 906, 'buchhändler': 907, 'budgetverhandlungen': 908, 'budgetären': 909, 'bundes\\xadminister': 910, 'bun\\xaddesregierung': 911, 'ca': 912, 'coronakurzarbeit': 913, 'dankbar': 914, 'danke': 915, 'dankeschön': 916, 'daraus': 917, 'dargelegt': 918, 'davor': 919, 'deren': 920, 'detail': 921, 'dienst': 922, 'dienstag': 923, 'diesbezüglich': 924, 'digitales': 925, 'digitalisierung': 926, 'dinge': 927, 'diskussionen': 928, 'disziplin': 929, 'dramatisch': 930, 'dramatische': 931, 'drastisch': 932, 'dringende': 933, 'dritte': 934, 'durchschlagen': 935, 'durfte': 936, 'dynamik': 937, 'echte': 938, 'effiziente': 939, 'eigenen': 940, 'einbrechen': 941, 'einerseits': 942, 'eingebrochenem': 943, 'eingefordert': 944, 'einig': 945, 'einzelhandel': 946, 'einzig': 947, 'einzigen': 948, 'endet': 949, 'endlich': 950, 'enthalten': 951, 'entschädigung': 952, 'entsprechende': 953, 'entstehen': 954, 'erfahrungen': 955, 'erfolgt': 956, 'erforderliche': 957, 'erholung': 958, 'erklärt': 959, 'ermöglicht': 960, 'ertragsanteile': 961, 'erwarten': 962, 'eure': 963, 'experten': 964, 'falls': 965, 'familienbeihilfe': 966, 'fangen': 967, 'feiern': 968, 'finanziell': 969, 'finanzmittel': 970, 'finanz\\xadminister': 971, 'finde': 972, 'fixkosten\\xadzuschuss': 973, 'fleisch': 974, 'folgender': 975, 'fonds': 976, 'fordere': 977, 'forschung': 978, 'fortgesetzt': 979, 'fr': 980, 'fragen': 981, 'freiheitliche': 982, 'freilich': 983, 'fälle': 984, 'führen': 985, 'führt': 986, 'gagen': 987, 'ganzen': 988, 'ganzer': 989, 'geblieben': 990, 'gedanken': 991, 'geehrten': 992, 'gefahr': 993, 'gefehlt': 994, 'gefor\\xaddert': 995, 'gefragt': 996, 'gefühl': 997, 'gefühlt': 998, 'gegenüber': 999, 'gehalten': 1000, 'gelassen': 1001, 'gelaufen': 1002, 'gelegenheit': 1003, 'gemeint': 1004, 'genommen': 1005, 'genom\\xadmen': 1006, 'genossen': 1007, 'genossinnen': 1008, 'genug': 1009, 'genützt': 1010, 'geringer': 1011, 'geschichte': 1012, 'geschrieben': 1013, 'geschätzt': 1014, 'gesetzliche': 1015, 'gestellt': 1016, 'gestiegen': 1017, 'gesundheit': 1018, 'gesundheitlich': 1019, 'gesundheitsminister': 1020, 'gewerbe': 1021, 'gewährung': 1022, 'gezeigt': 1023, 'ge\\xadstiegen': 1024, 'geändert': 1025, 'gott': 1026, 'greifen': 1027, 'gruppen': 1028, 'größte': 1029, 'größten': 1030, 'halbwegs': 1031, 'handel': 1032, 'handeln': 1033, 'hebein': 1034, 'heldinnen': 1035, 'her': 1036, 'herauszukommen': 1037, 'hermann': 1038, 'herrn': 1039, 'herrscht': 1040, 'herunten': 1041, 'herunterdrücken': 1042, 'heurige': 1043, 'heurigen': 1044, 'hilfspaketen': 1045, 'hilft': 1046, 'hin': 1047, 'hinaus': 1048, 'hinterfragt': 1049, 'hintergrund': 1050, 'hintertür': 1051, 'hoffnung': 1052, 'hohe': 1053, 'härtefallfonds': 1054, 'höhe': 1055, 'höher': 1056, 'ihres': 1057, 'infektionsgeschehen': 1058, 'infektionszahlen': 1059, 'infizierten': 1060, 'inland': 1061, 'inmitten': 1062, 'insofern': 1063, 'international': 1064, 'interview': 1065, 'irgendwie': 1066, 'israel': 1067, 'it': 1068, 'jahre': 1069, 'jahren': 1070, 'jahrzehnte': 1071, 'jeder': 1072, 'juli': 1073, 'kenntnis': 1074, 'kerbe': 1075, 'kinder\\xadbetreuung': 1076, 'kocher': 1077, 'kommenden': 1078, 'kompensation': 1079, 'konkreten': 1080, 'konnte': 1081, 'kontaktlosen': 1082, 'kopf': 1083, 'kultureinrichtungen': 1084, 'kulturminister': 1085, 'kurier': 1086, 'kurzfristig': 1087, 'könnten': 1088, 'lang': 1089, 'langsam': 1090, 'lehrer': 1091, 'lehrerinnen': 1092, 'lehrlingskosten': 1093, 'leicht': 1094, 'leistungen': 1095, 'lernen': 1096, 'letztlich': 1097, 'leuten': 1098, 'liebe': 1099, 'liegt': 1100, 'loacker': 1101, 'lockdownbonus': 1102, 'lokal': 1103, 'luftverschmutzung': 1104, 'ländern': 1105, 'ma': 1106, 'massiven': 1107, 'mediziner': 1108, 'mehrwertsteuersenkungen': 1109, 'meinung': 1110, 'menschenleben': 1111, 'men\\xadschen': 1112, 'mindestens': 1113, 'minus': 1114, 'mitarbeiterinnen': 1115, 'mittel': 1116, 'mitteln': 1117, 'momentan': 1118, 'mögliche': 1119, 'möglicherweise': 1120, 'müll': 1121, 'müsse': 1122, 'müsste': 1123, 'müssten': 1124, 'na': 1125, 'nachlesen': 1126, 'netto': 1127, 'neu': 1128, 'neßler': 1129, 'niveau': 1130, 'nochmals': 1131, 'normalisierung': 1132, 'not': 1133, 'notvergabe': 1134, 'notwendigkeit': 1135, 'nützen': 1136, 'onlinehandel': 1137, 'opposition': 1138, 'per': 1139, 'pflichtschulen': 1140, 'planen': 1141, 'plant': 1142, 'politik': 1143, 'polizisten': 1144, 'polizistinnen': 1145, 'positive': 1146, 'private': 1147, 'probleme': 1148, 'problemen': 1149, 'prognosezeitraum': 1150, 'prozentpunkte': 1151, 'präsident': 1152, 'puls': 1153, 'quartal': 1154, 'quasi': 1155, 'raab': 1156, 'radikale': 1157, 'raten': 1158, 'reagiert': 1159, 'realisiert': 1160, 'redet': 1161, 'reduktion': 1162, 'reduzieren': 1163, 'regierungsparteien': 1164, 'regie\\xadrungsvorlage': 1165, 'reichen': 1166, 'rendi': 1167, 'retten': 1168, 'ringen': 1169, 'rollen': 1170, 'rosa': 1171, 'rücken': 1172, 'saison': 1173, 'scharf': 1174, 'scharfen': 1175, 'schlag': 1176, 'schmeißen': 1177, 'schneller': 1178, 'schreiben': 1179, 'schul': 1180, 'schulden': 1181, 'schulschließungs-': 1182, 'schweden': 1183, 'schweiz': 1184, 'schwerer': 1185, 'schwierig': 1186, 'schwierige': 1187, 'schwierigkeiten': 1188, 'schäden': 1189, 'schön': 1190, 'sechs': 1191, 'selbstständige': 1192, 'selbstverständlich': 1193, 'senken': 1194, 'september': 1195, 'sichergestellt': 1196, 'sicherheit': 1197, 'sicht': 1198, 'sieben': 1199, 'signale': 1200, 'sodass': 1201, 'sogenannte': 1202, 'sogenannten': 1203, 'solche': 1204, 'sommermonate': 1205, 'sozusagen': 1206, 'spannend': 1207, 'sparen': 1208, 'spitäler': 1209, 'spreche': 1210, 'sprechen': 1211, 'spö': 1212, 'spürbar': 1213, 'staaten': 1214, 'starke': 1215, 'startet': 1216, 'stellt': 1217, 'stimmen': 1218, 'ständig': 1219, 'svs': 1220, 'sämtliche': 1221, 'tageszeitung': 1222, 'tiere': 1223, 'tonnen': 1224, 'tool': 1225, 'totallockdown': 1226, 'trinken': 1227, 'täglich': 1228, 'tätigen': 1229, 'umsatz': 1230, 'umsatzentschädigung': 1231, 'umsatzrückgang': 1232, 'umsatzsteuerminderung': 1233, 'umsetzung': 1234, 'umstellung': 1235, 'undenkbar': 1236, 'unklaren': 1237, 'unmittelbar': 1238, 'unnötige': 1239, 'unsicherheit': 1240, 'untergliederung': 1241, 'unternehmerinnen': 1242, 'unterrichten': 1243, 'unterstützt': 1244, 'verehrten': 1245, 'vereinbarte': 1246, 'vereine': 1247, 'vergangene': 1248, 'verhindert': 1249, 'verhinderung': 1250, 'verlangt': 1251, 'verlierer': 1252, 'verlängert': 1253, 'vermehrt': 1254, 'vernunft': 1255, 'verordnet': 1256, 'verordnungen': 1257, 'verschlechternden': 1258, 'verschulden': 1259, 'verschärft': 1260, 'versorgung': 1261, 'verständnis': 1262, 'verstärkt': 1263, 'versucht': 1264, 'vertreter': 1265, 'vieles': 1266, 'vierten': 1267, 'voller': 1268, 'vorgelegt': 1269, 'vorgeschlagen': 1270, 'vorher': 1271, 'vorhinein': 1272, 'vorlegt': 1273, 'vorstellen': 1274, 'vorzulegen': 1275, 'völligen': 1276, 'wagner': 1277, 'warnt': 1278, 'warnte': 1279, 'wegbrechen': 1280, 'weihnachten': 1281, 'weitergehen': 1282, 'weiterlaufen': 1283, 'weitreichende': 1284, 'welt': 1285, 'wenigen': 1286, 'wertschöpfungsverlust': 1287, 'westeuropa': 1288, 'wichtige': 1289, 'wichtiger': 1290, 'wiederholen': 1291, 'wiederum': 1292, 'wirken': 1293, 'wirkt': 1294, 'wirtschaftskrise': 1295, 'wirtschaftsleistung': 1296, 'wirtschaftspolitische': 1297, 'wissenschaft': 1298, 'wobei': 1299, 'wusste': 1300, 'zeigen': 1301, 'zeitung': 1302, 'zitiere': 1303, 'zitiert': 1304, 'zugrunde': 1305, 'zuletzt': 1306, 'zulieferfirmen': 1307, 'zumindest': 1308, 'zurückgehen': 1309, 'zusperren': 1310, 'zustimmung': 1311, 'ändern': 1312, 'ärzte': 1313, 'äußerst': 1314, 'öffentliche': 1315, 'öffnung': 1316, 'überbrückungsfonds': 1317, 'übrigens': 1318, '‚': 1319, '  ': 1320, '-initiativen': 1321, '-trägern': 1322, '-ver\\xadanstalter': 1323, '000er': 1324, '03.11.2020': 1325, '05.nov': 1326, '075': 1327, '095': 1328, '1,1': 1329, '1,6': 1330, '1,8': 1331, '107': 1332, '11': 1333, '11.30': 1334, '117': 1335, '12,4': 1336, '13.3': 1337, '14': 1338, '16.11': 1339, '16.3': 1340, '16.3.2020': 1341, '17': 1342, '183': 1343, '189.000': 1344, '19': 1345, '1950': 1346, '1956': 1347, '1967': 1348, '1979': 1349, '2.11.2020': 1350, '200': 1351, '2020/2021': 1352, '21': 1353, '22': 1354, '24': 1355, '25': 1356, '26': 1357, '28': 1358, '31': 1359, '32': 1360, '40': 1361, '450': 1362, '50': 1363, '500': 1364, '53,7': 1365, '55': 1366, '6.12': 1367, '600': 1368, '613': 1369, '7,7': 1370, '800': 1371, '82': 1372, '900': 1373, '936': 1374, 'a': 1375, 'abarbeitung': 1376, 'abbildung': 1377, 'abbringen': 1378, 'abdeckt': 1379, 'abfederung': 1380, 'abfertigungsgesetz': 1381, 'abgefedert': 1382, 'abgeflacht': 1383, 'abgehalten': 1384, 'abgeholt': 1385, 'abgeschafft': 1386, 'abgesehen': 1387, 'abgestritten': 1388, 'abgewiesen': 1389, 'abschluss': 1390, 'absicherung': 1391, 'absolute': 1392, 'abwandern': 1393, 'abwehren': 1394, 'abwicklung': 1395, 'abzuschaffen': 1396, 'abzuschätzen': 1397, 'abzuwenden': 1398, 'abzuwickeln': 1399, 'abänderungsantrag': 1400, 'abänderungsbudget': 1401, 'acht': 1402, 'adaptiert': 1403, 'adressiert': 1404, 'afrikanische': 1405, 'agiert': 1406, 'akontozahlung': 1407, 'aktienkurse': 1408, 'akzeptiert': 1409, 'alleingang': 1410, 'alleinige': 1411, 'allerdings': 1412, 'allerseelen': 1413, 'alt': 1414, 'alter': 1415, 'alternativen': 1416, 'amtskollegin': 1417, 'analog': 1418, 'anbelangt': 1419, 'anbietern': 1420, 'anblick': 1421, 'anderem': 1422, 'ande\\xadren': 1423, 'anfragen': 1424, 'angeblich': 1425, 'angeblichen': 1426, 'angebote': 1427, 'angeboten': 1428, 'angeführt': 1429, 'angehörigen': 1430, 'angelegtes': 1431, 'angeordnet': 1432, 'angesetzten': 1433, 'angesteckt': 1434, 'angestellt': 1435, 'angestiegen': 1436, 'angesucht': 1437, 'angewiesen': 1438, 'ange\\xadwiesen': 1439, 'angriff': 1440, 'ankommen': 1441, 'ankündigungen': 1442, 'anlaufenden': 1443, 'anmerken': 1444, 'annäherungsverbote': 1445, 'anonym': 1446, 'anpassungen': 1447, 'anrechnung': 1448, 'anregen': 1449, 'ans': 1450, 'ansatzweise': 1451, 'anschaue': 1452, 'anschaut': 1453, 'anscheinend': 1454, 'ansetzt': 1455, 'ansteigen': 1456, 'anstrengungen': 1457, 'ansuchen': 1458, 'ante': 1459, 'antragslos': 1460, 'antragslose': 1461, 'antragsteller': 1462, 'anzubieten': 1463, 'anzukündigen': 1464, 'apa0170': 1465, 'apotheken': 1466, 'arbeitenden': 1467, 'arbeitnehmer': 1468, 'arbeitnehmerinnen': 1469, 'arbeitslos': 1470, 'arbeitslosengeld': 1471, 'arbeitslosen\\xadzahlen': 1472, 'arbeitsmarkt': 1473, 'arbeitsmarktpolitik': 1474, 'arbeitsministerin': 1475, 'arbeitsplatz': 1476, 'arbeitsplätze': 1477, 'arbeitsprozesse': 1478, 'arbeitswelt': 1479, 'arbeitszeit': 1480, 'arbeits\\xadlosen\\xadzahlen': 1481, 'arme': 1482, 'armutsfalle': 1483, 'art': 1484, 'arzneimittelversorgung': 1485, 'arztpraxen': 1486, 'ar\\xadbeitslosigkeit': 1487, 'attes\\xadtiert': 1488, 'aufeinander': 1489, 'auffällt': 1490, 'aufgehoben': 1491, 'aufgepasst': 1492, 'aufgesetzt': 1493, 'aufhorchen': 1494, 'aufhört': 1495, 'auflachen': 1496, 'auflagen': 1497, 'aufmerksamkeit': 1498, 'aufrufe': 1499, 'aufrufen': 1500, 'aufs': 1501, 'aufseiten': 1502, 'aufwärtsgegangen': 1503, 'aufzufangen': 1504, 'aufzustellen': 1505, 'auf\\xadgefordert': 1506, 'augen': 1507, 'august': 1508, 'ausbezahlt': 1509, 'ausbreitung': 1510, 'ausbricht': 1511, 'ausdehnung': 1512, 'ausgangbeschränkungen': 1513, 'ausgangsbeschränkung': 1514, 'ausgangsbeschränkungen': 1515, 'ausgangsverbote': 1516, 'ausganssperren': 1517, 'ausgebrochen': 1518, 'ausgeht': 1519, 'ausgerechnet': 1520, 'ausgereicht': 1521, 'ausgerufen': 1522, 'ausgestalten': 1523, 'ausgeweitet': 1524, 'ausgewirkt': 1525, 'ausgezeichnet': 1526, 'ausgleich': 1527, 'ausgleichen': 1528, 'auslastungszahlen': 1529, 'ausliefern': 1530, 'ausländerbeschäftigungsge\\xadsetz': 1531, 'ausreichend': 1532, 'ausreichende': 1533, 'ausreichender': 1534, 'ausschuss': 1535, 'aussieht': 1536, 'aussteigen': 1537, 'ausstrahlt': 1538, 'auswirkung': 1539, 'auszunutzen': 1540, 'aus\\xadreichende': 1541, 'aus\\xadsehen': 1542, 'außen': 1543, 'außenministeriums': 1544, 'außenminis\\xadterium': 1545, 'außenstellen': 1546, 'außerhalb': 1547, 'barbara': 1548, 'basieren': 1549, 'basis': 1550, 'bauernhof': 1551, 'bauern\\xadfamilien': 1552, 'beamten': 1553, 'beamtinnen': 1554, 'beam\\xadten': 1555, 'beantragen': 1556, 'beantragt': 1557, 'bean\\xadtragen': 1558, 'bearbeitet': 1559, 'beatmungsgeräte': 1560, 'bedarf': 1561, 'bedeuten': 1562, 'bedeutung': 1563, 'bedrohung': 1564, 'befeuern': 1565, 'befindlichen': 1566, 'befristete': 1567, 'befürchtet': 1568, 'begegnen': 1569, 'beginnend': 1570, 'beginnt': 1571, 'begleiten': 1572, 'begleitet': 1573, 'begleitforschung': 1574, 'begleit\\xadforschung': 1575, 'begründet': 1576, 'begrüßen': 1577, 'begrüßte': 1578, 'behandeln': 1579, 'behandelt': 1580, 'behandlungsrückstau': 1581, 'behaupten': 1582, 'beherbergungsbetriebe': 1583, 'beherber\\xadgungs\\xadbetriebe': 1584, 'beherzt': 1585, 'behördenweg': 1586, 'behördlich': 1587, 'beiden': 1588, 'beigetragen': 1589, 'beihilfenrecht': 1590, 'beihilfen\\xadrecht\\xadlichen': 1591, 'beispielsweise': 1592, 'beitragen': 1593, 'beiträgt': 1594, 'bekanntlich': 1595, 'bekenntnis': 1596, 'beklagte': 1597, 'bekommt': 1598, 'belakowitsch': 1599, 'belastet': 1600, 'belastungen': 1601, 'belgien': 1602, 'belvedere': 1603, 'bemerkung': 1604, 'benötigtes': 1605, 'beobachten': 1606, 'beratung': 1607, 'beratungen': 1608, 'beratungsinformationen': 1609, 'berechnet': 1610, 'berechnungen': 1611, 'berechtigt': 1612, 'bereiten': 1613, 'bereitgestellt': 1614, 'bereitschaft': 1615, 'bereitzustellen': 1616, 'berichtet': 1617, 'berichtete': 1618, 'beruf': 1619, 'berufe': 1620, 'berufliche': 1621, 'berücksichtigen': 1622, 'berücksichtigung': 1623, 'berühren': 1624, 'beschlossene': 1625, 'beschwerden': 1626, 'beschäftigt': 1627, 'beschäftigtenstand': 1628, 'beschäftigung': 1629, 'beschäftigungspaket': 1630, 'besondere': 1631, 'besonderes': 1632, 'beson\\xadders': 1633, 'besorgniserregende': 1634, 'bessere': 1635, 'besseren': 1636, 'bestanden': 1637, 'besten': 1638, 'bestens': 1639, 'bestmögliche': 1640, 'bestätigt': 1641, 'besuche': 1642, 'besucherinnen': 1643, 'besucherrückgängen': 1644, 'betonen': 1645, 'betont': 1646, 'betrachtung': 1647, 'betrachtungszeitraum': 1648, 'betreiben': 1649, 'betretungsverbot': 1650, 'betretungsverbo\\xadte': 1651, 'betreuen': 1652, 'betreut': 1653, 'betrieben': 1654, 'betriebsschließungen': 1655, 'betroffene': 1656, 'betrof\\xadfen': 1657, 'beträchtlich': 1658, 'beurteilt': 1659, 'beur\\xadteilen': 1660, 'bevorstehenden': 1661, 'bevorsteht': 1662, 'bewegung': 1663, 'bewegungsmangel': 1664, 'beweist': 1665, 'bewertung': 1666, 'bewiesen': 1667, 'bewirkt': 1668, 'bewältigbar': 1669, 'bezahlten': 1670, 'beziffert': 1671, 'bezirken': 1672, 'bezogen': 1673, 'be\\xadginn': 1674, 'be\\xadlieben': 1675, 'be\\xadtreffen': 1676, 'be\\xadtroffenen': 1677, 'be\\xadvölkerung': 1678, 'bier': 1679, 'bieten': 1680, 'bilanz': 1681, 'bilden': 1682, 'bilder': 1683, 'bildungsbereich': 1684, 'bildungslockdown': 1685, 'bildungssystems': 1686, 'bildungsverluste': 1687, 'bildungszeit': 1688, 'binnen': 1689, 'birgit': 1690, 'bisher': 1691, 'bitten': 1692, 'blase': 1693, 'bleibe': 1694, 'blindflug': 1695, 'blödsinn': 1696, 'bombardieren': 1697, 'books': 1698, 'botschaften': 1699, 'botschafter': 1700, 'brau\\xadchen': 1701, 'brechen': 1702, 'brief': 1703, 'bringt': 1704, 'bruttoinlandsprodukt': 1705, 'bräuchte': 1706, 'bräuchten': 1707, 'buchhandel': 1708, 'buchhändlerin\\xadnen': 1709, 'buchkultur': 1710, 'budgetausschuss': 1711, 'budgetbericht': 1712, 'budgetdebatte': 1713, 'budgetpolitik': 1714, 'budgetrede': 1715, 'budgets': 1716, 'budget\\xadausschusses': 1717, 'budget\\xadhearing': 1718, 'budgetänderung': 1719, 'budgetär': 1720, 'bud\\xadgets': 1721, 'bud\\xadgetär': 1722, 'bund': 1723, 'bundesfinanzrahmengesetz': 1724, 'bundeskanzleramt': 1725, 'bundeskanzlers': 1726, 'bundesländerspezifische': 1727, 'bundesmuseen': 1728, 'bundesparteiobmann': 1729, 'bundesvoran\\xadschlages': 1730, 'bundes\\xadkanzler': 1731, 'buschenschanken': 1732, 'börsencrash': 1733, 'bürgerinnen': 1734, 'bürgermeisterkollegen': 1735, 'canberra': 1736, 'caritas': 1737, 'cash': 1738, 'chance': 1739, 'chancenindex': 1740, 'chaos': 1741, 'chef': 1742, 'chefcoronaleugner': 1743, 'china': 1744, 'christen': 1745, 'christoph': 1746, 'chronologie': 1747, 'circa': 1748, 'comebackzuschuss': 1749, 'computer-': 1750, 'coronaausnahmesituation': 1751, 'coronabedingtem': 1752, 'coronainfizierte': 1753, 'coronakommission': 1754, 'coronapandemie': 1755, 'coronapolitik': 1756, 'coronasituation': 1757, 'coronaviruspandemie': 1758, 'coronazeit': 1759, 'co\\xadronatoten': 1760, 'co\\xadvid-19': 1761, 'cyberinfrastruktur': 1762, 'dabei': 1763, 'dagegengestimmt': 1764, 'dahin': 1765, 'dahinter': 1766, 'danach': 1767, 'danebenliegt': 1768, 'danken': 1769, 'darangesetzt': 1770, 'dargestellt': 1771, 'darstellte': 1772, 'darzustellen': 1773, 'dasselbe': 1774, 'datenbasis': 1775, 'datenerhebungen': 1776, 'datenerhe\\xadbun\\xadgen': 1777, 'dazusagen': 1778, 'de': 1779, 'debatten': 1780, 'defizite': 1781, 'demnach': 1782, 'denkt': 1783, 'derartigen': 1784, 'derartiges': 1785, 'derer': 1786, 'derjenige': 1787, 'derselben': 1788, 'details': 1789, 'deutsch\\xadland': 1790, 'dich': 1791, 'dienen': 1792, 'dienstes': 1793, 'dienstrechtsgesetz': 1794, 'diesbezüg\\xadlich': 1795, 'diesmal': 1796, 'dietmar': 1797, 'die\\xadsem': 1798, 'digi\\xadtalisierung': 1799, 'dimension': 1800, 'dingen': 1801, 'diskursen': 1802, 'distance': 1803, 'doppelresidenz': 1804, 'doppelten': 1805, 'dop\\xadpelte': 1806, 'dorthin': 1807, 'dortigen': 1808, 'draht': 1809, 'dramas': 1810, 'draufhaben': 1811, 'dreier': 1812, 'dreifache': 1813, 'dringlichkeit': 1814, 'drittel': 1815, 'drohen': 1816, 'drohenden': 1817, 'drängt': 1818, 'du': 1819, 'durchaus': 1820, 'durchgehen': 1821, 'durchgekommen': 1822, 'durchgesetzt': 1823, 'durchgezogen': 1824, 'durchzubringen': 1825, 'durch\\xadführt': 1826, 'durch\\xadge\\xadführt': 1827, 'dürfen': 1828, 'e': 1829, 'ebendiesen': 1830, 'ebenso': 1831, 'effekt': 1832, 'effizienter': 1833, 'egal': 1834, 'eh': 1835, 'eigene': 1836, 'eigenes': 1837, 'eigenlob': 1838, 'eigentlichen': 1839, 'eigent\\xadlich': 1840, 'eigenver\\xadantwortung': 1841, 'einbringen': 1842, 'einbringt': 1843, 'einbruch': 1844, 'einbußen': 1845, 'eindeutig': 1846, 'eindruck': 1847, 'eindrucksvoll': 1848, 'eindrücklich': 1849, 'eindämmung': 1850, 'einfacher': 1851, 'einforderten': 1852, 'eingegangen': 1853, 'eingehe': 1854, 'eingehen': 1855, 'eingerichteten': 1856, 'eingeschränkt': 1857, 'eingeschränkten': 1858, 'eingesprungen': 1859, 'eingetretene': 1860, 'eingetroffen': 1861, 'eingriffe': 1862, 'einhalten': 1863, 'einhaltung': 1864, 'einhorn': 1865, 'einiges': 1866, 'einkaufsmöglichkeiten': 1867, 'einkommensverlust': 1868, 'einmalzahlung': 1869, 'einnahmenentfall': 1870, 'einpendeln': 1871, 'einreichmodalitäten': 1872, 'einschenken': 1873, 'einschneidend': 1874, 'einschränken': 1875, 'einschätzung': 1876, 'einstellen': 1877, 'einstellung': 1878, 'einstimmig': 1879, 'eintragungswoche': 1880, 'einverneh\\xadmen': 1881, 'einzelne': 1882, 'einzudämmen': 1883, 'einzuführen': 1884, 'einzugreifen': 1885, 'einzupreisen': 1886, 'ein\\xadfordern': 1887, 'elefant': 1888, 'elf': 1889, 'empfinden': 1890, 'empörung': 1891, 'endes': 1892, 'endgerät': 1893, 'energieträger': 1894, 'enorm': 1895, 'enorme': 1896, 'enormen': 1897, 'enten': 1898, 'entfernt': 1899, 'entgeltsicherungsge\\xadsetz': 1900, 'entlastung': 1901, 'entscheiden': 1902, 'entscheidung': 1903, 'entscheidungsstärke': 1904, 'entscheidungs\\xadfindung': 1905, 'entschei\\xaddungen': 1906, 'entschließen': 1907, 'entschließungsantrag': 1908, 'entschädigungsanspruch': 1909, 'entschädi\\xadgungs\\xadanspruch': 1910, 'entschä\\xaddigung': 1911, 'entsprechend': 1912, 'entspricht': 1913, 'entsteht': 1914, 'entwickelt': 1915, 'entwicklungen': 1916, 'entwick\\xadlungen': 1917, 'entwurf': 1918, 'epidemieg': 1919, 'epidemiegesetz': 1920, 'erfahrung': 1921, 'erfolgen': 1922, 'erforderlich': 1923, 'erforderlichen': 1924, 'erfreulich': 1925, 'erfüllen': 1926, 'erhalt': 1927, 'erhebliche': 1928, 'erhebung': 1929, 'erhöhen': 1930, 'erinnern': 1931, 'erinnert': 1932, 'erinnerung': 1933, 'erkennens': 1934, 'erkenntnissen': 1935, 'erkrankten': 1936, 'erlauben': 1937, 'erlebt': 1938, 'erledigen': 1939, 'erledigt': 1940, 'ermöglichen': 1941, 'ernst': 1942, 'erratisches': 1943, 'erreichbar': 1944, 'errichten': 1945, 'erschließt': 1946, 'erstes': 1947, 'ersuche': 1948, 'erwachsenen': 1949, 'erweiterten': 1950, 'erweiterung': 1951, 'erwirtschaften': 1952, 'erwischt': 1953, 'erwähnen': 1954, 'erwähnt': 1955, 'erwähnung': 1956, 'erzwungenen': 1957, 'essen': 1958, 'europa': 1959, 'europaweit': 1960, 'eu\\xadropäischen': 1961, 'event-': 1962, 'evidenz': 1963, 'evidenzbasierte': 1964, 'evidenz\\xadbasierte': 1965, 'existenz': 1966, 'existenzielle': 1967, 'expertise': 1968, 'exporte': 1969, 'exportgenehmigungen': 1970, 'externe': 1971, 'facto': 1972, 'fair': 1973, 'faktor': 1974, 'fallen': 1975, 'falsch': 1976, 'falschen': 1977, 'familienlastenausgleichsgesetz': 1978, 'familienleistungen': 1979, 'familienverbandes': 1980, 'familien\\xadbeihilfe': 1981, 'fantasiebudget': 1982, 'fantasiezahlen': 1983, 'fast': 1984, 'fatal': 1985, 'fatalen': 1986, 'fatz': 1987, 'fehlen': 1988, 'fehlende': 1989, 'fehler': 1990, 'fehlt': 1991, 'fest': 1992, 'festhalten': 1993, 'feststellung': 1994, 'film-': 1995, 'finanziellen': 1996, 'finanzierungsgesetz': 1997, 'finanzministers': 1998, 'finanzrahmen': 1999, 'finanzverwaltung': 2000, 'finanz\\xadministerium': 2001, 'findet': 2002, 'fiskus': 2003, 'fitnessstudiobetreiber': 2004, 'fixkostenzuschusses': 2005, 'fkz': 2006, 'fleißig': 2007, 'flächendeckende': 2008, 'flächendeckenden': 2009, 'folgekosten': 2010, 'folgende': 2011, 'folgendes': 2012, 'folgt': 2013, 'folgte': 2014, 'forderung': 2015, 'form': 2016, 'formate': 2017, 'formen': 2018, 'formulieren': 2019, 'formuliert': 2020, 'formulierung': 2021, 'fortzusetzen': 2022, 'fossile': 2023, 'frappant': 2024, 'frauenbild': 2025, 'frauenministerin': 2026, 'frauenmi\\xadnisterin': 2027, 'frechheit': 2028, 'freiberufler': 2029, 'freiheitsrechte': 2030, 'freischaffenden': 2031, 'freiwillige': 2032, 'frequente': 2033, 'frequenz': 2034, 'freue': 2035, 'freunderl': 2036, 'freundinnen': 2037, 'freundliches': 2038, 'freut': 2039, 'froh': 2040, 'frühjahres': 2041, 'frühling': 2042, 'fuchs': 2043, 'funktion': 2044, 'furchtbar': 2045, 'fällen': 2046, 'förderangebote': 2047, 'förderpaket': 2048, 'förderungen': 2049, 'för\\xadderungen': 2050, 'fühlen': 2051, 'führend': 2052, 'fürchterlichen': 2053, 'ganzes': 2054, 'ganzjahresmaskerade': 2055, 'gas': 2056, 'gastronomiebetrieben': 2057, 'gast\\xadronomie': 2058, 'geantwortet': 2059, 'gebe': 2060, 'gebieten': 2061, 'gebracht': 2062, 'gebrachte': 2063, 'gebun\\xadden': 2064, 'gebührt': 2065, 'gedacht': 2066, 'geehrter': 2067, 'geeignet': 2068, 'geeigneten': 2069, 'gefunden': 2070, 'geführte': 2071, 'gegeißelt': 2072, 'gegen': 2073, 'gegend': 2074, 'gegenteil': 2075, 'geglaubt': 2076, 'gehaltsgesetz': 2077, 'gehe': 2078, 'geheißen': 2079, 'gehend': 2080, 'gehofft': 2081, 'gekennzeichnet': 2082, 'geklärt': 2083, 'gekostet': 2084, 'gekündigt': 2085, 'gelernt': 2086, 'gelin\\xaddere': 2087, 'geltende': 2088, 'gemeindeamt': 2089, 'gemeinsamen': 2090, 'gemeinsames': 2091, 'genannt': 2092, 'generalsekretär': 2093, 'genereller': 2094, 'gene\\xadralsekretär': 2095, 'genießen': 2096, 'gepflegt': 2097, 'gepredigt': 2098, 'gerasselt': 2099, 'geredet': 2100, 'gerettet': 2101, 'gerichtet': 2102, 'geringe': 2103, 'geringeren': 2104, 'geringeres': 2105, 'gerne': 2106, 'gernot': 2107, 'gerutscht': 2108, 'gerüchte': 2109, 'gerückt': 2110, 'gesamter': 2111, 'gesamtes': 2112, 'geschaffen': 2113, 'gescheites': 2114, 'geschlechtern': 2115, 'geschlittert': 2116, 'geschlossenen': 2117, 'geschnürt': 2118, 'geschuldet': 2119, 'geschwiegen': 2120, 'geschwungen': 2121, 'geschätzte': 2122, 'gesellschaftlich': 2123, 'gesell\\xadschaftliche': 2124, 'gesessen': 2125, 'gesetze': 2126, 'gesetzlichen': 2127, 'gespart': 2128, 'gespräche': 2129, 'gesprächen': 2130, 'gestalten': 2131, 'gestiegenen': 2132, 'gestreut': 2133, 'gesund': 2134, 'gesundheitsbereich': 2135, 'gesundheitsberufen': 2136, 'gesundheitsbudget': 2137, 'gesundheitskrise': 2138, 'gesundheitsministerium': 2139, 'gesundheitsministeriums': 2140, 'gesundheitspolitisch': 2141, 'gesundheitspolitische': 2142, 'gesundheitswesen': 2143, 'gesundheitszustand': 2144, 'gesund\\xadheitsausschuss': 2145, 'gesund\\xadheitsversorgung': 2146, 'gesund\\xadheitszustand': 2147, 'gesund\\xadheits\\xadminister': 2148, 'getragen': 2149, 'getrennt': 2150, 'getwittert': 2151, 'gewalt': 2152, 'gewaltbereitschaft': 2153, 'gewalteskalationen': 2154, 'gewerbetreibenden': 2155, 'gewinnen': 2156, 'gewisser': 2157, 'gewissheit': 2158, 'gewählt': 2159, 'gewähren': 2160, 'gewährleisten': 2161, 'gewährleistet': 2162, 'gezielte': 2163, 'ge\\xadnannt': 2164, 'ge\\xadschlossenen': 2165, 'ge\\xadschlos\\xadsenen': 2166, 'ge\\xadsellschaft': 2167, 'ge\\xadsundheitswesen': 2168, 'ge\\xadwalt': 2169, 'ge\\xadzielte': 2170, 'gib': 2171, 'ging': 2172, 'glaskugel': 2173, 'glauben': 2174, 'glaubt': 2175, 'gnadenhöfen': 2176, 'gp': 2177, 'grenzschließung': 2178, 'grenzschließun\\xadgen': 2179, 'griff': 2180, 'grippalen': 2181, 'grippeepidemie': 2182, 'groß': 2183, 'großer': 2184, 'großflächi\\xadgen': 2185, 'großindustrie': 2186, 'großteil': 2187, 'großteils': 2188, 'grund-': 2189, 'grundkonsens': 2190, 'grundlegenden': 2191, 'grund\\xadsätzlich': 2192, 'größer': 2193, 'gründe': 2194, 'gründen': 2195, 'grünen': 2196, 'guten': 2197, 'gänge': 2198, 'halbjahr': 2199, 'halt': 2200, 'handelns': 2201, 'handels': 2202, 'handwerk': 2203, 'hantel': 2204, 'harald': 2205, 'harter': 2206, 'hatte': 2207, 'hauptausschuss': 2208, 'hauptausschus\\xadses': 2209, 'hauptlast': 2210, 'hauptprofiteur': 2211, 'haushalt': 2212, 'haustür': 2213, 'ha\\xadben': 2214, 'heftige': 2215, 'hehl': 2216, 'heimischer': 2217, 'heimi\\xadschen': 2218, 'helden': 2219, 'heraus': 2220, 'herausforderung': 2221, 'herausfor\\xadde\\xadrung': 2222, 'herausgekommen': 2223, 'herausgestellt': 2224, 'herauskommen': 2225, 'herauszubringen': 2226, 'hereingebrochen': 2227, 'hergeben': 2228, 'hergehen': 2229, 'herhalten': 2230, 'herrschte': 2231, 'herstellen': 2232, 'herumzutesten': 2233, 'herunterfahren': 2234, 'hervorragende': 2235, 'herzliches': 2236, 'her\\xadstellen': 2237, 'heutigem': 2238, 'heutigen': 2239, 'hey': 2240, 'he\\xadrumfantasiert': 2241, 'hiezu': 2242, 'hilfestellungen': 2243, 'hilfsfonds': 2244, 'hilfsmaßnahme': 2245, 'hilfsmaßnahmen': 2246, 'hilfsorganisationen': 2247, 'hinausgeht': 2248, 'hinein': 2249, 'hineingegangen': 2250, 'hineinzuschreiben': 2251, 'hingedeutet': 2252, 'hingestellt': 2253, 'hingewiesen': 2254, 'hinsicht': 2255, 'hinsichtlich': 2256, 'hinstellt': 2257, 'hinstolpert': 2258, 'hinterfragen': 2259, 'hinterlassen': 2260, 'hinweis': 2261, 'hinüberturnen': 2262, 'hochfährt': 2263, 'hochschulen': 2264, 'hoffe': 2265, 'hoher': 2266, 'home': 2267, 'homeoffice': 2268, 'homeschooling': 2269, 'hotels': 2270, 'humanitäre': 2271, 'hunderttausende': 2272, 'hut': 2273, 'hygienemaßnahmen': 2274, 'hält': 2275, 'hände': 2276, 'hängen': 2277, 'hängt': 2278, 'häusliche': 2279, 'häuslichen': 2280, 'höchstens': 2281, 'höchstnotwendige': 2282, 'höhepunkt': 2283, 'höllentempo': 2284, 'höre': 2285, 'hören': 2286, 'hört': 2287, 'hörte': 2288, 'iden': 2289, 'iger': 2290, 'immerhin': 2291, 'impfstoff': 2292, 'impfung': 2293, 'indem': 2294, 'indexierung': 2295, 'indikator': 2296, 'industrie': 2297, 'infekte': 2298, 'infektionsfälle': 2299, 'infektionskrankheit': 2300, 'infektionsrate': 2301, 'infektionsraten': 2302, 'infektionswerte': 2303, 'infizierte': 2304, 'informationen': 2305, 'infrastrukturellen': 2306, 'initiiert': 2307, 'inländischen': 2308, 'innenministerium': 2309, 'insgesamt': 2310, 'insolvenz': 2311, 'institutionen': 2312, 'integrieren': 2313, 'intensiv': 2314, 'intensivbetten': 2315, 'intensive': 2316, 'interdisziplinäre': 2317, 'interessant': 2318, 'interessante': 2319, 'interessanterweise': 2320, 'investition': 2321, 'investitions-': 2322, 'investment': 2323, 'inves\\xadtitionsanreizen': 2324, 'inves\\xadtitionsprämie': 2325, 'irgendeinem': 2326, 'irgendetwas': 2327, 'irgendet\\xadwas': 2328, 'irgendwelche': 2329, 'irgendwelchen': 2330, 'italienischen': 2331, 'jahresende': 2332, 'jahreswechsel': 2333, 'je': 2334, 'jede': 2335, 'jederzeit': 2336, 'jedes': 2337, 'jemand': 2338, 'jener': 2339, 'jobs': 2340, 'josef': 2341, 'journalist': 2342, 'jugendliche': 2343, 'jugendlichen': 2344, 'jugendnotruf': 2345, 'juliauszahlung': 2346, 'juni': 2347, 'jux': 2348, 'jährliches': 2349, 'kamen': 2350, 'kameras': 2351, 'kanada': 2352, 'kaniak': 2353, 'kannten': 2354, 'kapazitäten': 2355, 'kapazitäts\\xadgrenzen': 2356, 'karl': 2357, 'katastrophal': 2358, 'katastrophe': 2359, 'katastrophenhilfe': 2360, 'katholischen': 2361, 'kauf': 2362, 'kaum': 2363, 'keck': 2364, 'kehren': 2365, 'kehrt': 2366, 'keil': 2367, 'keinster': 2368, 'keller': 2369, 'kellnern': 2370, 'kennen': 2371, 'khm': 2372, 'kinder-': 2373, 'kinderrechte': 2374, 'klarheit': 2375, 'kleine': 2376, 'kleineren': 2377, 'klub\\xadobmann': 2378, 'kohle': 2379, 'kollateralschaden': 2380, 'kollegin\\xadnen': 2381, 'komme': 2382, 'kommenwird': 2383, 'kompensiert': 2384, 'kompe\\xadtenzverteilung': 2385, 'komplett': 2386, 'kompletten': 2387, 'kom\\xadmen': 2388, 'kom\\xadmission': 2389, 'konsequent': 2390, 'konsequente': 2391, 'konstant': 2392, 'kontakt': 2393, 'kontakte': 2394, 'kontaktlose': 2395, 'kontakt\\xadlose': 2396, 'kontext': 2397, 'kontraproduktiv': 2398, 'konzeption': 2399, 'kostendeckungen': 2400, 'kostenlos': 2401, 'kranken': 2402, 'krankenkassen': 2403, 'kreative': 2404, 'kriege': 2405, 'kriegen': 2406, 'krisen': 2407, 'krisenbewältigung': 2408, 'krisenbewältigungsmodus': 2409, 'krisenmanagement': 2410, 'kritik': 2411, 'kritische': 2412, 'kritischen': 2413, 'kritisieren': 2414, 'kritisiert': 2415, 'kuala': 2416, 'kucher': 2417, 'kuchl': 2418, 'kultur-': 2419, 'kulturbetriebe': 2420, 'kulturbetrieben': 2421, 'kulturbudget': 2422, 'kulturinstitutionen': 2423, 'kulturschaffenden': 2424, 'kultur\\xadbranche': 2425, 'kunden': 2426, 'kundenkontakte': 2427, 'kundinnen': 2428, 'kunsthistorische': 2429, 'kurzarbeitsregelung': 2430, 'kurze': 2431, 'kurzem': 2432, 'kurzfristigen': 2433, 'käme': 2434, 'kämpfen': 2435, 'könne': 2436, 'könnt': 2437, 'könn\\xadten': 2438, 'kündigungswelle': 2439, 'künstlerin': 2440, 'künstlersozialversicherung': 2441, 'künstlichen': 2442, 'kürze': 2443, 'landegenehmigungen': 2444, 'landen': 2445, 'landeten': 2446, 'langen': 2447, 'langfristig': 2448, 'langfristigen': 2449, 'lasse': 2450, 'lasten': 2451, 'lauer': 2452, 'laufen': 2453, 'laufenden': 2454, 'learning': 2455, 'lebenseinkommen': 2456, 'lebensmittel': 2457, 'lebenssituation': 2458, 'lehnt': 2459, 'lehre': 2460, 'lehrer_innen': 2461, 'lehrpersonal': 2462, 'leichten': 2463, 'leiden': 2464, 'leistungsträgerinnen': 2465, 'leis\\xadtungsfähig': 2466, 'letztendlich': 2467, 'letzter': 2468, 'level': 2469, 'licht-': 2470, 'lieber': 2471, 'lieblingslokal': 2472, 'ließen': 2473, 'lifte': 2474, 'lima': 2475, 'literatur': 2476, 'live': 2477, 'lockdownforderer': 2478, 'lockdownkompensation': 2479, 'lockdownkrise': 2480, 'lockdownmodus': 2481, 'lockdownszenario': 2482, 'lockdownverlierern': 2483, 'lockdownzeit': 2484, 'lockdownzeiten': 2485, 'lockdown\\xadverlierer': 2486, 'lockdownähnlichen': 2487, 'lockert': 2488, 'lockerungen': 2489, 'lock\\xaddownphase': 2490, 'lokale': 2491, 'lokals': 2492, 'lumpur': 2493, 'lustig': 2494, 'lächeln': 2495, 'länger': 2496, 'lässt': 2497, 'lösen': 2498, 'lösung': 2499, 'machbar': 2500, 'machbaren': 2501, 'machtfülle': 2502, 'mag': 2503, 'mahnte': 2504, 'mai': 2505, 'makulatur': 2506, 'management': 2507, 'mann': 2508, 'mantel': 2509, 'man\\xadche': 2510, 'man\\xadcherlei': 2511, 'marke': 2512, 'martin': 2513, 'maskenball': 2514, 'maskenpflicht': 2515, 'massenimpfung': 2516, 'massentests': 2517, 'massentestung': 2518, 'material': 2519, 'materialkosten': 2520, 'matznetter': 2521, 'maximal': 2522, 'ma\\xadchen': 2523, 'maßgeblich': 2524, 'maßnahmengesetz': 2525, 'maßnahmengesetzes': 2526, 'maßnah\\xadmen': 2527, 'maß\\xadnahmen': 2528, 'medizinerinnen': 2529, 'medizinischen': 2530, 'medizinisches': 2531, 'medizinprodukten': 2532, 'mehrfach': 2533, 'mehrheitsfraktion': 2534, 'mehrzahl': 2535, 'meilenweit': 2536, 'meinem': 2537, 'meinen': 2538, 'meinungen': 2539, 'meinungsfreiheit': 2540, 'meinungshoheit': 2541, 'meisten': 2542, 'meldete': 2543, 'menge': 2544, 'menschen\\xadexperiment': 2545, 'merken': 2546, 'merkt': 2547, 'methode': 2548, 'mikrofon': 2549, 'min': 2550, 'mindeststandards': 2551, 'minimieren': 2552, 'minizahlung': 2553, 'minuten': 2554, 'min\\xaddeststandards': 2555, 'misstraut': 2556, 'missverständnisse': 2557, 'miteinander': 2558, 'miteinbeziehung': 2559, 'mitgegangen': 2560, 'mitgeteilt': 2561, 'mitgetragen': 2562, 'mitge\\xadstimmt': 2563, 'mitgliedern': 2564, 'mitleidenschaft': 2565, 'mitteilen': 2566, 'mittel-': 2567, 'mitternacht': 2568, 'mittleren': 2569, 'mittler\\xadweile': 2570, 'mittwoch': 2571, 'mitzuüberweisen': 2572, 'mi\\xadnus': 2573, 'modellierung': 2574, 'modellierungsversuchen': 2575, 'moder\\xadnem': 2576, 'momentanen': 2577, 'monatelang': 2578, 'motto': 2579, 'multiarbeit': 2580, 'museum': 2581, 'mutter': 2582, 'mythen': 2583, 'möglichen': 2584, 'möglicher': 2585, 'möglichkeiten': 2586, 'nachbarländer': 2587, 'nachdenkt': 2588, 'nachfolgende': 2589, 'nachgehen': 2590, 'nachgeliefert': 2591, 'nachhinein': 2592, 'nachstehenden': 2593, 'nachstehender': 2594, 'nachste\\xadhenden': 2595, 'nacht': 2596, 'nachverfolgung': 2597, 'nachvollziehbarer': 2598, 'nach\\xadste\\xadhen\\xadden': 2599, 'nase': 2600, 'nationalbank': 2601, 'nationalrates': 2602, 'nebenjob': 2603, 'negative': 2604, 'negativen': 2605, 'nehmen': 2606, 'netz': 2607, 'neuausrichtung': 2608, 'neuer': 2609, 'neues': 2610, 'neuinfektionen': 2611, 'nieder': 2612, 'niedergelassener': 2613, 'niedrige': 2614, 'niemals': 2615, 'niemandem': 2616, 'nord-': 2617, 'nordamerika': 2618, 'nor\\xadmalität': 2619, 'notmaßnahmenverordnung': 2620, 'notwen\\xaddigkeit': 2621, 'not\\xadwendig': 2622, 'nov': 2623, 'novellierung': 2624, 'novem\\xadber': 2625, 'npo': 2626, 'nullpunkt': 2627, 'nunmehr': 2628, 'nächstes': 2629, 'nächtigungen': 2630, 'nötig': 2631, 'nötigen': 2632, 'oberstufe': 2633, 'oberstufen': 2634, 'obfrau': 2635, 'of': 2636, 'oftmals': 2637, 'ohnehin': 2638, 'operationen': 2639, 'opfer': 2640, 'oppositionsparteien': 2641, 'oppo\\xadsition': 2642, 'oppo\\xadsitionsparteien': 2643, 'orf': 2644, 'organisiert': 2645, 'orga\\xadni\\xadsationen': 2646, 'originalverpackt': 2647, 'ort': 2648, 'ortschaften': 2649, 'ost-': 2650, 'ots': 2651, 'paare': 2652, 'paket': 2653, 'pamela': 2654, 'pannen': 2655, 'pan\\xaddemie': 2656, 'papierkübel': 2657, 'parlament': 2658, 'parlaments': 2659, 'parlamentsklubs': 2660, 'parteipolitischen': 2661, 'passieren': 2662, 'pauschalität': 2663, 'peking': 2664, 'pension': 2665, 'pensionsraub': 2666, 'perfide': 2667, 'permanent': 2668, 'personal': 2669, 'perspektive': 2670, 'perspek\\xadtiven': 2671, 'persönlich': 2672, 'phantasiert': 2673, 'phasen': 2674, 'planungsarbeiten': 2675, 'plattform': 2676, 'pleite-': 2677, 'plenums': 2678, 'plätze': 2679, 'plötzlich': 2680, 'plötzliche': 2681, 'podium': 2682, 'politiker': 2683, 'politischen': 2684, 'polizeistaatlich': 2685, 'pop': 2686, 'portas': 2687, 'positiven': 2688, 'positives': 2689, 'preis': 2690, 'preise': 2691, 'presse': 2692, 'pressekonferenzen': 2693, 'pres\\xadseaussendungen': 2694, 'privatbereich': 2695, 'privaten': 2696, 'privatleben': 2697, 'pro': 2698, 'problem': 2699, 'produktion': 2700, 'produktionen': 2701, 'produzieren': 2702, 'prof.': 2703, 'profiteure': 2704, 'projektaufgaben': 2705, 'projektbezogen': 2706, 'prominenten': 2707, 'protokoll': 2708, 'prozentigen': 2709, 'pro\\xadzentigen': 2710, 'prämien': 2711, 'präsentiert': 2712, 'präsidenten': 2713, 'präventivmaßnahme': 2714, 'psychische': 2715, 'psychologischen': 2716, 'puls-4': 2717, 'pult': 2718, 'punkte': 2719, 'pöttinger': 2720, 'quaken': 2721, 'qualität': 2722, 'quartiere': 2723, 'quer\\xadschnittstestungen': 2724, 'rascher': 2725, 'rat': 2726, 'ratz': 2727, 'reaktion': 2728, 'realistische': 2729, 'rechnet': 2730, 'rechnungen': 2731, 'rechte': 2732, 'rechtsanspruch': 2733, 'rechtzeitig': 2734, 'rechtzeitigen': 2735, 'redner': 2736, 'regeln': 2737, 'regelung': 2738, 'regen': 2739, 'regierungsexperte': 2740, 'regierungslinie': 2741, 'regierungsseite': 2742, 'region': 2743, 'regionalen': 2744, 'reguläre': 2745, 'rehabilitationen': 2746, 'reiben': 2747, 'reibungslos': 2748, 'reinen': 2749, 'reise': 2750, 'reisebeschränkungen': 2751, 'reisebüros': 2752, 'reisewarnungen': 2753, 'reisewar\\xadnungen': 2754, 'relativ': 2755, 'renate': 2756, 'rennt': 2757, 'reparatur': 2758, 'repara\\xadtur': 2759, 'ressortministern': 2760, 'return': 2761, 'revidiert': 2762, 'rezession': 2763, 'richtet': 2764, 'richtigen': 2765, 'richtigerweise': 2766, 'richtiges': 2767, 'richtigstellen': 2768, 'richtung': 2769, 'rigorosen': 2770, 'risikoszenario': 2771, 'rolle': 2772, 'ruhe': 2773, 'runden': 2774, 'runtergefahren': 2775, 'rückgang': 2776, 'rückholaktion': 2777, 'rückschritte': 2778, 'rückverweis': 2779, 'rückwirkend': 2780, 'rückwirkung': 2781, 'sache': 2782, 'sachen': 2783, 'sachliche': 2784, 'sagst': 2785, 'salamitaktik': 2786, 'sammeln': 2787, 'sanfte': 2788, 'sanften': 2789, 'sanfter': 2790, 'sanitäter': 2791, 'sanitäterinnen': 2792, 'satz': 2793, 'sa\\xadgen': 2794, 'schaden': 2795, 'schadenersatz': 2796, 'schadenersatzes': 2797, 'schallmauer': 2798, 'schallmeiner': 2799, 'schall\\xadmauer': 2800, 'scharfer': 2801, 'schau\\xaden': 2802, 'scha\\xadden': 2803, 'scheichelbauer': 2804, 'schlagartig': 2805, 'schlage': 2806, 'schlange': 2807, 'schlecht': 2808, 'schlechter': 2809, 'schlechtwetterentschädigungsgesetz': 2810, 'schließlich': 2811, 'schließt': 2812, 'schließ\\xadlich': 2813, 'schloss': 2814, 'schlug': 2815, 'schluss': 2816, 'schlussappell': 2817, 'schlussberatungen': 2818, 'schlägt': 2819, 'schmerzhaft': 2820, 'schmidhofer': 2821, 'schnelle': 2822, 'schnellen': 2823, 'schnellstmögliche': 2824, 'schnöde': 2825, 'schooling': 2826, 'schrittweise': 2827, 'schrittweisen': 2828, 'schullockdown': 2829, 'schul\\xadlockdown': 2830, 'schuster': 2831, 'schwarzen': 2832, 'schwebt': 2833, 'schwer': 2834, 'schwe\\xadre': 2835, 'schwierigkeit': 2836, 'schwierigsten': 2837, 'schwächere': 2838, 'schwätzen': 2839, 'schätzungen': 2840, 'schönes': 2841, 'schöngerechnet': 2842, 'schüler_innen': 2843, 'schülerinnen': 2844, 'sebastian': 2845, 'sechswöchigen': 2846, 'sehe': 2847, 'seid': 2848, 'seien': 2849, 'seilbahnbereich': 2850, 'seinem': 2851, 'seiner': 2852, 'seines': 2853, 'seitdem': 2854, 'seiten': 2855, 'seitens': 2856, 'sekundarstufe': 2857, 'selben': 2858, 'selbstständigen': 2859, 'sensibilisierungs\\xadkampagne': 2860, 'sepp': 2861, 'setzt': 2862, 'sichergehen': 2863, 'sicherstellt': 2864, 'signalisieren': 2865, 'signalisiert': 2866, 'signifikant': 2867, 'simulationsexperte': 2868, 'sinken': 2869, 'sinnbefreit': 2870, 'sinnvoll': 2871, 'sitzung': 2872, 'skigebiete': 2873, 'sofortigen': 2874, 'sofortiger': 2875, 'soft': 2876, 'solch': 2877, 'solchen': 2878, 'soldaten': 2879, 'soldatinnen': 2880, 'solide': 2881, 'solides': 2882, 'solle': 2883, 'sollte': 2884, 'sollten': 2885, 'sonderberichte': 2886, 'sonderbetreuung': 2887, 'sonderbetreuungszeit': 2888, 'sonderbetreuungszeiten': 2889, 'sonderprüfungen': 2890, 'sonstige': 2891, 'sorge': 2892, 'sorgt': 2893, 'sozial': 2894, 'sozialausschuss': 2895, 'soziales': 2896, 'sozialversicherung': 2897, 'sozialversicherungen': 2898, 'so\\xaddass': 2899, 'spaltung': 2900, 'sparring\\xadpartner': 2901, 'sparte': 2902, 'spaß': 2903, 'spezielle': 2904, 'spielen': 2905, 'spitzenkandidatinnen': 2906, 'spitzensport': 2907, 'spitzen\\xadmonaten': 2908, 'spitälern': 2909, 'sport': 2910, 'sprache': 2911, 'springende': 2912, 'spuren': 2913, 'spät': 2914, 'später': 2915, 'spätestens': 2916, 'spä\\xadter': 2917, 'spüren': 2918, 'staates': 2919, 'staatshilfen': 2920, 'staat\\xadlichen': 2921, 'stadium': 2922, 'standen': 2923, 'starten': 2924, 'statt': 2925, 'stattdessen': 2926, 'stattfindet': 2927, 'stattgefunden': 2928, 'statt\\xadfindet': 2929, 'statt\\xadgefunden': 2930, 'stau\\xaddamm': 2931, 'stecken': 2932, 'steigenden': 2933, 'steigerung': 2934, 'steigt': 2935, 'steiner': 2936, 'stellenabbau\\xadprogram\\xadme': 2937, 'steuer': 2938, 'steuerliche': 2939, 'stimmt': 2940, 'stim\\xadmen': 2941, 'stoß': 2942, 'strategie': 2943, 'straucheln': 2944, 'straße': 2945, 'strengeren': 2946, 'strengster': 2947, 'stresses': 2948, 'streut': 2949, 'studie': 2950, 'studierende': 2951, 'stunden': 2952, 'stur': 2953, 'sturmgewehr': 2954, 'stärker': 2955, 'stärksten': 2956, 'stütze': 2957, 'substanziell': 2958, 'suchen': 2959, 'super': 2960, 'susanne': 2961, 'systematisch': 2962, 'systemrelevante': 2963, 'system\\xaderhalterinnen': 2964, 'sämtlicher': 2965, 'sätze': 2966, 'säule': 2967, 'säulen': 2968, 'südamerika': 2969, 'südosteuropa': 2970, 'tagaus': 2971, 'tagein': 2972, 'tagesordnungspunkt': 2973, 'takes': 2974, 'tatsächliche': 2975, 'tatsächlichen': 2976, 'tech\\xadnisch': 2977, 'teilnahme': 2978, 'teilnimmt': 2979, 'teilweise': 2980, 'teilzeit': 2981, 'teil\\xadweise': 2982, 'telefonisch': 2983, 'testprogramme': 2984, 'testungen': 2985, 'themas': 2986, 'tiefschlaf': 2987, 'tierwohlge\\xadrechtes': 2988, 'tirana': 2989, 'tirol': 2990, 'tiroler': 2991, 'titelte': 2992, 'todesfälle': 2993, 'todesfällen': 2994, 'todesstoß': 2995, 'tollerei': 2996, 'tontechniker': 2997, 'tore': 2998, 'totale': 2999, 'tote': 3000, 'toten': 3001, 'tourismusausschusses': 3002, 'tourismussprecherin': 3003, 'tradition': 3004, 'traditionelle': 3005, 'traditionellen': 3006, 'trauen': 3007, 'treffe': 3008, 'tritt': 3009, 'trotz': 3010, 'trump': 3011, 'tu': 3012, 'tut': 3013, 'tv': 3014, 'tägliche': 3015, 'tätig': 3016, 'tätigkeit': 3017, 'türkisen': 3018, 'u.a': 3019, 'umfangreich': 3020, 'umfassenden': 3021, 'umfassender': 3022, 'umfeld': 3023, 'umformen': 3024, 'umfragewerte': 3025, 'umgegangen': 3026, 'umgehen': 3027, 'umgesetzt': 3028, 'umsatzausfall': 3029, 'umsatzeinbruch': 3030, 'umsatzes': 3031, 'umsatzteile': 3032, 'umsatz\\xadersatz': 3033, 'umso': 3034, 'umsätze': 3035, 'umwelteinflüsse': 3036, 'umweltförderungsgesetz': 3037, 'umzustellen': 3038, 'unausgegorenen': 3039, 'unbedingt': 3040, 'unbegreiflicher': 3041, 'unbemerkte': 3042, 'unbürokratisch': 3043, 'unbürokratische': 3044, 'unbüro\\xadkratisch': 3045, 'unerklärlichen': 3046, 'unge\\xadrecht': 3047, 'unglaublichen': 3048, 'ungleichheit': 3049, 'union': 3050, 'unmit\\xadtelbar': 3051, 'unnötiger': 3052, 'unpfändbarkeit': 3053, 'unplanbarkeit': 3054, 'unschuldig': 3055, 'unschuldige': 3056, 'unseren': 3057, 'unse\\xadre': 3058, 'untergehen': 3059, 'unternehmensunterstützungen': 3060, 'unternehmer_innen': 3061, 'unternehmerlohn': 3062, 'unternehmern': 3063, 'unternommen': 3064, 'unterricht': 3065, 'unterrichtsstunden': 3066, 'untersagt': 3067, 'unterschiedliche': 3068, 'unterschiedlichste': 3069, 'unterschriften': 3070, 'unterstufe': 3071, 'unterstützungen': 3072, 'unterstützungseinrichtungen': 3073, 'untersuchungen': 3074, 'unterwegs': 3075, 'unterzeichnenden': 3076, 'unter\\xadricht': 3077, 'unter\\xadschiede': 3078, 'unter\\xadstützen': 3079, 'untätigkeit': 3080, 'unverantwortlich': 3081, 'unverzüglich': 3082, 'unzulänglich': 3083, 'unzählige': 3084, 'unübersehbar': 3085, 'up': 3086, 'urlaub': 3087, 'urlaubs-': 3088, 'usa': 3089, 'varianten': 3090, 'veraltet': 3091, 'verankert': 3092, 'veranstaltungs': 3093, 'veranstaltungsbranche': 3094, 'verantwortlich': 3095, 'verantwor\\xadtungsträger': 3096, 'verbessert': 3097, 'verbindungen': 3098, 'verboten': 3099, 'verbreitet': 3100, 'verbunden': 3101, 'verbundenen': 3102, 'verdienstentgang': 3103, 'vereinfachen': 3104, 'vereinsmitglieder': 3105, 'verfassungswidrig': 3106, 'vergangen': 3107, 'verhandeln': 3108, 'verharmlost': 3109, 'verheerend': 3110, 'verhin\\xaddert': 3111, 'verhängen': 3112, 'verhängung': 3113, 'verkaufen': 3114, 'verkündeten': 3115, 'verlas\\xadsen': 3116, 'verlegen': 3117, 'verliehen': 3118, 'verlieren': 3119, 'verliererinnen': 3120, 'verlorene': 3121, 'verlustausgleich': 3122, 'verlängern': 3123, 'verlängerung': 3124, 'verlässlichkeit': 3125, 'vermeintliche': 3126, 'vermisst': 3127, 'vermutlich': 3128, 'verordnungsermächtigungen': 3129, 'versagens': 3130, 'versagerinnen': 3131, 'versammeln': 3132, 'versandhandel': 3133, 'versandkosten': 3134, 'verschafft': 3135, 'verschieben': 3136, 'verschlechtern': 3137, 'verschoben': 3138, 'verschärfen': 3139, 'verschärfter': 3140, 'verschärfung': 3141, 'versorgungsengpässe': 3142, 'verspätet': 3143, 'verstanden': 3144, 'verstehen': 3145, 'verstärkung': 3146, 'versäumnisse': 3147, 'versäumt': 3148, 'vertragsbediensteten': 3149, 'vertrauten': 3150, 'verunsichern': 3151, 'verunsicherung': 3152, 'vervierfacht': 3153, 'verwalten': 3154, 'verwechselt': 3155, 'verwirklichen': 3156, 'verzapft': 3157, 'verzug': 3158, 'ver\\xadfügung': 3159, 'ver\\xadordnete': 3160, 'ver\\xadsprochen': 3161, 'vieler': 3162, 'vielfach': 3163, 'vielfaches': 3164, 'vielfalt': 3165, 'vielzahl': 3166, 'viermal': 3167, 'virusleugner': 3168, 'vizekanzler': 3169, 'volksbegehren': 3170, 'volkspartei': 3171, 'volksschule': 3172, 'volkswirtschaft': 3173, 'volks\\xadwirtschaft': 3174, 'voll': 3175, 'vollkommen': 3176, 'vonseiten': 3177, 'vorab': 3178, 'vorankommen': 3179, 'vorbei': 3180, 'vorbereitet': 3181, 'vorbereitungsmaßnahmen': 3182, 'vorbildland': 3183, 'vorgelegte': 3184, 'vorgenommen': 3185, 'vorgesetzten': 3186, 'vorhin': 3187, 'vorjahr': 3188, 'vorjahresmonats': 3189, 'vorjahres\\xadzeitraum': 3190, 'vorlage': 3191, 'vorrednerin': 3192, 'vorreiter': 3193, 'vorschau': 3194, 'vorschlag': 3195, 'vorschlägt': 3196, 'vorsichtige': 3197, 'vorteil': 3198, 'vorteile': 3199, 'vorwurf': 3200, 'vor\\xadherrschen': 3201, 'völlig': 3202, 'völliger': 3203, 'wachstum': 3204, 'wachstumsprognosen': 3205, 'wagen': 3206, 'wahnsinn': 3207, 'wahr': 3208, 'wahrgenommen': 3209, 'wahr\\xadscheinlich': 3210, 'wand': 3211, 'wandel': 3212, 'warnen': 3213, 'weggeblieben': 3214, 'weggebrochen': 3215, 'wegge\\xadbrochen': 3216, 'wegnehmen': 3217, 'wegweisungen': 3218, 'wehgetan': 3219, 'wehrlos': 3220, 'weihnachts\\xadgeschäft': 3221, 'wein': 3222, 'weiterentwickelt': 3223, 'weitergeht': 3224, 'weiters': 3225, 'weiter\\xadgelaufen': 3226, 'welcher': 3227, 'welches': 3228, 'weltweiten': 3229, 'welt\\xadweiten': 3230, 'wen': 3231, 'wenden': 3232, 'wenig': 3233, 'wenige': 3234, 'wenigstens': 3235, 'werde': 3236, 'wert': 3237, 'wertschöpfungsketten': 3238, 'wert\\xadschöpfungskette': 3239, 'wesentlich': 3240, 'wesentliche': 3241, 'wesentlichen': 3242, 'wesent\\xadlichen': 3243, 'westbahn': 3244, 'we\\xadge': 3245, 'whatever': 3246, 'wichtigen': 3247, 'wichtigsten': 3248, 'widersprechen': 3249, 'wiederherzustellen': 3250, 'wien.orf.at': 3251, 'wiener': 3252, 'will': 3253, 'willkommen': 3254, 'willkürlich': 3255, 'winter': 3256, 'wintertourismus': 3257, 'wirkenden': 3258, 'wirklichkeit': 3259, 'wirksam': 3260, 'wirksame': 3261, 'wirksamkeit': 3262, 'wirkung': 3263, 'wirtschaftsausschuss': 3264, 'wirtschaftsexperte': 3265, 'wirtschaftsforschern': 3266, 'wirtschaftsprognosen': 3267, 'wirtschaftsreparaturpaket': 3268, 'wirtschaftssystem': 3269, 'wirt\\xadschaft': 3270, 'wissend': 3271, 'wissenschaftliche': 3272, 'wissenschaftsfreiheit': 3273, 'wissensstand': 3274, 'wissentlich': 3275, 'wko': 3276, 'wkö': 3277, 'wohle': 3278, 'wollt': 3279, 'worten': 3280, 'wozu': 3281, 'wucht': 3282, 'wunderschön': 3283, 'wunsch': 3284, 'wurm': 3285, 'wussten': 3286, 'wäh\\xadrend': 3287, 'wände': 3288, 'wöchentlich': 3289, 'wünschen': 3290, 'würden': 3291, 'wüssten': 3292, 'zadić': 3293, 'zahlenmäßig': 3294, 'zahlenwerk': 3295, 'zahlung': 3296, 'zah\\xadlenwerk': 3297, 'zeche': 3298, 'zehntausende': 3299, 'zeigt': 3300, 'zeitge\\xadrecht': 3301, 'zeitgleich': 3302, 'zeitungen': 3303, 'zeitverwendungsstudie': 3304, 'zentral': 3305, 'zerreißen': 3306, 'zertrümmert': 3307, 'zickzackkurs': 3308, 'ziehen': 3309, 'zielgruppen': 3310, 'ziemlich': 3311, 'zitieren': 3312, 'zittern': 3313, 'zoom': 3314, 'zudrehen': 3315, 'zufolge': 3316, 'zugestimmt': 3317, 'zugleich': 3318, 'zugute': 3319, 'zulieferbetrieben': 3320, 'zumutbaren': 3321, 'zunimmt': 3322, 'zunächst': 3323, 'zurecht': 3324, 'zurechtrücken': 3325, 'zurückgelehnt': 3326, 'zurückgenommen': 3327, 'zurückkehren': 3328, 'zurückrutschen': 3329, 'zurückversetzt': 3330, 'zusammen': 3331, 'zusammenbringen': 3332, 'zusammengebracht': 3333, 'zusammengekom\\xadmen': 3334, 'zusammenspiel': 3335, 'zusam\\xadmenbruch': 3336, 'zusam\\xadmenhang': 3337, 'zuschriften': 3338, 'zuschuss': 3339, 'zustrom': 3340, 'zustände': 3341, 'zuständen': 3342, 'zuständig': 3343, 'zusätzlichen': 3344, 'zu\\xadrückgehen': 3345, 'zwei\\xadten': 3346, '\\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0 ': 3347, 'ähnlich': 3348, 'ähnliche': 3349, 'ähnliches': 3350, 'ärztinnen': 3351, 'äu\\xadßerst': 3352, 'äußersten': 3353, 'öbb': 3354, 'öffnete': 3355, 'öfters': 3356, 'ökonomisch': 3357, 'österreichern': 3358, 'österreiche\\xadrinnen': 3359, 'österreichi\\xadschen': 3360, 'österreichweit': 3361, 'ös\\xadterreich': 3362, 'überall': 3363, 'überarbeiten': 3364, 'überaus': 3365, 'überbordenden': 3366, 'überbrückungsfinanzierung': 3367, 'überholt': 3368, 'überlagert': 3369, 'überlastung': 3370, 'übernehmen': 3371, 'übernervöse': 3372, 'überprüfung': 3373, 'überschreiten': 3374, 'überschritten': 3375, 'überstanden': 3376, 'überwiegenden': 3377, 'überzeugt': 3378, 'über\\xadgriffe': 3379, 'über\\xadlastet': 3380, 'üblich': 3381, '”': 3382, '•': 3383})\n"
     ]
    }
   ],
   "source": [
    "#initialize glove embeddings\n",
    "# the custom vectors we use in de_glove_deepsetai.txt have 300 dimensions\n",
    "de_embeddings = torchtext.vocab.Vectors(name = 'vectors/de_glove_deepsetai.txt')\n",
    "TEXT.build_vocab(training_data,min_freq=1,vectors = de_embeddings)\n",
    "LABEL.build_vocab(training_data)\n",
    "\n",
    "#No. of unique tokens in text\n",
    "print(\"Size of TEXT vocabulary:\",len(TEXT.vocab))\n",
    "\n",
    "#No. of unique tokens in label\n",
    "print(\"Size of LABEL vocabulary:\",len(LABEL.vocab))\n",
    "\n",
    "#Commonly used words\n",
    "print(TEXT.vocab.freqs.most_common(10))\n",
    "\n",
    "#Word dictionary\n",
    "print(TEXT.vocab.stoi)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "#check whether cuda is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#set batch size\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "#Load an iterator\n",
    "train_iterator, valid_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    sort_key = lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    "    device = device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "for x in train_iterator:\n",
    "    y = x\n",
    "    break\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([2, 0, 2, 0, 1, 0, 0, 1, 0, 2, 2, 2, 2, 1, 0, 1])"
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.label.type(torch.LongTensor)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class classifier(nn.Module):\n",
    "\n",
    "    #define all the layers used in model\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers,\n",
    "                 bidirectional, dropout):\n",
    "\n",
    "        #Constructor\n",
    "        super().__init__()\n",
    "\n",
    "        #embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        #lstm layer\n",
    "        self.lstm = nn.LSTM(embedding_dim,\n",
    "                           hidden_dim,\n",
    "                           num_layers=n_layers,\n",
    "                           bidirectional=bidirectional,\n",
    "                           dropout=dropout,\n",
    "                           batch_first=True)\n",
    "\n",
    "        #dense layer\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "\n",
    "        #activation function\n",
    "        self.act = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, text, text_lengths):\n",
    "\n",
    "        #text = [batch size,sent_length]\n",
    "        embedded = self.embedding(text)\n",
    "        #embedded = [batch size, sent_len, emb dim]\n",
    "\n",
    "        #packed sequence\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths,batch_first=True)\n",
    "\n",
    "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
    "        #hidden = [batch size, num layers * num directions,hid dim]\n",
    "        #cell = [batch size, num layers * num directions,hid dim]\n",
    "\n",
    "        #concat the final forward and backward hidden state\n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n",
    "\n",
    "        #hidden = [batch size, hid dim * num directions]\n",
    "        dense_outputs=self.fc(hidden)\n",
    "\n",
    "        #Final activation function\n",
    "        outputs=self.act(dense_outputs)\n",
    "\n",
    "        return outputs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "#define hyperparameters\n",
    "size_of_vocab = len(TEXT.vocab)\n",
    "embedding_dim = 300  # the custom vectors we use in de_glove_deepsetai.txt have 300 dimensions\n",
    "num_hidden_nodes = 32\n",
    "num_output_nodes = 3  # we have 3 different labels (-/o/+) so we need 3 output nodes.\n",
    "num_layers = 2\n",
    "bidirection = True\n",
    "dropout = 0.2\n",
    "\n",
    "#instantiate the model\n",
    "model = classifier(size_of_vocab, embedding_dim, num_hidden_nodes,num_output_nodes, num_layers,\n",
    "                   bidirectional = True, dropout = dropout)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier(\n",
      "  (embedding): Embedding(3384, 300)\n",
      "  (lstm): LSTM(300, 32, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "  (fc): Linear(in_features=64, out_features=3, bias=True)\n",
      "  (act): Sigmoid()\n",
      ")\n",
      "The model has 1,125,987 trainable parameters\n",
      "torch.Size([3384, 300])\n"
     ]
    }
   ],
   "source": [
    "#architecture\n",
    "print(model)\n",
    "\n",
    "#No. of trianable parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "\n",
    "#Initialize the pretrained embedding\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "print(pretrained_embeddings.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "#define optimizer and loss\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#define metric\n",
    "def binary_accuracy(preds, y):\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.argmax(preds, dim=-1)\n",
    "\n",
    "    correct = (rounded_preds == y)\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "#push to cuda if available\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "\n",
    "    #initialize every epoch\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    #set the model in training phase\n",
    "    model.train()\n",
    "\n",
    "    for batch in iterator:\n",
    "\n",
    "        #resets the gradients after every batch\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #retrieve text and no. of words\n",
    "        text, text_lengths = batch.text\n",
    "\n",
    "        # predictions has list of likelihoods for the 3 classes for each sample.\n",
    "        predictions = model(text, text_lengths)\n",
    "\n",
    "        #compute the loss\n",
    "        loss = criterion(predictions, batch.label.type(torch.LongTensor))\n",
    "\n",
    "        #compute the binary accuracy\n",
    "        acc = binary_accuracy(predictions, batch.label.type(torch.LongTensor))\n",
    "\n",
    "        #backpropage the loss and compute the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        #update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        #loss and accuracy\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "\n",
    "    #initialize every epoch\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    #deactivating dropout layers\n",
    "    model.eval()\n",
    "\n",
    "    #deactivates autograd\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for batch in iterator:\n",
    "\n",
    "            #retrieve text and no. of words\n",
    "            text, text_lengths = batch.text\n",
    "\n",
    "            #convert to 1d tensor\n",
    "            predictions = model(text, text_lengths)\n",
    "\n",
    "            #compute loss and accuracy\n",
    "            loss = criterion(predictions, batch.label.type(torch.LongTensor))\n",
    "            acc = binary_accuracy(predictions, batch.label.type(torch.LongTensor))\n",
    "\n",
    "            #keep track of loss and accuracy\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 1.099 | Train Acc: 30.99%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 72.19%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.98%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 72.19%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 72.19%\n",
      "\tTrain Loss: 1.099 | Train Acc: 29.57%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 72.19%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.66%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 72.19%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.72%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 72.19%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.09%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 72.19%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 72.19%\n",
      "\tTrain Loss: 1.100 | Train Acc: 30.30%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 72.19%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.98%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 72.19%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.98%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 72.19%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.72%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 72.19%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.78%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 72.19%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.88%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 72.19%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.88%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 72.19%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.87%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 72.19%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 72.19%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.51%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 72.19%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.09%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 72.19%\n",
      "\tTrain Loss: 1.099 | Train Acc: 30.99%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 72.19%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.67%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 72.19%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.36%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 72.19%\n",
      "\tTrain Loss: 1.099 | Train Acc: 30.62%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 72.19%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.09%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 72.19%\n",
      "\tTrain Loss: 1.099 | Train Acc: 30.20%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 72.19%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.30%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 72.19%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.14%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 72.19%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.51%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 72.19%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.67%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 72.19%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.46%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 72.19%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.41%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 72.19%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.83%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 72.19%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.72%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 72.19%\n",
      "\tTrain Loss: 1.100 | Train Acc: 32.93%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 72.19%\n",
      "\tTrain Loss: 1.099 | Train Acc: 29.94%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 72.19%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.78%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 72.19%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 72.19%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.14%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 72.19%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 72.19%\n",
      "\tTrain Loss: 1.099 | Train Acc: 35.87%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 72.19%\n",
      "\tTrain Loss: 1.100 | Train Acc: 30.99%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 78.32%\n",
      "\tTrain Loss: 1.099 | Train Acc: 30.99%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 78.32%\n",
      "\tTrain Loss: 1.099 | Train Acc: 30.99%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 78.32%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.72%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 78.32%\n",
      "\tTrain Loss: 1.100 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 78.32%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.78%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 78.32%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.41%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 78.32%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.09%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 78.32%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.72%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 78.32%\n",
      "\tTrain Loss: 1.100 | Train Acc: 31.72%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 78.32%\n",
      "\tTrain Loss: 1.100 | Train Acc: 29.88%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 78.32%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.78%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 78.32%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.72%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 78.32%\n",
      "\tTrain Loss: 1.099 | Train Acc: 29.88%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 78.32%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.46%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 78.32%\n",
      "\tTrain Loss: 1.100 | Train Acc: 33.61%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 78.32%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.72%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 78.32%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.03%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 78.32%\n",
      "\tTrain Loss: 1.100 | Train Acc: 34.30%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 78.32%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.46%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 78.32%\n",
      "\tTrain Loss: 1.099 | Train Acc: 30.62%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 78.32%\n",
      "\tTrain Loss: 1.100 | Train Acc: 34.03%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 78.32%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.93%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 78.32%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.83%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 78.32%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.72%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 78.32%\n",
      "\tTrain Loss: 1.100 | Train Acc: 31.36%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 78.32%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.72%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 78.32%\n",
      "\tTrain Loss: 1.100 | Train Acc: 33.56%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 78.32%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.46%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 78.32%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.36%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 78.32%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.14%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 78.32%\n",
      "\tTrain Loss: 1.100 | Train Acc: 30.62%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 78.32%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.41%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 78.32%\n",
      "\tTrain Loss: 1.100 | Train Acc: 32.46%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 78.32%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.72%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 78.32%\n",
      "\tTrain Loss: 1.100 | Train Acc: 29.88%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 78.32%\n",
      "\tTrain Loss: 1.100 | Train Acc: 31.72%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 78.32%\n",
      "\tTrain Loss: 1.100 | Train Acc: 34.30%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 78.32%\n",
      "\tTrain Loss: 1.100 | Train Acc: 32.83%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 78.32%\n",
      "\tTrain Loss: 1.100 | Train Acc: 36.19%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 78.32%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.071 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.071 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.071 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.071 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.071 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.071 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.071 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.071 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.071 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.071 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.071 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.071 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.071 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.071 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.071 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.071 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.071 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.071 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.071 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.071 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.071 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.071 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.071 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.071 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.071 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.071 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.071 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.071 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.071 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.071 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.071 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.071 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.071 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.071 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.071 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.071 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.071 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.071 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.071 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.071 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.88%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.30%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.88%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.61%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.56%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.93%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.56%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.93%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.93%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.88%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.67%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.88%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.61%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.93%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.56%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.30%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.30%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.93%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.30%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.56%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.88%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.61%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.30%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.51%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.88%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.30%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 11.61%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.77%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 11.61%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.72%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 11.61%\n",
      "\tTrain Loss: 1.099 | Train Acc: 30.88%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 11.61%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.77%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 11.61%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.35%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 11.61%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.51%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 11.61%\n",
      "\tTrain Loss: 1.099 | Train Acc: 30.93%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 11.61%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.56%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 11.61%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.82%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 11.61%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.51%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 11.61%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.04%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 11.61%\n",
      "\tTrain Loss: 1.099 | Train Acc: 30.51%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 11.61%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.67%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 11.61%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.67%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 11.61%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.25%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 11.61%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.57%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 11.61%\n",
      "\tTrain Loss: 1.099 | Train Acc: 29.62%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 11.61%\n",
      "\tTrain Loss: 1.099 | Train Acc: 29.78%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 11.61%\n",
      "\tTrain Loss: 1.099 | Train Acc: 35.66%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 11.61%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.25%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 11.61%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.62%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 11.61%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.09%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 11.61%\n",
      "\tTrain Loss: 1.099 | Train Acc: 29.67%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 11.61%\n",
      "\tTrain Loss: 1.099 | Train Acc: 35.19%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 11.61%\n",
      "\tTrain Loss: 1.099 | Train Acc: 29.31%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 11.61%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.04%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 11.61%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.93%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 11.61%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.19%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 11.61%\n",
      "\tTrain Loss: 1.099 | Train Acc: 35.19%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 11.61%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.46%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 11.61%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.35%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 11.61%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.67%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 11.61%\n",
      "\tTrain Loss: 1.099 | Train Acc: 35.61%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 11.61%\n",
      "\tTrain Loss: 1.099 | Train Acc: 30.83%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 11.61%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.09%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 11.61%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.20%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 11.61%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.20%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 11.61%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.46%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 11.61%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.77%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 11.61%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.61%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 11.73%\n",
      "\tTrain Loss: 1.098 | Train Acc: 35.29%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 11.73%\n",
      "\tTrain Loss: 1.098 | Train Acc: 36.08%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 11.73%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.82%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 11.73%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.77%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 11.73%\n",
      "\tTrain Loss: 1.098 | Train Acc: 39.34%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 11.73%\n",
      "\tTrain Loss: 1.098 | Train Acc: 37.87%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 11.73%\n",
      "\tTrain Loss: 1.097 | Train Acc: 39.29%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 11.73%\n",
      "\tTrain Loss: 1.098 | Train Acc: 36.29%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 11.73%\n",
      "\tTrain Loss: 1.097 | Train Acc: 38.50%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 11.73%\n",
      "\tTrain Loss: 1.098 | Train Acc: 36.29%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 11.73%\n",
      "\tTrain Loss: 1.098 | Train Acc: 36.66%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 11.73%\n",
      "\tTrain Loss: 1.098 | Train Acc: 36.71%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 11.73%\n",
      "\tTrain Loss: 1.097 | Train Acc: 35.61%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 11.73%\n",
      "\tTrain Loss: 1.098 | Train Acc: 36.61%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 11.73%\n",
      "\tTrain Loss: 1.098 | Train Acc: 37.08%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 11.73%\n",
      "\tTrain Loss: 1.098 | Train Acc: 37.87%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 11.73%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.09%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 11.73%\n",
      "\tTrain Loss: 1.098 | Train Acc: 35.19%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 11.73%\n",
      "\tTrain Loss: 1.098 | Train Acc: 37.45%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 11.73%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.62%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 11.73%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.51%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 11.73%\n",
      "\tTrain Loss: 1.098 | Train Acc: 36.40%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 11.73%\n",
      "\tTrain Loss: 1.098 | Train Acc: 35.56%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 11.73%\n",
      "\tTrain Loss: 1.098 | Train Acc: 35.50%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 11.73%\n",
      "\tTrain Loss: 1.098 | Train Acc: 38.18%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 11.73%\n",
      "\tTrain Loss: 1.098 | Train Acc: 36.66%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 11.73%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.09%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 11.73%\n",
      "\tTrain Loss: 1.098 | Train Acc: 36.71%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 11.73%\n",
      "\tTrain Loss: 1.098 | Train Acc: 35.92%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 11.73%\n",
      "\tTrain Loss: 1.098 | Train Acc: 37.87%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 11.73%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.04%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 11.73%\n",
      "\tTrain Loss: 1.097 | Train Acc: 37.39%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 11.73%\n",
      "\tTrain Loss: 1.098 | Train Acc: 36.29%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 11.73%\n",
      "\tTrain Loss: 1.098 | Train Acc: 37.87%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 11.73%\n",
      "\tTrain Loss: 1.098 | Train Acc: 37.82%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 11.73%\n",
      "\tTrain Loss: 1.097 | Train Acc: 36.82%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 11.73%\n",
      "\tTrain Loss: 1.098 | Train Acc: 35.19%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 11.73%\n",
      "\tTrain Loss: 1.098 | Train Acc: 36.66%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 11.73%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.87%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 11.73%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.83%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.30%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.61%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.93%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.46%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.56%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.03%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.03%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.51%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.51%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.61%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.88%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.30%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.98%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.61%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.30%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.83%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.61%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.61%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.51%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.61%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.46%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.67%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.88%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.61%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.30%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.61%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.30%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.61%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.46%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.09%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.88%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.46%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.20%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.41%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.88%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.51%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.04%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.09%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.51%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.83%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.099 | Train Acc: 30.30%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.099 | Train Acc: 29.94%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.099 | Train Acc: 30.72%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.51%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.56%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.93%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.09%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.09%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.14%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.83%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.83%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.20%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.09%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.88%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.93%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.04%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.14%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.83%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.41%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.88%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.14%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.51%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.20%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.41%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.88%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.41%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.51%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.099 | Train Acc: 30.67%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.61%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.46%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 17.54%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.14%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 26.02%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.41%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 26.02%\n",
      "\tTrain Loss: 1.099 | Train Acc: 30.36%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 26.02%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.04%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 26.02%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.40%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 26.02%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.78%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 26.02%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.93%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 26.02%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.98%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 26.02%\n",
      "\tTrain Loss: 1.099 | Train Acc: 29.99%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 26.02%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.88%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 26.02%\n",
      "\tTrain Loss: 1.098 | Train Acc: 35.14%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 26.02%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.35%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 26.02%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.77%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 26.02%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.61%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 26.02%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.93%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 26.02%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.93%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 26.02%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.83%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 26.02%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.72%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 26.02%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.20%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 26.02%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.03%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 26.02%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.41%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 26.02%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.20%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 26.02%\n",
      "\tTrain Loss: 1.098 | Train Acc: 31.04%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 26.02%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.09%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 26.02%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.93%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 26.02%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 26.02%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 26.02%\n",
      "\tTrain Loss: 1.099 | Train Acc: 30.67%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 26.02%\n",
      "\tTrain Loss: 1.099 | Train Acc: 29.88%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 26.02%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.04%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 26.02%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.14%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 26.02%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.09%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 26.02%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.56%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 26.02%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.36%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 26.02%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.30%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 26.02%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.78%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 26.02%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.04%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 26.02%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.77%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 26.02%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 26.02%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 26.02%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.77%\n",
      "\t Val. Loss: 1.085 |  Val. Acc: 79.27%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.085 |  Val. Acc: 79.27%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.35%\n",
      "\t Val. Loss: 1.085 |  Val. Acc: 79.27%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.93%\n",
      "\t Val. Loss: 1.085 |  Val. Acc: 79.27%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.88%\n",
      "\t Val. Loss: 1.085 |  Val. Acc: 79.27%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.98%\n",
      "\t Val. Loss: 1.085 |  Val. Acc: 79.27%\n",
      "\tTrain Loss: 1.099 | Train Acc: 35.03%\n",
      "\t Val. Loss: 1.085 |  Val. Acc: 79.27%\n",
      "\tTrain Loss: 1.098 | Train Acc: 35.03%\n",
      "\t Val. Loss: 1.085 |  Val. Acc: 79.27%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.35%\n",
      "\t Val. Loss: 1.085 |  Val. Acc: 79.27%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.085 |  Val. Acc: 79.27%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.56%\n",
      "\t Val. Loss: 1.085 |  Val. Acc: 79.27%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.30%\n",
      "\t Val. Loss: 1.085 |  Val. Acc: 79.27%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.93%\n",
      "\t Val. Loss: 1.085 |  Val. Acc: 79.27%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.35%\n",
      "\t Val. Loss: 1.085 |  Val. Acc: 79.27%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.72%\n",
      "\t Val. Loss: 1.085 |  Val. Acc: 79.27%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.40%\n",
      "\t Val. Loss: 1.085 |  Val. Acc: 79.27%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.83%\n",
      "\t Val. Loss: 1.085 |  Val. Acc: 79.27%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.085 |  Val. Acc: 79.27%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.61%\n",
      "\t Val. Loss: 1.085 |  Val. Acc: 79.27%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.46%\n",
      "\t Val. Loss: 1.085 |  Val. Acc: 79.27%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.98%\n",
      "\t Val. Loss: 1.085 |  Val. Acc: 79.27%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.98%\n",
      "\t Val. Loss: 1.085 |  Val. Acc: 79.27%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.35%\n",
      "\t Val. Loss: 1.085 |  Val. Acc: 79.27%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.72%\n",
      "\t Val. Loss: 1.085 |  Val. Acc: 79.27%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.40%\n",
      "\t Val. Loss: 1.085 |  Val. Acc: 79.27%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.03%\n",
      "\t Val. Loss: 1.085 |  Val. Acc: 79.27%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.77%\n",
      "\t Val. Loss: 1.085 |  Val. Acc: 79.27%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.085 |  Val. Acc: 79.27%\n",
      "\tTrain Loss: 1.098 | Train Acc: 35.45%\n",
      "\t Val. Loss: 1.085 |  Val. Acc: 79.27%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.66%\n",
      "\t Val. Loss: 1.085 |  Val. Acc: 79.27%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.66%\n",
      "\t Val. Loss: 1.085 |  Val. Acc: 79.27%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.085 |  Val. Acc: 79.27%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.61%\n",
      "\t Val. Loss: 1.085 |  Val. Acc: 79.27%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.93%\n",
      "\t Val. Loss: 1.085 |  Val. Acc: 79.27%\n",
      "\tTrain Loss: 1.098 | Train Acc: 35.08%\n",
      "\t Val. Loss: 1.085 |  Val. Acc: 79.27%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.66%\n",
      "\t Val. Loss: 1.085 |  Val. Acc: 79.27%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.77%\n",
      "\t Val. Loss: 1.085 |  Val. Acc: 79.27%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.93%\n",
      "\t Val. Loss: 1.085 |  Val. Acc: 79.27%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.30%\n",
      "\t Val. Loss: 1.085 |  Val. Acc: 79.27%\n",
      "\tTrain Loss: 1.098 | Train Acc: 35.82%\n",
      "\t Val. Loss: 1.085 |  Val. Acc: 79.27%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.30%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.098 | Train Acc: 35.71%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.98%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.82%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.098 | Train Acc: 35.03%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.098 | Train Acc: 35.71%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.098 | Train Acc: 35.66%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.098 | Train Acc: 35.71%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.61%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.098 | Train Acc: 35.66%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.098 | Train Acc: 35.35%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.98%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.098 | Train Acc: 35.71%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.098 | Train Acc: 36.13%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.098 | Train Acc: 36.08%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.098 | Train Acc: 36.82%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.19%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.098 | Train Acc: 37.55%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.098 | Train Acc: 36.45%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.098 | Train Acc: 36.45%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.56%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.098 | Train Acc: 35.40%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.098 | Train Acc: 36.19%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.098 | Train Acc: 36.40%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.93%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.82%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.098 | Train Acc: 37.24%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.93%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.098 | Train Acc: 36.08%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.098 | Train Acc: 35.03%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.82%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.40%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.93%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.098 | Train Acc: 37.97%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.098 | Train Acc: 36.87%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.98%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.097 | Train Acc: 36.76%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.88%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.098 | Train Acc: 36.40%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.098 | Train Acc: 36.19%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.098 | Train Acc: 38.13%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 60.46%\n",
      "\tTrain Loss: 1.098 | Train Acc: 38.34%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 60.46%\n",
      "\tTrain Loss: 1.097 | Train Acc: 39.92%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 60.46%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.77%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 60.46%\n",
      "\tTrain Loss: 1.098 | Train Acc: 36.92%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 60.46%\n",
      "\tTrain Loss: 1.098 | Train Acc: 35.98%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 60.46%\n",
      "\tTrain Loss: 1.097 | Train Acc: 37.71%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 60.46%\n",
      "\tTrain Loss: 1.098 | Train Acc: 35.77%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 60.46%\n",
      "\tTrain Loss: 1.098 | Train Acc: 36.97%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 60.46%\n",
      "\tTrain Loss: 1.098 | Train Acc: 39.65%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 60.46%\n",
      "\tTrain Loss: 1.098 | Train Acc: 35.98%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 60.46%\n",
      "\tTrain Loss: 1.098 | Train Acc: 38.87%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 60.46%\n",
      "\tTrain Loss: 1.098 | Train Acc: 35.92%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 60.46%\n",
      "\tTrain Loss: 1.098 | Train Acc: 39.55%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 60.46%\n",
      "\tTrain Loss: 1.098 | Train Acc: 38.50%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 60.46%\n",
      "\tTrain Loss: 1.098 | Train Acc: 41.07%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 60.46%\n",
      "\tTrain Loss: 1.097 | Train Acc: 37.03%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 60.46%\n",
      "\tTrain Loss: 1.098 | Train Acc: 37.71%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 60.46%\n",
      "\tTrain Loss: 1.097 | Train Acc: 37.76%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 60.46%\n",
      "\tTrain Loss: 1.098 | Train Acc: 36.66%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 60.46%\n",
      "\tTrain Loss: 1.098 | Train Acc: 35.24%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 60.46%\n",
      "\tTrain Loss: 1.098 | Train Acc: 36.24%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 60.46%\n",
      "\tTrain Loss: 1.098 | Train Acc: 38.81%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 60.46%\n",
      "\tTrain Loss: 1.097 | Train Acc: 39.65%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 60.46%\n",
      "\tTrain Loss: 1.098 | Train Acc: 36.55%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 60.46%\n",
      "\tTrain Loss: 1.098 | Train Acc: 37.66%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 60.46%\n",
      "\tTrain Loss: 1.098 | Train Acc: 35.14%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 60.46%\n",
      "\tTrain Loss: 1.098 | Train Acc: 38.03%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 60.46%\n",
      "\tTrain Loss: 1.098 | Train Acc: 36.66%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 60.46%\n",
      "\tTrain Loss: 1.098 | Train Acc: 37.76%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 60.46%\n",
      "\tTrain Loss: 1.098 | Train Acc: 36.66%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 60.46%\n",
      "\tTrain Loss: 1.098 | Train Acc: 35.14%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 60.46%\n",
      "\tTrain Loss: 1.098 | Train Acc: 38.81%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 60.46%\n",
      "\tTrain Loss: 1.098 | Train Acc: 38.45%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 60.46%\n",
      "\tTrain Loss: 1.098 | Train Acc: 39.86%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 60.46%\n",
      "\tTrain Loss: 1.098 | Train Acc: 39.55%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 60.46%\n",
      "\tTrain Loss: 1.098 | Train Acc: 38.08%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 60.46%\n",
      "\tTrain Loss: 1.097 | Train Acc: 39.60%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 60.46%\n",
      "\tTrain Loss: 1.097 | Train Acc: 36.97%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 60.46%\n",
      "\tTrain Loss: 1.098 | Train Acc: 35.50%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 60.46%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.56%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 11.22%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.99%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 11.22%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.93%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 11.22%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.14%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 11.22%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.77%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 11.22%\n",
      "\tTrain Loss: 1.099 | Train Acc: 35.29%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 11.22%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.62%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 11.22%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.57%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 11.22%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.88%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 11.22%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.98%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 11.22%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.67%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 11.22%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.19%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 11.22%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.87%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 11.22%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.56%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 11.22%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.14%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 11.22%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.46%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 11.22%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.98%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 11.22%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.88%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 11.22%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.45%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 11.22%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.67%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 11.22%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.77%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 11.22%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.93%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 11.22%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.77%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 11.22%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.35%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 11.22%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.77%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 11.22%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.35%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 11.22%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.35%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 11.22%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.93%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 11.22%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.30%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 11.22%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.30%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 11.22%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.99%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 11.22%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.77%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 11.22%\n",
      "\tTrain Loss: 1.099 | Train Acc: 30.41%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 11.22%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.04%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 11.22%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.88%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 11.22%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.77%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 11.22%\n",
      "\tTrain Loss: 1.099 | Train Acc: 30.41%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 11.22%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.67%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 11.22%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.35%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 11.22%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.93%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 11.22%\n",
      "\tTrain Loss: 1.100 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.066 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.066 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.066 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.066 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.066 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.100 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.066 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.100 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.066 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.066 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.066 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.066 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.066 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.066 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.066 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.066 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.066 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.066 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.100 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.066 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.066 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.066 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.066 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.066 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.066 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.066 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.066 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.066 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.066 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.066 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.066 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.066 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.066 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.066 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.066 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.066 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.066 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.066 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.066 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.100 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.066 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.066 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.066 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.066 |  Val. Acc: 83.80%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.56%\n",
      "\t Val. Loss: 1.087 |  Val. Acc: 81.57%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.88%\n",
      "\t Val. Loss: 1.087 |  Val. Acc: 81.57%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.46%\n",
      "\t Val. Loss: 1.087 |  Val. Acc: 81.57%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.087 |  Val. Acc: 81.57%\n",
      "\tTrain Loss: 1.100 | Train Acc: 32.09%\n",
      "\t Val. Loss: 1.087 |  Val. Acc: 81.57%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.61%\n",
      "\t Val. Loss: 1.087 |  Val. Acc: 81.57%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.14%\n",
      "\t Val. Loss: 1.087 |  Val. Acc: 81.57%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.087 |  Val. Acc: 81.57%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.77%\n",
      "\t Val. Loss: 1.087 |  Val. Acc: 81.57%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.51%\n",
      "\t Val. Loss: 1.087 |  Val. Acc: 81.57%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.78%\n",
      "\t Val. Loss: 1.087 |  Val. Acc: 81.57%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.61%\n",
      "\t Val. Loss: 1.087 |  Val. Acc: 81.57%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.56%\n",
      "\t Val. Loss: 1.087 |  Val. Acc: 81.57%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.35%\n",
      "\t Val. Loss: 1.087 |  Val. Acc: 81.57%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.46%\n",
      "\t Val. Loss: 1.087 |  Val. Acc: 81.57%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.56%\n",
      "\t Val. Loss: 1.087 |  Val. Acc: 81.57%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.09%\n",
      "\t Val. Loss: 1.087 |  Val. Acc: 81.57%\n",
      "\tTrain Loss: 1.099 | Train Acc: 35.03%\n",
      "\t Val. Loss: 1.087 |  Val. Acc: 81.57%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.30%\n",
      "\t Val. Loss: 1.087 |  Val. Acc: 81.57%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.83%\n",
      "\t Val. Loss: 1.087 |  Val. Acc: 81.57%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.61%\n",
      "\t Val. Loss: 1.087 |  Val. Acc: 81.57%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.72%\n",
      "\t Val. Loss: 1.087 |  Val. Acc: 81.57%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.087 |  Val. Acc: 81.57%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.46%\n",
      "\t Val. Loss: 1.087 |  Val. Acc: 81.57%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.61%\n",
      "\t Val. Loss: 1.087 |  Val. Acc: 81.57%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.88%\n",
      "\t Val. Loss: 1.087 |  Val. Acc: 81.57%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.14%\n",
      "\t Val. Loss: 1.087 |  Val. Acc: 81.57%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.087 |  Val. Acc: 81.57%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.51%\n",
      "\t Val. Loss: 1.087 |  Val. Acc: 81.57%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.087 |  Val. Acc: 81.57%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.83%\n",
      "\t Val. Loss: 1.087 |  Val. Acc: 81.57%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.83%\n",
      "\t Val. Loss: 1.087 |  Val. Acc: 81.57%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.087 |  Val. Acc: 81.57%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.41%\n",
      "\t Val. Loss: 1.087 |  Val. Acc: 81.57%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.41%\n",
      "\t Val. Loss: 1.087 |  Val. Acc: 81.57%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.51%\n",
      "\t Val. Loss: 1.087 |  Val. Acc: 81.57%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.087 |  Val. Acc: 81.57%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.56%\n",
      "\t Val. Loss: 1.087 |  Val. Acc: 81.57%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.41%\n",
      "\t Val. Loss: 1.087 |  Val. Acc: 81.57%\n",
      "\tTrain Loss: 1.100 | Train Acc: 32.46%\n",
      "\t Val. Loss: 1.087 |  Val. Acc: 81.57%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.72%\n",
      "\t Val. Loss: 1.086 |  Val. Acc: 77.93%\n",
      "\tTrain Loss: 1.100 | Train Acc: 30.62%\n",
      "\t Val. Loss: 1.086 |  Val. Acc: 77.93%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.36%\n",
      "\t Val. Loss: 1.086 |  Val. Acc: 77.93%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.41%\n",
      "\t Val. Loss: 1.086 |  Val. Acc: 77.93%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.77%\n",
      "\t Val. Loss: 1.086 |  Val. Acc: 77.93%\n",
      "\tTrain Loss: 1.100 | Train Acc: 30.62%\n",
      "\t Val. Loss: 1.086 |  Val. Acc: 77.93%\n",
      "\tTrain Loss: 1.099 | Train Acc: 29.10%\n",
      "\t Val. Loss: 1.086 |  Val. Acc: 77.93%\n",
      "\tTrain Loss: 1.099 | Train Acc: 30.99%\n",
      "\t Val. Loss: 1.086 |  Val. Acc: 77.93%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.72%\n",
      "\t Val. Loss: 1.086 |  Val. Acc: 77.93%\n",
      "\tTrain Loss: 1.099 | Train Acc: 29.88%\n",
      "\t Val. Loss: 1.086 |  Val. Acc: 77.93%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.41%\n",
      "\t Val. Loss: 1.086 |  Val. Acc: 77.93%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.41%\n",
      "\t Val. Loss: 1.086 |  Val. Acc: 77.93%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.41%\n",
      "\t Val. Loss: 1.086 |  Val. Acc: 77.93%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.67%\n",
      "\t Val. Loss: 1.086 |  Val. Acc: 77.93%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.67%\n",
      "\t Val. Loss: 1.086 |  Val. Acc: 77.93%\n",
      "\tTrain Loss: 1.100 | Train Acc: 32.09%\n",
      "\t Val. Loss: 1.086 |  Val. Acc: 77.93%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.36%\n",
      "\t Val. Loss: 1.086 |  Val. Acc: 77.93%\n",
      "\tTrain Loss: 1.099 | Train Acc: 30.57%\n",
      "\t Val. Loss: 1.086 |  Val. Acc: 77.93%\n",
      "\tTrain Loss: 1.100 | Train Acc: 31.72%\n",
      "\t Val. Loss: 1.086 |  Val. Acc: 77.93%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.14%\n",
      "\t Val. Loss: 1.086 |  Val. Acc: 77.93%\n",
      "\tTrain Loss: 1.099 | Train Acc: 30.93%\n",
      "\t Val. Loss: 1.086 |  Val. Acc: 77.93%\n",
      "\tTrain Loss: 1.100 | Train Acc: 31.04%\n",
      "\t Val. Loss: 1.086 |  Val. Acc: 77.93%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.78%\n",
      "\t Val. Loss: 1.086 |  Val. Acc: 77.93%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.93%\n",
      "\t Val. Loss: 1.086 |  Val. Acc: 77.93%\n",
      "\tTrain Loss: 1.099 | Train Acc: 30.67%\n",
      "\t Val. Loss: 1.086 |  Val. Acc: 77.93%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.36%\n",
      "\t Val. Loss: 1.086 |  Val. Acc: 77.93%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.36%\n",
      "\t Val. Loss: 1.086 |  Val. Acc: 77.93%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.09%\n",
      "\t Val. Loss: 1.086 |  Val. Acc: 77.93%\n",
      "\tTrain Loss: 1.099 | Train Acc: 29.88%\n",
      "\t Val. Loss: 1.086 |  Val. Acc: 77.93%\n",
      "\tTrain Loss: 1.099 | Train Acc: 30.93%\n",
      "\t Val. Loss: 1.086 |  Val. Acc: 77.93%\n",
      "\tTrain Loss: 1.099 | Train Acc: 29.83%\n",
      "\t Val. Loss: 1.086 |  Val. Acc: 77.93%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.72%\n",
      "\t Val. Loss: 1.086 |  Val. Acc: 77.93%\n",
      "\tTrain Loss: 1.099 | Train Acc: 30.25%\n",
      "\t Val. Loss: 1.086 |  Val. Acc: 77.93%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.04%\n",
      "\t Val. Loss: 1.086 |  Val. Acc: 77.93%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.36%\n",
      "\t Val. Loss: 1.086 |  Val. Acc: 77.93%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.04%\n",
      "\t Val. Loss: 1.086 |  Val. Acc: 77.93%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.30%\n",
      "\t Val. Loss: 1.086 |  Val. Acc: 77.93%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.086 |  Val. Acc: 77.93%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.78%\n",
      "\t Val. Loss: 1.086 |  Val. Acc: 77.93%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.04%\n",
      "\t Val. Loss: 1.086 |  Val. Acc: 77.93%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.56%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 24.68%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.40%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 24.68%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.14%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 24.68%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.61%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 24.68%\n",
      "\tTrain Loss: 1.099 | Train Acc: 30.62%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 24.68%\n",
      "\tTrain Loss: 1.098 | Train Acc: 31.78%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 24.68%\n",
      "\tTrain Loss: 1.099 | Train Acc: 30.93%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 24.68%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.09%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 24.68%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.98%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 24.68%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.56%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 24.68%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.56%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 24.68%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.72%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 24.68%\n",
      "\tTrain Loss: 1.099 | Train Acc: 30.99%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 24.68%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 24.68%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.04%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 24.68%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.46%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 24.68%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.04%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 24.68%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.46%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 24.68%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.09%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 24.68%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.78%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 24.68%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.56%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 24.68%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.61%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 24.68%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.41%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 24.68%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 24.68%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.36%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 24.68%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.09%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 24.68%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.78%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 24.68%\n",
      "\tTrain Loss: 1.099 | Train Acc: 30.30%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 24.68%\n",
      "\tTrain Loss: 1.098 | Train Acc: 31.72%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 24.68%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.61%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 24.68%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.09%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 24.68%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.46%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 24.68%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.51%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 24.68%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.88%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 24.68%\n",
      "\tTrain Loss: 1.099 | Train Acc: 29.52%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 24.68%\n",
      "\tTrain Loss: 1.099 | Train Acc: 29.10%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 24.68%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.14%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 24.68%\n",
      "\tTrain Loss: 1.099 | Train Acc: 30.30%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 24.68%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.41%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 24.68%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.61%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 24.68%\n",
      "\tTrain Loss: 1.098 | Train Acc: 36.19%\n",
      "\t Val. Loss: 1.125 |  Val. Acc: 0.89%\n",
      "\tTrain Loss: 1.098 | Train Acc: 35.82%\n",
      "\t Val. Loss: 1.125 |  Val. Acc: 0.89%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.125 |  Val. Acc: 0.89%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.125 |  Val. Acc: 0.89%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.56%\n",
      "\t Val. Loss: 1.125 |  Val. Acc: 0.89%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.66%\n",
      "\t Val. Loss: 1.125 |  Val. Acc: 0.89%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.88%\n",
      "\t Val. Loss: 1.125 |  Val. Acc: 0.89%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.61%\n",
      "\t Val. Loss: 1.125 |  Val. Acc: 0.89%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.66%\n",
      "\t Val. Loss: 1.125 |  Val. Acc: 0.89%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.24%\n",
      "\t Val. Loss: 1.125 |  Val. Acc: 0.89%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.61%\n",
      "\t Val. Loss: 1.125 |  Val. Acc: 0.89%\n",
      "\tTrain Loss: 1.098 | Train Acc: 35.45%\n",
      "\t Val. Loss: 1.125 |  Val. Acc: 0.89%\n",
      "\tTrain Loss: 1.098 | Train Acc: 31.72%\n",
      "\t Val. Loss: 1.125 |  Val. Acc: 0.89%\n",
      "\tTrain Loss: 1.098 | Train Acc: 35.40%\n",
      "\t Val. Loss: 1.125 |  Val. Acc: 0.89%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.98%\n",
      "\t Val. Loss: 1.125 |  Val. Acc: 0.89%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.125 |  Val. Acc: 0.89%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.56%\n",
      "\t Val. Loss: 1.125 |  Val. Acc: 0.89%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.88%\n",
      "\t Val. Loss: 1.125 |  Val. Acc: 0.89%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.125 |  Val. Acc: 0.89%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.125 |  Val. Acc: 0.89%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.66%\n",
      "\t Val. Loss: 1.125 |  Val. Acc: 0.89%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.125 |  Val. Acc: 0.89%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.35%\n",
      "\t Val. Loss: 1.125 |  Val. Acc: 0.89%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.51%\n",
      "\t Val. Loss: 1.125 |  Val. Acc: 0.89%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.14%\n",
      "\t Val. Loss: 1.125 |  Val. Acc: 0.89%\n",
      "\tTrain Loss: 1.098 | Train Acc: 35.40%\n",
      "\t Val. Loss: 1.125 |  Val. Acc: 0.89%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.98%\n",
      "\t Val. Loss: 1.125 |  Val. Acc: 0.89%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.30%\n",
      "\t Val. Loss: 1.125 |  Val. Acc: 0.89%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.35%\n",
      "\t Val. Loss: 1.125 |  Val. Acc: 0.89%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.125 |  Val. Acc: 0.89%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.88%\n",
      "\t Val. Loss: 1.125 |  Val. Acc: 0.89%\n",
      "\tTrain Loss: 1.098 | Train Acc: 36.13%\n",
      "\t Val. Loss: 1.125 |  Val. Acc: 0.89%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.51%\n",
      "\t Val. Loss: 1.125 |  Val. Acc: 0.89%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.14%\n",
      "\t Val. Loss: 1.125 |  Val. Acc: 0.89%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.56%\n",
      "\t Val. Loss: 1.125 |  Val. Acc: 0.89%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.56%\n",
      "\t Val. Loss: 1.125 |  Val. Acc: 0.89%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.30%\n",
      "\t Val. Loss: 1.125 |  Val. Acc: 0.89%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.56%\n",
      "\t Val. Loss: 1.125 |  Val. Acc: 0.89%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.30%\n",
      "\t Val. Loss: 1.125 |  Val. Acc: 0.89%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.83%\n",
      "\t Val. Loss: 1.125 |  Val. Acc: 0.89%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.14%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 18.49%\n",
      "\tTrain Loss: 1.100 | Train Acc: 32.88%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 18.49%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.61%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 18.49%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.61%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 18.49%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 18.49%\n",
      "\tTrain Loss: 1.100 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 18.49%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.67%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 18.49%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.61%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 18.49%\n",
      "\tTrain Loss: 1.100 | Train Acc: 31.30%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 18.49%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 18.49%\n",
      "\tTrain Loss: 1.100 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 18.49%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.61%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 18.49%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 18.49%\n",
      "\tTrain Loss: 1.100 | Train Acc: 34.35%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 18.49%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.46%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 18.49%\n",
      "\tTrain Loss: 1.100 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 18.49%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.61%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 18.49%\n",
      "\tTrain Loss: 1.100 | Train Acc: 31.72%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 18.49%\n",
      "\tTrain Loss: 1.100 | Train Acc: 32.51%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 18.49%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.03%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 18.49%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.35%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 18.49%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.03%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 18.49%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.30%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 18.49%\n",
      "\tTrain Loss: 1.100 | Train Acc: 32.51%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 18.49%\n",
      "\tTrain Loss: 1.100 | Train Acc: 31.67%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 18.49%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.67%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 18.49%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.09%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 18.49%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.46%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 18.49%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.46%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 18.49%\n",
      "\tTrain Loss: 1.100 | Train Acc: 32.83%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 18.49%\n",
      "\tTrain Loss: 1.100 | Train Acc: 32.51%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 18.49%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 18.49%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.46%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 18.49%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.72%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 18.49%\n",
      "\tTrain Loss: 1.100 | Train Acc: 32.46%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 18.49%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.83%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 18.49%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.72%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 18.49%\n",
      "\tTrain Loss: 1.100 | Train Acc: 32.88%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 18.49%\n",
      "\tTrain Loss: 1.100 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 18.49%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.88%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 18.49%\n",
      "\tTrain Loss: 1.098 | Train Acc: 37.08%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 15.69%\n",
      "\tTrain Loss: 1.098 | Train Acc: 36.71%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 15.69%\n",
      "\tTrain Loss: 1.098 | Train Acc: 39.44%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 15.69%\n",
      "\tTrain Loss: 1.099 | Train Acc: 36.71%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 15.69%\n",
      "\tTrain Loss: 1.098 | Train Acc: 35.29%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 15.69%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.77%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 15.69%\n",
      "\tTrain Loss: 1.098 | Train Acc: 35.56%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 15.69%\n",
      "\tTrain Loss: 1.098 | Train Acc: 37.87%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 15.69%\n",
      "\tTrain Loss: 1.098 | Train Acc: 36.87%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 15.69%\n",
      "\tTrain Loss: 1.099 | Train Acc: 35.29%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 15.69%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.09%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 15.69%\n",
      "\tTrain Loss: 1.098 | Train Acc: 35.98%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 15.69%\n",
      "\tTrain Loss: 1.098 | Train Acc: 35.92%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 15.69%\n",
      "\tTrain Loss: 1.098 | Train Acc: 41.18%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 15.69%\n",
      "\tTrain Loss: 1.098 | Train Acc: 37.13%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 15.69%\n",
      "\tTrain Loss: 1.098 | Train Acc: 37.92%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 15.69%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.30%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 15.69%\n",
      "\tTrain Loss: 1.098 | Train Acc: 36.82%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 15.69%\n",
      "\tTrain Loss: 1.098 | Train Acc: 36.82%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 15.69%\n",
      "\tTrain Loss: 1.098 | Train Acc: 36.34%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 15.69%\n",
      "\tTrain Loss: 1.099 | Train Acc: 36.03%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 15.69%\n",
      "\tTrain Loss: 1.098 | Train Acc: 35.29%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 15.69%\n",
      "\tTrain Loss: 1.098 | Train Acc: 36.40%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 15.69%\n",
      "\tTrain Loss: 1.099 | Train Acc: 35.35%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 15.69%\n",
      "\tTrain Loss: 1.098 | Train Acc: 37.13%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 15.69%\n",
      "\tTrain Loss: 1.098 | Train Acc: 37.50%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 15.69%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.51%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 15.69%\n",
      "\tTrain Loss: 1.098 | Train Acc: 35.87%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 15.69%\n",
      "\tTrain Loss: 1.098 | Train Acc: 38.97%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 15.69%\n",
      "\tTrain Loss: 1.098 | Train Acc: 40.81%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 15.69%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.19%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 15.69%\n",
      "\tTrain Loss: 1.098 | Train Acc: 35.19%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 15.69%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.77%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 15.69%\n",
      "\tTrain Loss: 1.099 | Train Acc: 36.29%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 15.69%\n",
      "\tTrain Loss: 1.098 | Train Acc: 35.24%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 15.69%\n",
      "\tTrain Loss: 1.098 | Train Acc: 36.82%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 15.69%\n",
      "\tTrain Loss: 1.098 | Train Acc: 35.87%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 15.69%\n",
      "\tTrain Loss: 1.098 | Train Acc: 36.82%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 15.69%\n",
      "\tTrain Loss: 1.099 | Train Acc: 36.40%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 15.69%\n",
      "\tTrain Loss: 1.098 | Train Acc: 36.76%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 15.69%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.51%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 1.79%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.51%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 1.79%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.77%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 1.79%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.88%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 1.79%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.51%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 1.79%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.51%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 1.79%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.24%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 1.79%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.77%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 1.79%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.56%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 1.79%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.83%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 1.79%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.93%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 1.79%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.77%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 1.79%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.51%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 1.79%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.14%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 1.79%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.14%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 1.79%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.51%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 1.79%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 1.79%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.24%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 1.79%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 1.79%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.66%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 1.79%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.14%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 1.79%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.14%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 1.79%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.88%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 1.79%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.51%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 1.79%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.93%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 1.79%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.93%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 1.79%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 1.79%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.51%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 1.79%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.56%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 1.79%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.51%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 1.79%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.56%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 1.79%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.30%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 1.79%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.14%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 1.79%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.51%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 1.79%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.51%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 1.79%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.77%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 1.79%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.88%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 1.79%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.41%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 1.79%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.93%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 1.79%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 1.79%\n",
      "\tTrain Loss: 1.099 | Train Acc: 30.67%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 13.90%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.04%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 13.90%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.14%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 13.90%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.09%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 13.90%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.46%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 13.90%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.09%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 13.90%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.20%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 13.90%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.66%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 13.90%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.62%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 13.90%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.51%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 13.90%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.67%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 13.90%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.35%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 13.90%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.61%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 13.90%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.72%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 13.90%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.83%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 13.90%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.14%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 13.90%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.40%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 13.90%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.04%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 13.90%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.56%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 13.90%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.09%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 13.90%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.83%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 13.90%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.03%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 13.90%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.78%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 13.90%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.72%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 13.90%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.77%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 13.90%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.51%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 13.90%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.62%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 13.90%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.30%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 13.90%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.51%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 13.90%\n",
      "\tTrain Loss: 1.099 | Train Acc: 35.08%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 13.90%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.09%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 13.90%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 13.90%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.67%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 13.90%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.04%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 13.90%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.82%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 13.90%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.88%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 13.90%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.88%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 13.90%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.25%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 13.90%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.98%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 13.90%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.98%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 13.90%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.88%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 4.91%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.93%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 4.91%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.98%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 4.91%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.24%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 4.91%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.24%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 4.91%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.35%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 4.91%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.14%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 4.91%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.99%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 4.91%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.88%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 4.91%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.88%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 4.91%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.24%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 4.91%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.88%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 4.91%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.88%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 4.91%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.51%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 4.91%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.09%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 4.91%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.88%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 4.91%\n",
      "\tTrain Loss: 1.099 | Train Acc: 35.66%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 4.91%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.04%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 4.91%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.14%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 4.91%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.99%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 4.91%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.14%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 4.91%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.88%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 4.91%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.51%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 4.91%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.04%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 4.91%\n",
      "\tTrain Loss: 1.098 | Train Acc: 35.66%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 4.91%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.51%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 4.91%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.51%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 4.91%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.14%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 4.91%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.72%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 4.91%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.56%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 4.91%\n",
      "\tTrain Loss: 1.099 | Train Acc: 35.35%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 4.91%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.93%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 4.91%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.61%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 4.91%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.88%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 4.91%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.04%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 4.91%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.93%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 4.91%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.51%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 4.91%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.67%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 4.91%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.51%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 4.91%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.30%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 4.91%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.62%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 57.21%\n",
      "\tTrain Loss: 1.098 | Train Acc: 37.76%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 57.21%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.40%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 57.21%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.56%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 57.21%\n",
      "\tTrain Loss: 1.098 | Train Acc: 35.08%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 57.21%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.62%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 57.21%\n",
      "\tTrain Loss: 1.098 | Train Acc: 36.66%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 57.21%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.45%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 57.21%\n",
      "\tTrain Loss: 1.098 | Train Acc: 31.46%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 57.21%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.98%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 57.21%\n",
      "\tTrain Loss: 1.098 | Train Acc: 35.92%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 57.21%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.03%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 57.21%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.51%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 57.21%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.77%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 57.21%\n",
      "\tTrain Loss: 1.098 | Train Acc: 37.03%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 57.21%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.30%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 57.21%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.51%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 57.21%\n",
      "\tTrain Loss: 1.098 | Train Acc: 35.14%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 57.21%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.14%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 57.21%\n",
      "\tTrain Loss: 1.098 | Train Acc: 35.50%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 57.21%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.35%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 57.21%\n",
      "\tTrain Loss: 1.097 | Train Acc: 35.87%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 57.21%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.35%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 57.21%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.30%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 57.21%\n",
      "\tTrain Loss: 1.098 | Train Acc: 37.71%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 57.21%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.77%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 57.21%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.77%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 57.21%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.40%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 57.21%\n",
      "\tTrain Loss: 1.098 | Train Acc: 36.66%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 57.21%\n",
      "\tTrain Loss: 1.098 | Train Acc: 31.83%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 57.21%\n",
      "\tTrain Loss: 1.098 | Train Acc: 35.19%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 57.21%\n",
      "\tTrain Loss: 1.098 | Train Acc: 35.03%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 57.21%\n",
      "\tTrain Loss: 1.098 | Train Acc: 36.19%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 57.21%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 57.21%\n",
      "\tTrain Loss: 1.098 | Train Acc: 38.87%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 57.21%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.93%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 57.21%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.03%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 57.21%\n",
      "\tTrain Loss: 1.098 | Train Acc: 35.08%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 57.21%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.09%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 57.21%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.40%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 57.21%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.51%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.88%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.61%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.67%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.88%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.35%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.51%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.56%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.30%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.56%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.51%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.35%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.88%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.14%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.30%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.67%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.03%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.14%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.93%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.93%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.98%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.51%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.20%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.61%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.72%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.93%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.35%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.88%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.88%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.88%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.03%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.40%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.30%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.30%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.61%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.56%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.40%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.67%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.67%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.61%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.67%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.61%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.93%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.30%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.40%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.88%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.98%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.30%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.56%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.30%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.30%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.88%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.30%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.61%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.67%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.93%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.88%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.61%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.61%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.67%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.40%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.30%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.30%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.03%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.03%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.30%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.61%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.88%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.61%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.30%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.88%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.20%\n",
      "\tTrain Loss: 1.100 | Train Acc: 32.09%\n",
      "\t Val. Loss: 1.083 |  Val. Acc: 82.91%\n",
      "\tTrain Loss: 1.100 | Train Acc: 32.88%\n",
      "\t Val. Loss: 1.083 |  Val. Acc: 82.91%\n",
      "\tTrain Loss: 1.100 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.083 |  Val. Acc: 82.91%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.46%\n",
      "\t Val. Loss: 1.083 |  Val. Acc: 82.91%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.93%\n",
      "\t Val. Loss: 1.083 |  Val. Acc: 82.91%\n",
      "\tTrain Loss: 1.100 | Train Acc: 32.51%\n",
      "\t Val. Loss: 1.083 |  Val. Acc: 82.91%\n",
      "\tTrain Loss: 1.100 | Train Acc: 32.51%\n",
      "\t Val. Loss: 1.083 |  Val. Acc: 82.91%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.083 |  Val. Acc: 82.91%\n",
      "\tTrain Loss: 1.100 | Train Acc: 32.46%\n",
      "\t Val. Loss: 1.083 |  Val. Acc: 82.91%\n",
      "\tTrain Loss: 1.100 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.083 |  Val. Acc: 82.91%\n",
      "\tTrain Loss: 1.100 | Train Acc: 32.83%\n",
      "\t Val. Loss: 1.083 |  Val. Acc: 82.91%\n",
      "\tTrain Loss: 1.100 | Train Acc: 33.61%\n",
      "\t Val. Loss: 1.083 |  Val. Acc: 82.91%\n",
      "\tTrain Loss: 1.100 | Train Acc: 32.88%\n",
      "\t Val. Loss: 1.083 |  Val. Acc: 82.91%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.88%\n",
      "\t Val. Loss: 1.083 |  Val. Acc: 82.91%\n",
      "\tTrain Loss: 1.100 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.083 |  Val. Acc: 82.91%\n",
      "\tTrain Loss: 1.100 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.083 |  Val. Acc: 82.91%\n",
      "\tTrain Loss: 1.100 | Train Acc: 32.14%\n",
      "\t Val. Loss: 1.083 |  Val. Acc: 82.91%\n",
      "\tTrain Loss: 1.100 | Train Acc: 32.51%\n",
      "\t Val. Loss: 1.083 |  Val. Acc: 82.91%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.61%\n",
      "\t Val. Loss: 1.083 |  Val. Acc: 82.91%\n",
      "\tTrain Loss: 1.100 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.083 |  Val. Acc: 82.91%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.083 |  Val. Acc: 82.91%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.61%\n",
      "\t Val. Loss: 1.083 |  Val. Acc: 82.91%\n",
      "\tTrain Loss: 1.100 | Train Acc: 32.51%\n",
      "\t Val. Loss: 1.083 |  Val. Acc: 82.91%\n",
      "\tTrain Loss: 1.100 | Train Acc: 32.51%\n",
      "\t Val. Loss: 1.083 |  Val. Acc: 82.91%\n",
      "\tTrain Loss: 1.100 | Train Acc: 31.72%\n",
      "\t Val. Loss: 1.083 |  Val. Acc: 82.91%\n",
      "\tTrain Loss: 1.100 | Train Acc: 32.88%\n",
      "\t Val. Loss: 1.083 |  Val. Acc: 82.91%\n",
      "\tTrain Loss: 1.100 | Train Acc: 32.46%\n",
      "\t Val. Loss: 1.083 |  Val. Acc: 82.91%\n",
      "\tTrain Loss: 1.100 | Train Acc: 32.88%\n",
      "\t Val. Loss: 1.083 |  Val. Acc: 82.91%\n",
      "\tTrain Loss: 1.100 | Train Acc: 32.46%\n",
      "\t Val. Loss: 1.083 |  Val. Acc: 82.91%\n",
      "\tTrain Loss: 1.100 | Train Acc: 33.61%\n",
      "\t Val. Loss: 1.083 |  Val. Acc: 82.91%\n",
      "\tTrain Loss: 1.100 | Train Acc: 32.83%\n",
      "\t Val. Loss: 1.083 |  Val. Acc: 82.91%\n",
      "\tTrain Loss: 1.100 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.083 |  Val. Acc: 82.91%\n",
      "\tTrain Loss: 1.100 | Train Acc: 31.72%\n",
      "\t Val. Loss: 1.083 |  Val. Acc: 82.91%\n",
      "\tTrain Loss: 1.100 | Train Acc: 32.83%\n",
      "\t Val. Loss: 1.083 |  Val. Acc: 82.91%\n",
      "\tTrain Loss: 1.100 | Train Acc: 33.56%\n",
      "\t Val. Loss: 1.083 |  Val. Acc: 82.91%\n",
      "\tTrain Loss: 1.100 | Train Acc: 32.88%\n",
      "\t Val. Loss: 1.083 |  Val. Acc: 82.91%\n",
      "\tTrain Loss: 1.100 | Train Acc: 33.61%\n",
      "\t Val. Loss: 1.083 |  Val. Acc: 82.91%\n",
      "\tTrain Loss: 1.100 | Train Acc: 32.83%\n",
      "\t Val. Loss: 1.083 |  Val. Acc: 82.91%\n",
      "\tTrain Loss: 1.100 | Train Acc: 32.83%\n",
      "\t Val. Loss: 1.083 |  Val. Acc: 82.91%\n",
      "\tTrain Loss: 1.100 | Train Acc: 32.83%\n",
      "\t Val. Loss: 1.083 |  Val. Acc: 82.91%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.78%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 79.34%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.56%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 79.34%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.83%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 79.34%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 79.34%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.83%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 79.34%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 79.34%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.83%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 79.34%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.51%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 79.34%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 79.34%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 79.34%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.46%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 79.34%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 79.34%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.14%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 79.34%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 79.34%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.56%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 79.34%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 79.34%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.46%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 79.34%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 79.34%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.93%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 79.34%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.78%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 79.34%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.61%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 79.34%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.88%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 79.34%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.72%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 79.34%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.61%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 79.34%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.88%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 79.34%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.88%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 79.34%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.83%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 79.34%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 79.34%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.78%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 79.34%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.51%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 79.34%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.14%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 79.34%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.46%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 79.34%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.14%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 79.34%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.78%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 79.34%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.61%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 79.34%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.14%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 79.34%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 79.34%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.98%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 79.34%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.98%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 79.34%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.83%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 79.34%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.14%\n",
      "\t Val. Loss: 1.115 |  Val. Acc: 15.31%\n",
      "\tTrain Loss: 1.098 | Train Acc: 35.14%\n",
      "\t Val. Loss: 1.115 |  Val. Acc: 15.31%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.115 |  Val. Acc: 15.31%\n",
      "\tTrain Loss: 1.098 | Train Acc: 31.78%\n",
      "\t Val. Loss: 1.115 |  Val. Acc: 15.31%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.46%\n",
      "\t Val. Loss: 1.115 |  Val. Acc: 15.31%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.115 |  Val. Acc: 15.31%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.56%\n",
      "\t Val. Loss: 1.115 |  Val. Acc: 15.31%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.14%\n",
      "\t Val. Loss: 1.115 |  Val. Acc: 15.31%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.51%\n",
      "\t Val. Loss: 1.115 |  Val. Acc: 15.31%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.51%\n",
      "\t Val. Loss: 1.115 |  Val. Acc: 15.31%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.77%\n",
      "\t Val. Loss: 1.115 |  Val. Acc: 15.31%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.78%\n",
      "\t Val. Loss: 1.115 |  Val. Acc: 15.31%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.03%\n",
      "\t Val. Loss: 1.115 |  Val. Acc: 15.31%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.25%\n",
      "\t Val. Loss: 1.115 |  Val. Acc: 15.31%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.83%\n",
      "\t Val. Loss: 1.115 |  Val. Acc: 15.31%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.93%\n",
      "\t Val. Loss: 1.115 |  Val. Acc: 15.31%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.56%\n",
      "\t Val. Loss: 1.115 |  Val. Acc: 15.31%\n",
      "\tTrain Loss: 1.098 | Train Acc: 31.88%\n",
      "\t Val. Loss: 1.115 |  Val. Acc: 15.31%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.56%\n",
      "\t Val. Loss: 1.115 |  Val. Acc: 15.31%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.88%\n",
      "\t Val. Loss: 1.115 |  Val. Acc: 15.31%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.30%\n",
      "\t Val. Loss: 1.115 |  Val. Acc: 15.31%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.03%\n",
      "\t Val. Loss: 1.115 |  Val. Acc: 15.31%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.72%\n",
      "\t Val. Loss: 1.115 |  Val. Acc: 15.31%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.93%\n",
      "\t Val. Loss: 1.115 |  Val. Acc: 15.31%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.30%\n",
      "\t Val. Loss: 1.115 |  Val. Acc: 15.31%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.40%\n",
      "\t Val. Loss: 1.115 |  Val. Acc: 15.31%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.51%\n",
      "\t Val. Loss: 1.115 |  Val. Acc: 15.31%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.88%\n",
      "\t Val. Loss: 1.115 |  Val. Acc: 15.31%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.88%\n",
      "\t Val. Loss: 1.115 |  Val. Acc: 15.31%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.93%\n",
      "\t Val. Loss: 1.115 |  Val. Acc: 15.31%\n",
      "\tTrain Loss: 1.098 | Train Acc: 31.51%\n",
      "\t Val. Loss: 1.115 |  Val. Acc: 15.31%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.35%\n",
      "\t Val. Loss: 1.115 |  Val. Acc: 15.31%\n",
      "\tTrain Loss: 1.098 | Train Acc: 31.78%\n",
      "\t Val. Loss: 1.115 |  Val. Acc: 15.31%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.61%\n",
      "\t Val. Loss: 1.115 |  Val. Acc: 15.31%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.40%\n",
      "\t Val. Loss: 1.115 |  Val. Acc: 15.31%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.04%\n",
      "\t Val. Loss: 1.115 |  Val. Acc: 15.31%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.20%\n",
      "\t Val. Loss: 1.115 |  Val. Acc: 15.31%\n",
      "\tTrain Loss: 1.098 | Train Acc: 31.78%\n",
      "\t Val. Loss: 1.115 |  Val. Acc: 15.31%\n",
      "\tTrain Loss: 1.098 | Train Acc: 35.19%\n",
      "\t Val. Loss: 1.115 |  Val. Acc: 15.31%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.03%\n",
      "\t Val. Loss: 1.115 |  Val. Acc: 15.31%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.04%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 2.68%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.14%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 2.68%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.14%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 2.68%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.51%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 2.68%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.77%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 2.68%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.51%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 2.68%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.93%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 2.68%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.24%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 2.68%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.88%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 2.68%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.51%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 2.68%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.14%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 2.68%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.77%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 2.68%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.93%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 2.68%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.04%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 2.68%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.51%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 2.68%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.88%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 2.68%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.14%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 2.68%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.56%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 2.68%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.30%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 2.68%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 2.68%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.83%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 2.68%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.51%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 2.68%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.51%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 2.68%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.41%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 2.68%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.83%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 2.68%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.51%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 2.68%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.56%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 2.68%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.77%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 2.68%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.51%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 2.68%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.14%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 2.68%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.88%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 2.68%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.77%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 2.68%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.83%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 2.68%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.93%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 2.68%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.88%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 2.68%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 2.68%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.93%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 2.68%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.77%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 2.68%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.83%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 2.68%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.56%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 2.68%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.46%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.14%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.56%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.41%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.14%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.14%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.41%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.09%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.77%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.51%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.77%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.77%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.77%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.56%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.83%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.14%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.51%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.14%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.77%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.77%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.04%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.93%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.77%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.77%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.14%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.56%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.35%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.14%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.14%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.77%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.83%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.83%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.14%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.41%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.56%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.83%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.098 | Train Acc: 40.18%\n",
      "\t Val. Loss: 1.096 |  Val. Acc: 20.66%\n",
      "\tTrain Loss: 1.098 | Train Acc: 40.07%\n",
      "\t Val. Loss: 1.096 |  Val. Acc: 20.66%\n",
      "\tTrain Loss: 1.098 | Train Acc: 36.40%\n",
      "\t Val. Loss: 1.096 |  Val. Acc: 20.66%\n",
      "\tTrain Loss: 1.098 | Train Acc: 36.87%\n",
      "\t Val. Loss: 1.096 |  Val. Acc: 20.66%\n",
      "\tTrain Loss: 1.098 | Train Acc: 37.87%\n",
      "\t Val. Loss: 1.096 |  Val. Acc: 20.66%\n",
      "\tTrain Loss: 1.098 | Train Acc: 37.13%\n",
      "\t Val. Loss: 1.096 |  Val. Acc: 20.66%\n",
      "\tTrain Loss: 1.098 | Train Acc: 38.60%\n",
      "\t Val. Loss: 1.096 |  Val. Acc: 20.66%\n",
      "\tTrain Loss: 1.098 | Train Acc: 39.34%\n",
      "\t Val. Loss: 1.096 |  Val. Acc: 20.66%\n",
      "\tTrain Loss: 1.098 | Train Acc: 38.29%\n",
      "\t Val. Loss: 1.096 |  Val. Acc: 20.66%\n",
      "\tTrain Loss: 1.098 | Train Acc: 40.91%\n",
      "\t Val. Loss: 1.096 |  Val. Acc: 20.66%\n",
      "\tTrain Loss: 1.098 | Train Acc: 38.29%\n",
      "\t Val. Loss: 1.096 |  Val. Acc: 20.66%\n",
      "\tTrain Loss: 1.098 | Train Acc: 39.81%\n",
      "\t Val. Loss: 1.096 |  Val. Acc: 20.66%\n",
      "\tTrain Loss: 1.098 | Train Acc: 39.86%\n",
      "\t Val. Loss: 1.096 |  Val. Acc: 20.66%\n",
      "\tTrain Loss: 1.098 | Train Acc: 37.13%\n",
      "\t Val. Loss: 1.096 |  Val. Acc: 20.66%\n",
      "\tTrain Loss: 1.098 | Train Acc: 37.97%\n",
      "\t Val. Loss: 1.096 |  Val. Acc: 20.66%\n",
      "\tTrain Loss: 1.098 | Train Acc: 39.71%\n",
      "\t Val. Loss: 1.096 |  Val. Acc: 20.66%\n",
      "\tTrain Loss: 1.098 | Train Acc: 40.44%\n",
      "\t Val. Loss: 1.096 |  Val. Acc: 20.66%\n",
      "\tTrain Loss: 1.098 | Train Acc: 41.54%\n",
      "\t Val. Loss: 1.096 |  Val. Acc: 20.66%\n",
      "\tTrain Loss: 1.098 | Train Acc: 37.50%\n",
      "\t Val. Loss: 1.096 |  Val. Acc: 20.66%\n",
      "\tTrain Loss: 1.098 | Train Acc: 37.55%\n",
      "\t Val. Loss: 1.096 |  Val. Acc: 20.66%\n",
      "\tTrain Loss: 1.097 | Train Acc: 39.34%\n",
      "\t Val. Loss: 1.096 |  Val. Acc: 20.66%\n",
      "\tTrain Loss: 1.098 | Train Acc: 39.44%\n",
      "\t Val. Loss: 1.096 |  Val. Acc: 20.66%\n",
      "\tTrain Loss: 1.098 | Train Acc: 38.34%\n",
      "\t Val. Loss: 1.096 |  Val. Acc: 20.66%\n",
      "\tTrain Loss: 1.098 | Train Acc: 36.45%\n",
      "\t Val. Loss: 1.096 |  Val. Acc: 20.66%\n",
      "\tTrain Loss: 1.098 | Train Acc: 37.92%\n",
      "\t Val. Loss: 1.096 |  Val. Acc: 20.66%\n",
      "\tTrain Loss: 1.098 | Train Acc: 38.24%\n",
      "\t Val. Loss: 1.096 |  Val. Acc: 20.66%\n",
      "\tTrain Loss: 1.098 | Train Acc: 36.76%\n",
      "\t Val. Loss: 1.096 |  Val. Acc: 20.66%\n",
      "\tTrain Loss: 1.098 | Train Acc: 37.13%\n",
      "\t Val. Loss: 1.096 |  Val. Acc: 20.66%\n",
      "\tTrain Loss: 1.098 | Train Acc: 36.34%\n",
      "\t Val. Loss: 1.096 |  Val. Acc: 20.66%\n",
      "\tTrain Loss: 1.098 | Train Acc: 37.97%\n",
      "\t Val. Loss: 1.096 |  Val. Acc: 20.66%\n",
      "\tTrain Loss: 1.098 | Train Acc: 38.97%\n",
      "\t Val. Loss: 1.096 |  Val. Acc: 20.66%\n",
      "\tTrain Loss: 1.098 | Train Acc: 37.92%\n",
      "\t Val. Loss: 1.096 |  Val. Acc: 20.66%\n",
      "\tTrain Loss: 1.098 | Train Acc: 37.50%\n",
      "\t Val. Loss: 1.096 |  Val. Acc: 20.66%\n",
      "\tTrain Loss: 1.098 | Train Acc: 45.01%\n",
      "\t Val. Loss: 1.096 |  Val. Acc: 20.66%\n",
      "\tTrain Loss: 1.098 | Train Acc: 37.97%\n",
      "\t Val. Loss: 1.096 |  Val. Acc: 20.66%\n",
      "\tTrain Loss: 1.098 | Train Acc: 39.71%\n",
      "\t Val. Loss: 1.096 |  Val. Acc: 20.66%\n",
      "\tTrain Loss: 1.098 | Train Acc: 36.40%\n",
      "\t Val. Loss: 1.096 |  Val. Acc: 20.66%\n",
      "\tTrain Loss: 1.098 | Train Acc: 35.77%\n",
      "\t Val. Loss: 1.096 |  Val. Acc: 20.66%\n",
      "\tTrain Loss: 1.098 | Train Acc: 40.86%\n",
      "\t Val. Loss: 1.096 |  Val. Acc: 20.66%\n",
      "\tTrain Loss: 1.098 | Train Acc: 37.55%\n",
      "\t Val. Loss: 1.096 |  Val. Acc: 20.66%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.88%\n",
      "\t Val. Loss: 1.123 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.24%\n",
      "\t Val. Loss: 1.123 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.98%\n",
      "\t Val. Loss: 1.123 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.88%\n",
      "\t Val. Loss: 1.123 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.14%\n",
      "\t Val. Loss: 1.123 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.93%\n",
      "\t Val. Loss: 1.123 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.24%\n",
      "\t Val. Loss: 1.123 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.51%\n",
      "\t Val. Loss: 1.123 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.14%\n",
      "\t Val. Loss: 1.123 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.83%\n",
      "\t Val. Loss: 1.123 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.14%\n",
      "\t Val. Loss: 1.123 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.88%\n",
      "\t Val. Loss: 1.123 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.14%\n",
      "\t Val. Loss: 1.123 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.93%\n",
      "\t Val. Loss: 1.123 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.88%\n",
      "\t Val. Loss: 1.123 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.98%\n",
      "\t Val. Loss: 1.123 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.123 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.14%\n",
      "\t Val. Loss: 1.123 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.66%\n",
      "\t Val. Loss: 1.123 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.51%\n",
      "\t Val. Loss: 1.123 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.88%\n",
      "\t Val. Loss: 1.123 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.51%\n",
      "\t Val. Loss: 1.123 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.61%\n",
      "\t Val. Loss: 1.123 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.14%\n",
      "\t Val. Loss: 1.123 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.41%\n",
      "\t Val. Loss: 1.123 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.93%\n",
      "\t Val. Loss: 1.123 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.88%\n",
      "\t Val. Loss: 1.123 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.56%\n",
      "\t Val. Loss: 1.123 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.41%\n",
      "\t Val. Loss: 1.123 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.83%\n",
      "\t Val. Loss: 1.123 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.41%\n",
      "\t Val. Loss: 1.123 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.93%\n",
      "\t Val. Loss: 1.123 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.82%\n",
      "\t Val. Loss: 1.123 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.67%\n",
      "\t Val. Loss: 1.123 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.88%\n",
      "\t Val. Loss: 1.123 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.24%\n",
      "\t Val. Loss: 1.123 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.30%\n",
      "\t Val. Loss: 1.123 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.51%\n",
      "\t Val. Loss: 1.123 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.123 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.24%\n",
      "\t Val. Loss: 1.123 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.98%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.88%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.72%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.88%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.93%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.51%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.98%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.51%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.51%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.51%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.61%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.30%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.88%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.41%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.40%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 35.40%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.46%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.41%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.98%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.98%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.83%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.61%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.72%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.098 | Train Acc: 35.77%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.35%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.30%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.30%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.56%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.098 | Train Acc: 32.72%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.24%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.88%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.72%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.14%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.098 | Train Acc: 35.35%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.61%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.56%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.24%\n",
      "\t Val. Loss: 1.102 |  Val. Acc: 3.57%\n",
      "\tTrain Loss: 1.099 | Train Acc: 35.35%\n",
      "\t Val. Loss: 1.102 |  Val. Acc: 3.57%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.77%\n",
      "\t Val. Loss: 1.102 |  Val. Acc: 3.57%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.61%\n",
      "\t Val. Loss: 1.102 |  Val. Acc: 3.57%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.51%\n",
      "\t Val. Loss: 1.102 |  Val. Acc: 3.57%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.51%\n",
      "\t Val. Loss: 1.102 |  Val. Acc: 3.57%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.14%\n",
      "\t Val. Loss: 1.102 |  Val. Acc: 3.57%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.77%\n",
      "\t Val. Loss: 1.102 |  Val. Acc: 3.57%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.98%\n",
      "\t Val. Loss: 1.102 |  Val. Acc: 3.57%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.77%\n",
      "\t Val. Loss: 1.102 |  Val. Acc: 3.57%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.61%\n",
      "\t Val. Loss: 1.102 |  Val. Acc: 3.57%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.82%\n",
      "\t Val. Loss: 1.102 |  Val. Acc: 3.57%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.93%\n",
      "\t Val. Loss: 1.102 |  Val. Acc: 3.57%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.99%\n",
      "\t Val. Loss: 1.102 |  Val. Acc: 3.57%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.40%\n",
      "\t Val. Loss: 1.102 |  Val. Acc: 3.57%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.35%\n",
      "\t Val. Loss: 1.102 |  Val. Acc: 3.57%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.88%\n",
      "\t Val. Loss: 1.102 |  Val. Acc: 3.57%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.46%\n",
      "\t Val. Loss: 1.102 |  Val. Acc: 3.57%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.61%\n",
      "\t Val. Loss: 1.102 |  Val. Acc: 3.57%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.51%\n",
      "\t Val. Loss: 1.102 |  Val. Acc: 3.57%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.04%\n",
      "\t Val. Loss: 1.102 |  Val. Acc: 3.57%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.66%\n",
      "\t Val. Loss: 1.102 |  Val. Acc: 3.57%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.30%\n",
      "\t Val. Loss: 1.102 |  Val. Acc: 3.57%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.102 |  Val. Acc: 3.57%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.46%\n",
      "\t Val. Loss: 1.102 |  Val. Acc: 3.57%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.46%\n",
      "\t Val. Loss: 1.102 |  Val. Acc: 3.57%\n",
      "\tTrain Loss: 1.099 | Train Acc: 35.66%\n",
      "\t Val. Loss: 1.102 |  Val. Acc: 3.57%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.51%\n",
      "\t Val. Loss: 1.102 |  Val. Acc: 3.57%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.66%\n",
      "\t Val. Loss: 1.102 |  Val. Acc: 3.57%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.88%\n",
      "\t Val. Loss: 1.102 |  Val. Acc: 3.57%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.24%\n",
      "\t Val. Loss: 1.102 |  Val. Acc: 3.57%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.46%\n",
      "\t Val. Loss: 1.102 |  Val. Acc: 3.57%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.82%\n",
      "\t Val. Loss: 1.102 |  Val. Acc: 3.57%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.77%\n",
      "\t Val. Loss: 1.102 |  Val. Acc: 3.57%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.88%\n",
      "\t Val. Loss: 1.102 |  Val. Acc: 3.57%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.93%\n",
      "\t Val. Loss: 1.102 |  Val. Acc: 3.57%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.99%\n",
      "\t Val. Loss: 1.102 |  Val. Acc: 3.57%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.24%\n",
      "\t Val. Loss: 1.102 |  Val. Acc: 3.57%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.61%\n",
      "\t Val. Loss: 1.102 |  Val. Acc: 3.57%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.98%\n",
      "\t Val. Loss: 1.102 |  Val. Acc: 3.57%\n",
      "\tTrain Loss: 1.100 | Train Acc: 28.83%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.71%\n",
      "\tTrain Loss: 1.100 | Train Acc: 27.78%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.71%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.56%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.71%\n",
      "\tTrain Loss: 1.100 | Train Acc: 30.25%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.71%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.56%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.71%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.46%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.71%\n",
      "\tTrain Loss: 1.100 | Train Acc: 30.99%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.71%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.41%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.71%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.25%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.71%\n",
      "\tTrain Loss: 1.100 | Train Acc: 30.57%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.71%\n",
      "\tTrain Loss: 1.100 | Train Acc: 29.20%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.71%\n",
      "\tTrain Loss: 1.100 | Train Acc: 29.62%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.71%\n",
      "\tTrain Loss: 1.100 | Train Acc: 31.04%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.71%\n",
      "\tTrain Loss: 1.100 | Train Acc: 30.67%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.71%\n",
      "\tTrain Loss: 1.100 | Train Acc: 31.83%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.71%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.71%\n",
      "\tTrain Loss: 1.100 | Train Acc: 30.99%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.71%\n",
      "\tTrain Loss: 1.100 | Train Acc: 31.46%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.71%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.83%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.71%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.36%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.71%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.93%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.71%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.41%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.71%\n",
      "\tTrain Loss: 1.100 | Train Acc: 30.67%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.71%\n",
      "\tTrain Loss: 1.100 | Train Acc: 31.04%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.71%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.46%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.71%\n",
      "\tTrain Loss: 1.100 | Train Acc: 30.72%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.71%\n",
      "\tTrain Loss: 1.100 | Train Acc: 29.57%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.71%\n",
      "\tTrain Loss: 1.100 | Train Acc: 30.67%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.71%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.93%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.71%\n",
      "\tTrain Loss: 1.100 | Train Acc: 30.67%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.71%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.30%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.71%\n",
      "\tTrain Loss: 1.100 | Train Acc: 33.77%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.71%\n",
      "\tTrain Loss: 1.100 | Train Acc: 31.72%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.71%\n",
      "\tTrain Loss: 1.100 | Train Acc: 30.30%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.71%\n",
      "\tTrain Loss: 1.100 | Train Acc: 31.72%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.71%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.04%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.71%\n",
      "\tTrain Loss: 1.099 | Train Acc: 29.57%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.71%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.09%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.71%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.46%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.71%\n",
      "\tTrain Loss: 1.100 | Train Acc: 30.04%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 16.71%\n",
      "\tTrain Loss: 1.100 | Train Acc: 33.67%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.61%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.30%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.30%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.98%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.61%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.100 | Train Acc: 33.61%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.61%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.100 | Train Acc: 32.88%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.100 | Train Acc: 33.61%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.100 | Train Acc: 33.30%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.61%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.40%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.30%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.100 | Train Acc: 33.61%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.100 | Train Acc: 33.61%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.61%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.30%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.61%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.67%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.61%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.100 | Train Acc: 33.61%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.100 | Train Acc: 33.67%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.100 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.30%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.100 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.61%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.30%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.67%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.61%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.35%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.100 | Train Acc: 33.67%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.98%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 16.65%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.56%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 1.34%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.61%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 1.34%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.93%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 1.34%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.51%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 1.34%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.61%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 1.34%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.98%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 1.34%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.93%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 1.34%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.88%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 1.34%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.24%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 1.34%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.14%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 1.34%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.61%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 1.34%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.66%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 1.34%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.51%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 1.34%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.88%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 1.34%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.98%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 1.34%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.51%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 1.34%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.72%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 1.34%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.93%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 1.34%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.14%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 1.34%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.30%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 1.34%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.88%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 1.34%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.88%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 1.34%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.30%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 1.34%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.24%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 1.34%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.24%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 1.34%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.83%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 1.34%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.51%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 1.34%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.24%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 1.34%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.30%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 1.34%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.24%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 1.34%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.24%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 1.34%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.88%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 1.34%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.51%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 1.34%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.30%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 1.34%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.51%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 1.34%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.30%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 1.34%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.24%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 1.34%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.61%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 1.34%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.88%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 1.34%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.88%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 1.34%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.09%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.72%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.099 | Train Acc: 35.35%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.46%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.98%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.30%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.62%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.099 | Train Acc: 36.40%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.51%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.04%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.82%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.56%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.61%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.51%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.35%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.67%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.099 | Train Acc: 35.98%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.30%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.93%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.20%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.24%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.72%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.67%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.51%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.46%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.35%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.099 | Train Acc: 35.03%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.56%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.099 | Train Acc: 35.35%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.099 | Train Acc: 30.83%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.62%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.099 | Train Acc: 35.35%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.72%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.67%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.77%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.099 | Train Acc: 36.34%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.099 | Train Acc: 36.03%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.46%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.09%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.19%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 4.15%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.14%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 22.45%\n",
      "\tTrain Loss: 1.100 | Train Acc: 31.46%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 22.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.14%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 22.45%\n",
      "\tTrain Loss: 1.100 | Train Acc: 32.98%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 22.45%\n",
      "\tTrain Loss: 1.100 | Train Acc: 31.51%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 22.45%\n",
      "\tTrain Loss: 1.100 | Train Acc: 33.72%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 22.45%\n",
      "\tTrain Loss: 1.100 | Train Acc: 30.41%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 22.45%\n",
      "\tTrain Loss: 1.100 | Train Acc: 31.83%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 22.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.20%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 22.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 29.99%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 22.45%\n",
      "\tTrain Loss: 1.100 | Train Acc: 31.36%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 22.45%\n",
      "\tTrain Loss: 1.100 | Train Acc: 29.99%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 22.45%\n",
      "\tTrain Loss: 1.100 | Train Acc: 31.46%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 22.45%\n",
      "\tTrain Loss: 1.100 | Train Acc: 30.72%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 22.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.88%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 22.45%\n",
      "\tTrain Loss: 1.100 | Train Acc: 32.20%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 22.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.20%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 22.45%\n",
      "\tTrain Loss: 1.100 | Train Acc: 31.41%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 22.45%\n",
      "\tTrain Loss: 1.100 | Train Acc: 31.09%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 22.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.83%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 22.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 29.62%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 22.45%\n",
      "\tTrain Loss: 1.100 | Train Acc: 31.83%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 22.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.83%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 22.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 30.41%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 22.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 30.30%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 22.45%\n",
      "\tTrain Loss: 1.100 | Train Acc: 28.89%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 22.45%\n",
      "\tTrain Loss: 1.100 | Train Acc: 30.30%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 22.45%\n",
      "\tTrain Loss: 1.100 | Train Acc: 29.20%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 22.45%\n",
      "\tTrain Loss: 1.100 | Train Acc: 29.99%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 22.45%\n",
      "\tTrain Loss: 1.100 | Train Acc: 31.78%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 22.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.88%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 22.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.04%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 22.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.83%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 22.45%\n",
      "\tTrain Loss: 1.100 | Train Acc: 29.99%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 22.45%\n",
      "\tTrain Loss: 1.100 | Train Acc: 31.78%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 22.45%\n",
      "\tTrain Loss: 1.100 | Train Acc: 31.46%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 22.45%\n",
      "\tTrain Loss: 1.100 | Train Acc: 32.20%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 22.45%\n",
      "\tTrain Loss: 1.100 | Train Acc: 31.51%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 22.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 28.89%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 22.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 30.36%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 22.45%\n",
      "\tTrain Loss: 1.100 | Train Acc: 29.62%\n",
      "\t Val. Loss: 1.091 |  Val. Acc: 67.09%\n",
      "\tTrain Loss: 1.100 | Train Acc: 29.15%\n",
      "\t Val. Loss: 1.091 |  Val. Acc: 67.09%\n",
      "\tTrain Loss: 1.100 | Train Acc: 28.89%\n",
      "\t Val. Loss: 1.091 |  Val. Acc: 67.09%\n",
      "\tTrain Loss: 1.099 | Train Acc: 29.62%\n",
      "\t Val. Loss: 1.091 |  Val. Acc: 67.09%\n",
      "\tTrain Loss: 1.100 | Train Acc: 30.62%\n",
      "\t Val. Loss: 1.091 |  Val. Acc: 67.09%\n",
      "\tTrain Loss: 1.100 | Train Acc: 29.20%\n",
      "\t Val. Loss: 1.091 |  Val. Acc: 67.09%\n",
      "\tTrain Loss: 1.100 | Train Acc: 30.25%\n",
      "\t Val. Loss: 1.091 |  Val. Acc: 67.09%\n",
      "\tTrain Loss: 1.100 | Train Acc: 29.83%\n",
      "\t Val. Loss: 1.091 |  Val. Acc: 67.09%\n",
      "\tTrain Loss: 1.100 | Train Acc: 29.94%\n",
      "\t Val. Loss: 1.091 |  Val. Acc: 67.09%\n",
      "\tTrain Loss: 1.100 | Train Acc: 29.94%\n",
      "\t Val. Loss: 1.091 |  Val. Acc: 67.09%\n",
      "\tTrain Loss: 1.100 | Train Acc: 29.31%\n",
      "\t Val. Loss: 1.091 |  Val. Acc: 67.09%\n",
      "\tTrain Loss: 1.100 | Train Acc: 31.46%\n",
      "\t Val. Loss: 1.091 |  Val. Acc: 67.09%\n",
      "\tTrain Loss: 1.100 | Train Acc: 31.09%\n",
      "\t Val. Loss: 1.091 |  Val. Acc: 67.09%\n",
      "\tTrain Loss: 1.100 | Train Acc: 30.62%\n",
      "\t Val. Loss: 1.091 |  Val. Acc: 67.09%\n",
      "\tTrain Loss: 1.100 | Train Acc: 28.78%\n",
      "\t Val. Loss: 1.091 |  Val. Acc: 67.09%\n",
      "\tTrain Loss: 1.100 | Train Acc: 30.30%\n",
      "\t Val. Loss: 1.091 |  Val. Acc: 67.09%\n",
      "\tTrain Loss: 1.100 | Train Acc: 29.57%\n",
      "\t Val. Loss: 1.091 |  Val. Acc: 67.09%\n",
      "\tTrain Loss: 1.100 | Train Acc: 33.30%\n",
      "\t Val. Loss: 1.091 |  Val. Acc: 67.09%\n",
      "\tTrain Loss: 1.100 | Train Acc: 31.78%\n",
      "\t Val. Loss: 1.091 |  Val. Acc: 67.09%\n",
      "\tTrain Loss: 1.100 | Train Acc: 30.30%\n",
      "\t Val. Loss: 1.091 |  Val. Acc: 67.09%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.41%\n",
      "\t Val. Loss: 1.091 |  Val. Acc: 67.09%\n",
      "\tTrain Loss: 1.099 | Train Acc: 28.47%\n",
      "\t Val. Loss: 1.091 |  Val. Acc: 67.09%\n",
      "\tTrain Loss: 1.100 | Train Acc: 31.41%\n",
      "\t Val. Loss: 1.091 |  Val. Acc: 67.09%\n",
      "\tTrain Loss: 1.100 | Train Acc: 33.30%\n",
      "\t Val. Loss: 1.091 |  Val. Acc: 67.09%\n",
      "\tTrain Loss: 1.100 | Train Acc: 29.99%\n",
      "\t Val. Loss: 1.091 |  Val. Acc: 67.09%\n",
      "\tTrain Loss: 1.100 | Train Acc: 31.09%\n",
      "\t Val. Loss: 1.091 |  Val. Acc: 67.09%\n",
      "\tTrain Loss: 1.100 | Train Acc: 30.67%\n",
      "\t Val. Loss: 1.091 |  Val. Acc: 67.09%\n",
      "\tTrain Loss: 1.100 | Train Acc: 32.51%\n",
      "\t Val. Loss: 1.091 |  Val. Acc: 67.09%\n",
      "\tTrain Loss: 1.100 | Train Acc: 28.78%\n",
      "\t Val. Loss: 1.091 |  Val. Acc: 67.09%\n",
      "\tTrain Loss: 1.100 | Train Acc: 28.10%\n",
      "\t Val. Loss: 1.091 |  Val. Acc: 67.09%\n",
      "\tTrain Loss: 1.100 | Train Acc: 33.98%\n",
      "\t Val. Loss: 1.091 |  Val. Acc: 67.09%\n",
      "\tTrain Loss: 1.100 | Train Acc: 27.78%\n",
      "\t Val. Loss: 1.091 |  Val. Acc: 67.09%\n",
      "\tTrain Loss: 1.100 | Train Acc: 30.04%\n",
      "\t Val. Loss: 1.091 |  Val. Acc: 67.09%\n",
      "\tTrain Loss: 1.100 | Train Acc: 30.62%\n",
      "\t Val. Loss: 1.091 |  Val. Acc: 67.09%\n",
      "\tTrain Loss: 1.100 | Train Acc: 28.89%\n",
      "\t Val. Loss: 1.091 |  Val. Acc: 67.09%\n",
      "\tTrain Loss: 1.100 | Train Acc: 31.83%\n",
      "\t Val. Loss: 1.091 |  Val. Acc: 67.09%\n",
      "\tTrain Loss: 1.100 | Train Acc: 27.00%\n",
      "\t Val. Loss: 1.091 |  Val. Acc: 67.09%\n",
      "\tTrain Loss: 1.100 | Train Acc: 29.25%\n",
      "\t Val. Loss: 1.091 |  Val. Acc: 67.09%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.20%\n",
      "\t Val. Loss: 1.091 |  Val. Acc: 67.09%\n",
      "\tTrain Loss: 1.100 | Train Acc: 29.57%\n",
      "\t Val. Loss: 1.091 |  Val. Acc: 67.09%\n",
      "\tTrain Loss: 1.099 | Train Acc: 35.03%\n",
      "\t Val. Loss: 1.089 |  Val. Acc: 64.92%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.45%\n",
      "\t Val. Loss: 1.089 |  Val. Acc: 64.92%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.78%\n",
      "\t Val. Loss: 1.089 |  Val. Acc: 64.92%\n",
      "\tTrain Loss: 1.099 | Train Acc: 35.03%\n",
      "\t Val. Loss: 1.089 |  Val. Acc: 64.92%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.35%\n",
      "\t Val. Loss: 1.089 |  Val. Acc: 64.92%\n",
      "\tTrain Loss: 1.099 | Train Acc: 35.82%\n",
      "\t Val. Loss: 1.089 |  Val. Acc: 64.92%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.61%\n",
      "\t Val. Loss: 1.089 |  Val. Acc: 64.92%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.83%\n",
      "\t Val. Loss: 1.089 |  Val. Acc: 64.92%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.35%\n",
      "\t Val. Loss: 1.089 |  Val. Acc: 64.92%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.98%\n",
      "\t Val. Loss: 1.089 |  Val. Acc: 64.92%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.089 |  Val. Acc: 64.92%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.66%\n",
      "\t Val. Loss: 1.089 |  Val. Acc: 64.92%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.20%\n",
      "\t Val. Loss: 1.089 |  Val. Acc: 64.92%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.61%\n",
      "\t Val. Loss: 1.089 |  Val. Acc: 64.92%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.61%\n",
      "\t Val. Loss: 1.089 |  Val. Acc: 64.92%\n",
      "\tTrain Loss: 1.099 | Train Acc: 30.99%\n",
      "\t Val. Loss: 1.089 |  Val. Acc: 64.92%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.98%\n",
      "\t Val. Loss: 1.089 |  Val. Acc: 64.92%\n",
      "\tTrain Loss: 1.099 | Train Acc: 30.67%\n",
      "\t Val. Loss: 1.089 |  Val. Acc: 64.92%\n",
      "\tTrain Loss: 1.099 | Train Acc: 30.67%\n",
      "\t Val. Loss: 1.089 |  Val. Acc: 64.92%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.98%\n",
      "\t Val. Loss: 1.089 |  Val. Acc: 64.92%\n",
      "\tTrain Loss: 1.098 | Train Acc: 35.03%\n",
      "\t Val. Loss: 1.089 |  Val. Acc: 64.92%\n",
      "\tTrain Loss: 1.099 | Train Acc: 35.40%\n",
      "\t Val. Loss: 1.089 |  Val. Acc: 64.92%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.089 |  Val. Acc: 64.92%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.03%\n",
      "\t Val. Loss: 1.089 |  Val. Acc: 64.92%\n",
      "\tTrain Loss: 1.099 | Train Acc: 36.87%\n",
      "\t Val. Loss: 1.089 |  Val. Acc: 64.92%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.089 |  Val. Acc: 64.92%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.03%\n",
      "\t Val. Loss: 1.089 |  Val. Acc: 64.92%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.61%\n",
      "\t Val. Loss: 1.089 |  Val. Acc: 64.92%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.93%\n",
      "\t Val. Loss: 1.089 |  Val. Acc: 64.92%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.98%\n",
      "\t Val. Loss: 1.089 |  Val. Acc: 64.92%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.30%\n",
      "\t Val. Loss: 1.089 |  Val. Acc: 64.92%\n",
      "\tTrain Loss: 1.099 | Train Acc: 36.66%\n",
      "\t Val. Loss: 1.089 |  Val. Acc: 64.92%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.72%\n",
      "\t Val. Loss: 1.089 |  Val. Acc: 64.92%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.93%\n",
      "\t Val. Loss: 1.089 |  Val. Acc: 64.92%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.089 |  Val. Acc: 64.92%\n",
      "\tTrain Loss: 1.099 | Train Acc: 35.98%\n",
      "\t Val. Loss: 1.089 |  Val. Acc: 64.92%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.30%\n",
      "\t Val. Loss: 1.089 |  Val. Acc: 64.92%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.35%\n",
      "\t Val. Loss: 1.089 |  Val. Acc: 64.92%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.09%\n",
      "\t Val. Loss: 1.089 |  Val. Acc: 64.92%\n",
      "\tTrain Loss: 1.099 | Train Acc: 35.14%\n",
      "\t Val. Loss: 1.089 |  Val. Acc: 64.92%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.09%\n",
      "\t Val. Loss: 1.094 |  Val. Acc: 54.02%\n",
      "\tTrain Loss: 1.099 | Train Acc: 35.14%\n",
      "\t Val. Loss: 1.094 |  Val. Acc: 54.02%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.25%\n",
      "\t Val. Loss: 1.094 |  Val. Acc: 54.02%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.88%\n",
      "\t Val. Loss: 1.094 |  Val. Acc: 54.02%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.83%\n",
      "\t Val. Loss: 1.094 |  Val. Acc: 54.02%\n",
      "\tTrain Loss: 1.099 | Train Acc: 30.99%\n",
      "\t Val. Loss: 1.094 |  Val. Acc: 54.02%\n",
      "\tTrain Loss: 1.100 | Train Acc: 27.73%\n",
      "\t Val. Loss: 1.094 |  Val. Acc: 54.02%\n",
      "\tTrain Loss: 1.099 | Train Acc: 29.94%\n",
      "\t Val. Loss: 1.094 |  Val. Acc: 54.02%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.30%\n",
      "\t Val. Loss: 1.094 |  Val. Acc: 54.02%\n",
      "\tTrain Loss: 1.099 | Train Acc: 29.57%\n",
      "\t Val. Loss: 1.094 |  Val. Acc: 54.02%\n",
      "\tTrain Loss: 1.100 | Train Acc: 28.73%\n",
      "\t Val. Loss: 1.094 |  Val. Acc: 54.02%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.88%\n",
      "\t Val. Loss: 1.094 |  Val. Acc: 54.02%\n",
      "\tTrain Loss: 1.099 | Train Acc: 30.25%\n",
      "\t Val. Loss: 1.094 |  Val. Acc: 54.02%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.09%\n",
      "\t Val. Loss: 1.094 |  Val. Acc: 54.02%\n",
      "\tTrain Loss: 1.099 | Train Acc: 28.10%\n",
      "\t Val. Loss: 1.094 |  Val. Acc: 54.02%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.04%\n",
      "\t Val. Loss: 1.094 |  Val. Acc: 54.02%\n",
      "\tTrain Loss: 1.099 | Train Acc: 28.52%\n",
      "\t Val. Loss: 1.094 |  Val. Acc: 54.02%\n",
      "\tTrain Loss: 1.099 | Train Acc: 30.67%\n",
      "\t Val. Loss: 1.094 |  Val. Acc: 54.02%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.45%\n",
      "\t Val. Loss: 1.094 |  Val. Acc: 54.02%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.094 |  Val. Acc: 54.02%\n",
      "\tTrain Loss: 1.099 | Train Acc: 28.89%\n",
      "\t Val. Loss: 1.094 |  Val. Acc: 54.02%\n",
      "\tTrain Loss: 1.099 | Train Acc: 30.99%\n",
      "\t Val. Loss: 1.094 |  Val. Acc: 54.02%\n",
      "\tTrain Loss: 1.099 | Train Acc: 29.57%\n",
      "\t Val. Loss: 1.094 |  Val. Acc: 54.02%\n",
      "\tTrain Loss: 1.100 | Train Acc: 31.41%\n",
      "\t Val. Loss: 1.094 |  Val. Acc: 54.02%\n",
      "\tTrain Loss: 1.100 | Train Acc: 29.57%\n",
      "\t Val. Loss: 1.094 |  Val. Acc: 54.02%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.78%\n",
      "\t Val. Loss: 1.094 |  Val. Acc: 54.02%\n",
      "\tTrain Loss: 1.100 | Train Acc: 31.41%\n",
      "\t Val. Loss: 1.094 |  Val. Acc: 54.02%\n",
      "\tTrain Loss: 1.099 | Train Acc: 29.99%\n",
      "\t Val. Loss: 1.094 |  Val. Acc: 54.02%\n",
      "\tTrain Loss: 1.099 | Train Acc: 29.10%\n",
      "\t Val. Loss: 1.094 |  Val. Acc: 54.02%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.14%\n",
      "\t Val. Loss: 1.094 |  Val. Acc: 54.02%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.51%\n",
      "\t Val. Loss: 1.094 |  Val. Acc: 54.02%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.93%\n",
      "\t Val. Loss: 1.094 |  Val. Acc: 54.02%\n",
      "\tTrain Loss: 1.099 | Train Acc: 29.94%\n",
      "\t Val. Loss: 1.094 |  Val. Acc: 54.02%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.93%\n",
      "\t Val. Loss: 1.094 |  Val. Acc: 54.02%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.41%\n",
      "\t Val. Loss: 1.094 |  Val. Acc: 54.02%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.30%\n",
      "\t Val. Loss: 1.094 |  Val. Acc: 54.02%\n",
      "\tTrain Loss: 1.099 | Train Acc: 29.94%\n",
      "\t Val. Loss: 1.094 |  Val. Acc: 54.02%\n",
      "\tTrain Loss: 1.099 | Train Acc: 29.99%\n",
      "\t Val. Loss: 1.094 |  Val. Acc: 54.02%\n",
      "\tTrain Loss: 1.099 | Train Acc: 35.45%\n",
      "\t Val. Loss: 1.094 |  Val. Acc: 54.02%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.51%\n",
      "\t Val. Loss: 1.094 |  Val. Acc: 54.02%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.45%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 14.86%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.03%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 14.86%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.51%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 14.86%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.25%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 14.86%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.40%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 14.86%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.20%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 14.86%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.56%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 14.86%\n",
      "\tTrain Loss: 1.098 | Train Acc: 35.56%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 14.86%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.56%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 14.86%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.77%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 14.86%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.61%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 14.86%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.09%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 14.86%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.67%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 14.86%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.09%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 14.86%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.51%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 14.86%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.98%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 14.86%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.20%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 14.86%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.14%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 14.86%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.67%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 14.86%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.45%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 14.86%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.09%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 14.86%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.51%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 14.86%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.56%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 14.86%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.61%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 14.86%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.35%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 14.86%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.09%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 14.86%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.56%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 14.86%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.51%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 14.86%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.88%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 14.86%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.82%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 14.86%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.56%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 14.86%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.67%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 14.86%\n",
      "\tTrain Loss: 1.099 | Train Acc: 30.67%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 14.86%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.72%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 14.86%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.40%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 14.86%\n",
      "\tTrain Loss: 1.099 | Train Acc: 35.50%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 14.86%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.93%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 14.86%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.40%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 14.86%\n",
      "\tTrain Loss: 1.099 | Train Acc: 35.14%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 14.86%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.30%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 14.86%\n",
      "\tTrain Loss: 1.099 | Train Acc: 35.50%\n",
      "\t Val. Loss: 1.110 |  Val. Acc: 14.41%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.51%\n",
      "\t Val. Loss: 1.110 |  Val. Acc: 14.41%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.30%\n",
      "\t Val. Loss: 1.110 |  Val. Acc: 14.41%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.35%\n",
      "\t Val. Loss: 1.110 |  Val. Acc: 14.41%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.36%\n",
      "\t Val. Loss: 1.110 |  Val. Acc: 14.41%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.87%\n",
      "\t Val. Loss: 1.110 |  Val. Acc: 14.41%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.03%\n",
      "\t Val. Loss: 1.110 |  Val. Acc: 14.41%\n",
      "\tTrain Loss: 1.099 | Train Acc: 35.92%\n",
      "\t Val. Loss: 1.110 |  Val. Acc: 14.41%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.14%\n",
      "\t Val. Loss: 1.110 |  Val. Acc: 14.41%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.14%\n",
      "\t Val. Loss: 1.110 |  Val. Acc: 14.41%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.61%\n",
      "\t Val. Loss: 1.110 |  Val. Acc: 14.41%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.09%\n",
      "\t Val. Loss: 1.110 |  Val. Acc: 14.41%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.72%\n",
      "\t Val. Loss: 1.110 |  Val. Acc: 14.41%\n",
      "\tTrain Loss: 1.099 | Train Acc: 29.94%\n",
      "\t Val. Loss: 1.110 |  Val. Acc: 14.41%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.93%\n",
      "\t Val. Loss: 1.110 |  Val. Acc: 14.41%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.93%\n",
      "\t Val. Loss: 1.110 |  Val. Acc: 14.41%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.46%\n",
      "\t Val. Loss: 1.110 |  Val. Acc: 14.41%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.51%\n",
      "\t Val. Loss: 1.110 |  Val. Acc: 14.41%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.78%\n",
      "\t Val. Loss: 1.110 |  Val. Acc: 14.41%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.45%\n",
      "\t Val. Loss: 1.110 |  Val. Acc: 14.41%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.20%\n",
      "\t Val. Loss: 1.110 |  Val. Acc: 14.41%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.35%\n",
      "\t Val. Loss: 1.110 |  Val. Acc: 14.41%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.56%\n",
      "\t Val. Loss: 1.110 |  Val. Acc: 14.41%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.35%\n",
      "\t Val. Loss: 1.110 |  Val. Acc: 14.41%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.51%\n",
      "\t Val. Loss: 1.110 |  Val. Acc: 14.41%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.14%\n",
      "\t Val. Loss: 1.110 |  Val. Acc: 14.41%\n",
      "\tTrain Loss: 1.099 | Train Acc: 36.34%\n",
      "\t Val. Loss: 1.110 |  Val. Acc: 14.41%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.25%\n",
      "\t Val. Loss: 1.110 |  Val. Acc: 14.41%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.98%\n",
      "\t Val. Loss: 1.110 |  Val. Acc: 14.41%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.93%\n",
      "\t Val. Loss: 1.110 |  Val. Acc: 14.41%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.88%\n",
      "\t Val. Loss: 1.110 |  Val. Acc: 14.41%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.03%\n",
      "\t Val. Loss: 1.110 |  Val. Acc: 14.41%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.04%\n",
      "\t Val. Loss: 1.110 |  Val. Acc: 14.41%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.83%\n",
      "\t Val. Loss: 1.110 |  Val. Acc: 14.41%\n",
      "\tTrain Loss: 1.099 | Train Acc: 30.72%\n",
      "\t Val. Loss: 1.110 |  Val. Acc: 14.41%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.93%\n",
      "\t Val. Loss: 1.110 |  Val. Acc: 14.41%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.09%\n",
      "\t Val. Loss: 1.110 |  Val. Acc: 14.41%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.45%\n",
      "\t Val. Loss: 1.110 |  Val. Acc: 14.41%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.25%\n",
      "\t Val. Loss: 1.110 |  Val. Acc: 14.41%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.40%\n",
      "\t Val. Loss: 1.110 |  Val. Acc: 14.41%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.51%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.88%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.98%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.93%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.51%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.51%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.30%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.51%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.51%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.56%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.88%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.93%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.24%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.56%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.24%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.56%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.66%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.83%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.88%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.14%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.51%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.66%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.098 | Train Acc: 34.61%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.14%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.93%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.88%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.14%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.88%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.51%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.51%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.51%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.14%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.66%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.51%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.88%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.88%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.56%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.24%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.66%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
      "\tTrain Loss: 1.099 | Train Acc: 30.46%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 20.60%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.35%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 20.60%\n",
      "\tTrain Loss: 1.099 | Train Acc: 29.83%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 20.60%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.98%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 20.60%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.93%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 20.60%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.62%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 20.60%\n",
      "\tTrain Loss: 1.100 | Train Acc: 32.35%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 20.60%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.46%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 20.60%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.09%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 20.60%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.30%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 20.60%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.25%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 20.60%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.56%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 20.60%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.77%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 20.60%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.51%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 20.60%\n",
      "\tTrain Loss: 1.099 | Train Acc: 30.41%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 20.60%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.25%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 20.60%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.56%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 20.60%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.25%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 20.60%\n",
      "\tTrain Loss: 1.099 | Train Acc: 30.83%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 20.60%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.09%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 20.60%\n",
      "\tTrain Loss: 1.099 | Train Acc: 30.78%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 20.60%\n",
      "\tTrain Loss: 1.099 | Train Acc: 30.78%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 20.60%\n",
      "\tTrain Loss: 1.099 | Train Acc: 30.41%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 20.60%\n",
      "\tTrain Loss: 1.100 | Train Acc: 29.83%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 20.60%\n",
      "\tTrain Loss: 1.099 | Train Acc: 30.51%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 20.60%\n",
      "\tTrain Loss: 1.099 | Train Acc: 29.57%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 20.60%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.20%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 20.60%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.25%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 20.60%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.30%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 20.60%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.40%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 20.60%\n",
      "\tTrain Loss: 1.099 | Train Acc: 28.05%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 20.60%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.20%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 20.60%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.62%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 20.60%\n",
      "\tTrain Loss: 1.100 | Train Acc: 29.62%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 20.60%\n",
      "\tTrain Loss: 1.099 | Train Acc: 30.09%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 20.60%\n",
      "\tTrain Loss: 1.099 | Train Acc: 35.71%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 20.60%\n",
      "\tTrain Loss: 1.099 | Train Acc: 30.83%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 20.60%\n",
      "\tTrain Loss: 1.099 | Train Acc: 30.88%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 20.60%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 20.60%\n",
      "\tTrain Loss: 1.099 | Train Acc: 30.25%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 20.60%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.51%\n",
      "\t Val. Loss: 1.113 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.93%\n",
      "\t Val. Loss: 1.113 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.88%\n",
      "\t Val. Loss: 1.113 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.88%\n",
      "\t Val. Loss: 1.113 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.14%\n",
      "\t Val. Loss: 1.113 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.56%\n",
      "\t Val. Loss: 1.113 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.56%\n",
      "\t Val. Loss: 1.113 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.51%\n",
      "\t Val. Loss: 1.113 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.14%\n",
      "\t Val. Loss: 1.113 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.93%\n",
      "\t Val. Loss: 1.113 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.51%\n",
      "\t Val. Loss: 1.113 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.19%\n",
      "\t Val. Loss: 1.113 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.93%\n",
      "\t Val. Loss: 1.113 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.14%\n",
      "\t Val. Loss: 1.113 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.88%\n",
      "\t Val. Loss: 1.113 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.51%\n",
      "\t Val. Loss: 1.113 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.51%\n",
      "\t Val. Loss: 1.113 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.88%\n",
      "\t Val. Loss: 1.113 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.51%\n",
      "\t Val. Loss: 1.113 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.24%\n",
      "\t Val. Loss: 1.113 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.51%\n",
      "\t Val. Loss: 1.113 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.77%\n",
      "\t Val. Loss: 1.113 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.56%\n",
      "\t Val. Loss: 1.113 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.88%\n",
      "\t Val. Loss: 1.113 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.77%\n",
      "\t Val. Loss: 1.113 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.77%\n",
      "\t Val. Loss: 1.113 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.56%\n",
      "\t Val. Loss: 1.113 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.098 | Train Acc: 33.56%\n",
      "\t Val. Loss: 1.113 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.14%\n",
      "\t Val. Loss: 1.113 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.100 | Train Acc: 32.77%\n",
      "\t Val. Loss: 1.113 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.93%\n",
      "\t Val. Loss: 1.113 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.14%\n",
      "\t Val. Loss: 1.113 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.88%\n",
      "\t Val. Loss: 1.113 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.51%\n",
      "\t Val. Loss: 1.113 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.51%\n",
      "\t Val. Loss: 1.113 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.93%\n",
      "\t Val. Loss: 1.113 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.51%\n",
      "\t Val. Loss: 1.113 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.88%\n",
      "\t Val. Loss: 1.113 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.88%\n",
      "\t Val. Loss: 1.113 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.51%\n",
      "\t Val. Loss: 1.113 |  Val. Acc: 0.45%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.67%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 16.52%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.83%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 16.52%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.72%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 16.52%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.93%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 16.52%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.30%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 16.52%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.98%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 16.52%\n",
      "\tTrain Loss: 1.099 | Train Acc: 35.61%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 16.52%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 16.52%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.30%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 16.52%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.46%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 16.52%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.45%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 16.52%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.04%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 16.52%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.72%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 16.52%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.03%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 16.52%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.56%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 16.52%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.25%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 16.52%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.67%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 16.52%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.09%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 16.52%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.45%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 16.52%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.93%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 16.52%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.09%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 16.52%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.46%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 16.52%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.77%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 16.52%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.56%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 16.52%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.30%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 16.52%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.45%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 16.52%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.04%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 16.52%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.03%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 16.52%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.62%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 16.52%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.03%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 16.52%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.72%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 16.52%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.03%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 16.52%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.83%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 16.52%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.09%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 16.52%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.93%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 16.52%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.51%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 16.52%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.99%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 16.52%\n",
      "\tTrain Loss: 1.099 | Train Acc: 30.36%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 16.52%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.83%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 16.52%\n",
      "\tTrain Loss: 1.099 | Train Acc: 31.46%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 16.52%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.98%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 35.08%\n",
      "\tTrain Loss: 1.099 | Train Acc: 36.13%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 35.08%\n",
      "\tTrain Loss: 1.099 | Train Acc: 35.82%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 35.08%\n",
      "\tTrain Loss: 1.099 | Train Acc: 36.92%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 35.08%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.40%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 35.08%\n",
      "\tTrain Loss: 1.099 | Train Acc: 38.08%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 35.08%\n",
      "\tTrain Loss: 1.099 | Train Acc: 36.92%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 35.08%\n",
      "\tTrain Loss: 1.099 | Train Acc: 33.98%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 35.08%\n",
      "\tTrain Loss: 1.099 | Train Acc: 37.29%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 35.08%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.77%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 35.08%\n",
      "\tTrain Loss: 1.099 | Train Acc: 37.71%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 35.08%\n",
      "\tTrain Loss: 1.099 | Train Acc: 36.87%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 35.08%\n",
      "\tTrain Loss: 1.099 | Train Acc: 35.87%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 35.08%\n",
      "\tTrain Loss: 1.099 | Train Acc: 36.19%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 35.08%\n",
      "\tTrain Loss: 1.099 | Train Acc: 36.55%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 35.08%\n",
      "\tTrain Loss: 1.099 | Train Acc: 37.34%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 35.08%\n",
      "\tTrain Loss: 1.099 | Train Acc: 35.08%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 35.08%\n",
      "\tTrain Loss: 1.099 | Train Acc: 36.87%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 35.08%\n",
      "\tTrain Loss: 1.099 | Train Acc: 38.39%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 35.08%\n",
      "\tTrain Loss: 1.099 | Train Acc: 34.77%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 35.08%\n",
      "\tTrain Loss: 1.099 | Train Acc: 36.55%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 35.08%\n",
      "\tTrain Loss: 1.099 | Train Acc: 37.24%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 35.08%\n",
      "\tTrain Loss: 1.099 | Train Acc: 37.66%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 35.08%\n",
      "\tTrain Loss: 1.099 | Train Acc: 36.24%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 35.08%\n",
      "\tTrain Loss: 1.099 | Train Acc: 36.55%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 35.08%\n",
      "\tTrain Loss: 1.099 | Train Acc: 32.20%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 35.08%\n",
      "\tTrain Loss: 1.099 | Train Acc: 36.19%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 35.08%\n",
      "\tTrain Loss: 1.099 | Train Acc: 35.87%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 35.08%\n",
      "\tTrain Loss: 1.099 | Train Acc: 38.03%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 35.08%\n",
      "\tTrain Loss: 1.099 | Train Acc: 37.66%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 35.08%\n",
      "\tTrain Loss: 1.099 | Train Acc: 38.08%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 35.08%\n",
      "\tTrain Loss: 1.099 | Train Acc: 37.34%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 35.08%\n",
      "\tTrain Loss: 1.099 | Train Acc: 35.45%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 35.08%\n",
      "\tTrain Loss: 1.099 | Train Acc: 37.29%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 35.08%\n",
      "\tTrain Loss: 1.099 | Train Acc: 35.82%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 35.08%\n",
      "\tTrain Loss: 1.099 | Train Acc: 35.03%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 35.08%\n",
      "\tTrain Loss: 1.100 | Train Acc: 35.40%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 35.08%\n",
      "\tTrain Loss: 1.099 | Train Acc: 38.39%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 35.08%\n",
      "\tTrain Loss: 1.099 | Train Acc: 37.29%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 35.08%\n",
      "\tTrain Loss: 1.099 | Train Acc: 36.97%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 35.08%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 40\n",
    "runs = 50\n",
    "accuracies = []\n",
    "\n",
    "for run in range(runs):\n",
    "    #instantiate the model\n",
    "    model = classifier(size_of_vocab, embedding_dim, num_hidden_nodes,num_output_nodes, num_layers,\n",
    "                   bidirectional = True, dropout = dropout)\n",
    "\n",
    "    best_valid_loss = float('inf')\n",
    "    best_valid_acc = 0.0\n",
    "    for epoch in range(N_EPOCHS):\n",
    "\n",
    "        #train the model\n",
    "        train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "\n",
    "        #evaluate the model\n",
    "        valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "        #save the best model\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            best_valid_acc = valid_acc\n",
    "            torch.save(model.state_dict(), 'saved_weights.pt')\n",
    "\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "    accuracies.append(best_valid_acc)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot accuracies\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(sorted(accuracies), label='Model accuracies')\n",
    "plt.axhline(y=0.33, color='r', linestyle='-', label='Random guess')\n",
    "plt.axhline(y=sum(accuracies)/runs, color='orange', linestyle='-', label='Mean accuracy')\n",
    "plt.ylabel('Model Accuracies')\n",
    "plt.title(f\"Accuracies of {runs} individual train-evaluation runs\")\n",
    "plt.legend(loc=\"lower right\", borderaxespad=0)\n",
    "plt.savefig('plots/accuracies_deep03.png')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [],
   "source": [
    "\n",
    "#load weights\n",
    "path='saved_weights/deep_classifier_03.pt'\n",
    "model.load_state_dict(torch.load(path));\n",
    "model.eval();\n",
    "\n",
    "#inference"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [],
   "source": [
    "def predict_sentence(model, sentence):\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]  #tokenize the sentence\n",
    "    return predict(model, tokenized)\n",
    "\n",
    "\n",
    "def predict(model, tokenized):\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]          #convert to integer sequence\n",
    "    length = [len(indexed)]                                    #compute no. of words\n",
    "    tensor = torch.LongTensor(indexed).to(device)              #convert to tensor\n",
    "    tensor = tensor.unsqueeze(1).T                             #reshape in form of batch,no. of words\n",
    "    length_tensor = torch.LongTensor(length)                   #convert to tensor\n",
    "    prediction = model(tensor, length_tensor)                  #prediction\n",
    "    return prediction\n",
    "\n",
    "tens = predict_sentence(model, \"Ich bin ein Lockdown.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ich bin ein Lockdown. => +\n"
     ]
    }
   ],
   "source": [
    "#make predictions\n",
    "sentence = \"Ich bin ein Lockdown.\"\n",
    "probs = predict_sentence(model, sentence)\n",
    "label = LABEL.vocab.itos[torch.argmax(probs)]\n",
    "print(sentence + \" => \" + label)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [],
   "source": [
    "# determine validation accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "for e in valid_data:\n",
    "    # print(getattr(e,'label'))\n",
    "    # print(getattr(e, 'text'))\n",
    "    target_label = getattr(e, 'label')\n",
    "    tokenized_sentence = getattr(e, 'text')\n",
    "    preds = predict(model, tokenized_sentence)\n",
    "    pred_label = LABEL.vocab.itos[torch.argmax(preds)]\n",
    "    if pred_label == target_label:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "\n",
    "accuracy = correct / total"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier(\n",
      "  (embedding): Embedding(3384, 300)\n",
      "  (lstm): LSTM(300, 32, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "  (fc): Linear(in_features=64, out_features=3, bias=True)\n",
      "  (act): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}