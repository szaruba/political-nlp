{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#deal with tensors\n",
    "import torch\n",
    "import spacy\n",
    "from torchtext.legacy import data\n",
    "import torchtext.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "#Reproducing same results\n",
    "SEED = 2019\n",
    "\n",
    "#Torch\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "#Cuda algorithms\n",
    "torch.backends.cudnn.deterministic = True"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"de_dep_news_trf\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefan/.virtualenvs/diplom-code-env/lib/python3.8/site-packages/torchtext/data/utils.py:123: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
      "  warnings.warn(f'Spacy model \"{language}\" could not be loaded, trying \"{OLD_MODEL_SHORTCUTS[language]}\" instead')\n"
     ]
    }
   ],
   "source": [
    "TEXT = data.Field(tokenize='spacy',batch_first=True,include_lengths=True)\n",
    "LABEL = data.LabelField(dtype = torch.float)\n",
    "fields = [(None, None),(None, None),(None, None),(None, None),(None, None),(None, None),\n",
    "          ('label', LABEL),\n",
    "          (None, None),(None, None),(None, None),(None, None),(None, None),(None, None),\n",
    "          ('text',TEXT)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# loading custom dataset\n",
    "training_data=data.TabularDataset(path = 'protocols/labelled/only_lockdown.csv',format = 'csv',fields = fields,skip_header = True,\n",
    "                                  csv_reader_params={'delimiter': '\\t', 'quotechar': None})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "all_samples = [e for e in training_data.examples]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# Pre-processing\n",
    "# to lower case to match with vector embeddings\n",
    "tmp = []\n",
    "for sample in all_samples:\n",
    "    text = getattr(sample, 'text')\n",
    "    text = [s.lower() for s in text]\n",
    "    setattr(sample, 'text', text)\n",
    "\n",
    "positive = [e for e in all_samples if getattr(e, 'label') == '+']\n",
    "negative = [e for e in all_samples if getattr(e, 'label') == '-']\n",
    "neutral = [e for e in all_samples if getattr(e, 'label') == 'o']\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of TEXT vocabulary: 3384\n",
      "Size of LABEL vocabulary: 3\n",
      "[(',', 1345), ('die', 595), ('.', 508), ('der', 464), ('und', 457), ('lockdown', 380), ('\\xa0', 310), ('in', 251), ('–', 211), ('den', 201)]\n",
      "defaultdict(<bound method Vocab._default_unk_index of <torchtext.vocab.Vocab object at 0x7f712463f3d0>>, {'<unk>': 0, '<pad>': 1, ',': 2, 'die': 3, '.': 4, 'der': 5, 'und': 6, 'lockdown': 7, '\\xa0': 8, 'in': 9, '–': 10, 'den': 11, 'das': 12, 'es': 13, 'ist': 14, 'dass': 15, 'auch': 16, 'i': 17, 'm': 18, 'wir': 19, 'für': 20, 'des': 21, ':': 22, 'haben': 23, 'nicht': 24, 'hat': 25, 'sie': 26, 'zu': 27, 'von': 28, 'lockdowns': 29, 'sind': 30, 'dem': 31, 'ich': 32, 'mit': 33, '-': 34, 'zweiten': 35, 'eine': 36, 'ein': 37, 'einen': 38, 'noch': 39, 'jetzt': 40, 'wie': 41, 'aber': 42, 'wird': 43, 'sich': 44, 'vor': 45, 'werden': 46, 'auf': 47, 'um': 48, 'diese': 49, '„': 50, 'als': 51, 'so': 52, 'an': 53, 'durch': 54, 'am': 55, 'ja': 56, '“': 57, 'schon': 58, 'wieder': 59, 'wenn': 60, 'einem': 61, 'war': 62, 'da': 63, 'ersten': 64, 'sehr': 65, 'man': 66, 'was': 67, ';': 68, 'bei': 69, 'dieser': 70, 'aus': 71, 'nach': 72, 'österreich': 73, 'über': 74, 'uns': 75, 'weil': 76, 'zum': 77, 'maßnahmen': 78, 'während': 79, 'alle': 80, 'budget': 81, 'diesem': 82, 'er': 83, 'nur': 84, 'können': 85, 'heute': 86, 'bis': 87, 'bundesregierung': 88, 'diesen': 89, '!': 90, 'hier': 91, 'oder': 92, 'viele': 93, 'dann': 94, 'ganz': 95, 'zur': 96, 'damit': 97, 'märz': 98, 'regierung': 99, 'unternehmen': 100, 'wurde': 101, 'denn': 102, 'vom': 103, 'frauen': 104, 'gibt': 105, 'seit': 106, 'zeit': 107, '2020': 108, 'dieses': 109, 'einer': 110, 'euro': 111, 'herr': 112, 'immer': 113, 'menschen': 114, 'umsatzersatz': 115, 'mehr': 116, 'nämlich': 117, 'zweite': 118, 'allem': 119, 'aufgrund': 120, 'einmal': 121, 'möchte': 122, 'wochen': 123, 'worden': 124, '2021': 125, 'eines': 126, 'gar': 127, 'kommt': 128, 'lock\\xaddown': 129, 'machen': 130, 'unsere': 131, 'zahlen': 132, '?': 133, 'betriebe': 134, 'etwas': 135, 'gesagt': 136, 'keine': 137, 'krise': 138, 'mir': 139, 'natürlich': 140, 'prozent': 141, 'wirtschaft': 142, 'woche': 143, 'wäre': 144, 'würde': 145, 'betreffend': 146, 'bevölkerung': 147, 'daher': 148, 'hätte': 149, 'kickl': 150, 'kommen': 151, 'waren': 152, 'wissen': 153, ' ': 154, 'ende': 155, 'gefordert': 156, 'harten': 157, 'ihr': 158, 'könnte': 159, 'müssen': 160, 'sondern': 161, 'viel': 162, 'abgeordneten': 163, 'dafür': 164, 'kinder': 165, 'kollegen': 166, 'wirklich': 167, 'alles': 168, 'bereits': 169, 'gerade': 170, 'habe': 171, 'kann': 172, 'kolleginnen': 173, 'november': 174, 'phase': 175, 'sein': 176, 'sowie': 177, 'vielleicht': 178, 'wurden': 179, 'antrag': 180, 'auswirkungen': 181, 'beim': 182, 'beschließen': 183, 'corona': 184, 'eltern': 185, 'finanzminister': 186, 'geht': 187, 'gut': 188, 'ihnen': 189, 'jahr': 190, 'lassen': 191, 'muss': 192, 'möglich': 193, 'sagen': 194, 'situation': 195, 'sollen': 196, 'stehen': 197, 'unter': 198, 'wifo': 199, 'wollen': 200, 'zwei': 201, 'also': 202, 'arbeiten': 203, 'betroffen': 204, 'betroffenen': 205, 'dort': 206, 'fall': 207, 'frage': 208, 'wo': 209, '2': 210, 'anfang': 211, 'covid-19': 212, 'darf': 213, 'davon': 214, 'dazu': 215, 'einfach': 216, 'erst': 217, 'folgen': 218, 'folgenden': 219, 'frau': 220, 'gemacht': 221, 'harte': 222, 'ihre': 223, 'kosten': 224, 'letzten': 225, 'monaten': 226, 'nationalrat': 227, 'pandemie': 228, 'tage': 229, 'zahl': 230, 'allen': 231, 'anderen': 232, 'coronakrise': 233, 'damen': 234, 'dritten': 235, 'eingebracht': 236, 'gastronomie': 237, 'gegeben': 238, 'getan': 239, 'halten': 240, 'herren': 241, 'jene': 242, 'kollege': 243, 'mich': 244, 'milliarden': 245, 'mussten': 246, 'nichts': 247, 'notwendig': 248, 'nun': 249, 'oktober': 250, 'passiert': 251, 'sicherzustellen': 252, 'unser': 253, 'weiter': 254, 'wolle': 255, 'zeitpunkt': 256, 'zuge': 257, 'zwar': 258, '13': 259, 'ab': 260, 'aktuellen': 261, 'bereich': 262, 'bericht': 263, 'besonders': 264, 'braucht': 265, 'debatte': 266, 'erhalten': 267, 'familien': 268, 'frühjahr': 269, 'gab': 270, 'ihren': 271, 'insbesondere': 272, 'keinen': 273, 'kurz': 274, 'land': 275, 'laut': 276, 'lock\\xaddowns': 277, 'millionen': 278, 'nächsten': 279, 'ob': 280, 'offen': 281, 'ohne': 282, 'planungssicherheit': 283, 'schulen': 284, 'unternehmer': 285, 'verhindern': 286, 'wichtig': 287, 'ziel': 288, 'österreichischen': 289, 'övp': 290, '1': 291, 'andere': 292, 'befinden': 293, 'beginn': 294, 'bekommen': 295, 'branchen': 296, 'brauchen': 297, 'bundesgesetz': 298, 'bundeskanzler': 299, 'darüber': 300, 'deshalb': 301, 'dringend': 302, 'fpö': 303, 'gemeinden': 304, 'getroffen': 305, 'große': 306, 'indirekt': 307, 'klar': 308, 'leben': 309, 'light': 310, 'maßnahme': 311, 'meine': 312, 'meiner': 313, 'monate': 314, 'rasch': 315, 'richtig': 316, 'schulschließungen': 317, 'steht': 318, 'stellen': 319, 'tag': 320, 'unterstützung': 321, 'wegen': 322, 'weniger': 323, 'wären': 324, 'überhaupt': 325, '/': 326, '000': 327, '6': 328, 'abholstationen': 329, 'allein': 330, 'aufgefordert': 331, 'betrifft': 332, 'bevor': 333, 'dank': 334, 'drei': 335, 'eigentlich': 336, 'einigen': 337, 'geben': 338, 'gegangen': 339, 'gehabt': 340, 'gekommen': 341, 'genau': 342, 'gesamtwortlaut': 343, 'gesprochen': 344, 'gewesen': 345, 'gilt': 346, 'glaube': 347, 'hätten': 348, 'ihn': 349, 'ihrer': 350, 'ins': 351, 'kunst-': 352, 'leider': 353, 'mal': 354, 'massiv': 355, 'mittlerweile': 356, 'monat': 357, 'raum': 358, 'republik': 359, 'schwierigen': 360, 'sonst': 361, 'stark': 362, 'tatsächlich': 363, 'umgehend': 364, 'unserer': 365, 'vergleich': 366, 'verordnung': 367, 'warum': 368, 'wer': 369, 'zeiten': 370, 'zweiter': 371, 'österreichische': 372, '6,8': 373, '80': 374, 'all': 375, 'aller': 376, 'anders': 377, 'begonnen': 378, 'berücksichtigt': 379, 'besser': 380, 'bewusst': 381, 'branche': 382, 'budgetmittel': 383, 'coronavirus': 384, 'damals': 385, 'daran': 386, 'deswegen': 387, 'deutschland': 388, 'dezember': 389, 'diskussion': 390, 'down': 391, 'eben': 392, 'eingepreist': 393, 'einzige': 394, 'erste': 395, 'euch': 396, 'fixkostenzuschuss': 397, 'folgeschäden': 398, 'freitag': 399, 'ganze': 400, 'gehört': 401, 'geld': 402, 'gesamten': 403, 'gesellschaft': 404, 'gesundheitssystem': 405, 'gleichzeitig': 406, 'grund': 407, 'habt': 408, 'hause': 409, 'heißt': 410, 'helfen': 411, 'herbst': 412, 'hilfe': 413, 'ihrem': 414, 'ihs': 415, 'ii': 416, 'infektionen': 417, 'italien': 418, 'kein': 419, 'konnten': 420, 'kraft': 421, 'lange': 422, 'letzte': 423, 'lock': 424, 'macht': 425, 'minister': 426, 'ministerin': 427, 'nein': 428, 'neos': 429, 'neue': 430, 'neuen': 431, 'pressekonferenz': 432, 'punkt': 433, 'rede': 434, 'sagt': 435, 'schnell': 436, 'setzen': 437, 'soll': 438, 'sommer': 439, 'totalen': 440, 'tourismus': 441, 'trotzdem': 442, 'verfügung': 443, 'wahrscheinlich': 444, 'weg': 445, 'weise': 446, 'wirtschaftlich': 447, 'wirtschaftliche': 448, 'wirtschaftlichen': 449, 'zukunft': 450, 'zusammenhang': 451, '30': 452, '4': 453, '5': 454, '9': 455, 'abend': 456, 'abgebildet': 457, 'abgeordneter': 458, 'absolut': 459, 'angekündigt': 460, 'angerer': 461, 'anlagen': 462, 'arbeit': 463, 'ausmaß': 464, 'ausschließt': 465, 'begründung': 466, 'behauptet': 467, 'beispiel': 468, 'belastung': 469, 'beschlossen': 470, 'beschäftigten': 471, 'betrieb': 472, 'bewilligung': 473, 'bin': 474, 'bitte': 475, 'bleiben': 476, 'bringen': 477, 'budgetausschusses': 478, 'bürger': 479, 'dadurch': 480, 'darauf': 481, 'definitiv': 482, 'denen': 483, 'denke': 484, 'denken': 485, 'dennoch': 486, 'derzeit': 487, 'dessen': 488, 'deutlich': 489, 'distancelearning': 490, 'drozda': 491, 'einnahmen': 492, 'einrichtungen': 493, 'einzelnen': 494, 'entwicklung': 495, 'erinnere': 496, 'ermöglichung': 497, 'erwin': 498, 'folge': 499, 'freie': 500, 'freiheitlichen': 501, 'funktioniert': 502, 'geführt': 503, 'gesorgt': 504, 'gestern': 505, 'geworden': 506, 'grenzen': 507, 'gutes': 508, 'heimischen': 509, 'herinnen': 510, 'heuer': 511, 'hohen': 512, 'hotellerie': 513, 'instrument': 514, 'jahres': 515, 'jenen': 516, 'keiner': 517, 'kleinen': 518, 'klubobmann': 519, 'kollegin': 520, 'konsequenzen': 521, 'künstler': 522, 'künstlerinnen': 523, 'lage': 524, 'leisten': 525, 'leute': 526, 'länder': 527, 'manche': 528, 'medien': 529, 'mein': 530, 'mitte': 531, 'neuerliche': 532, 'neuerlicher': 533, 'nie': 534, 'notwendigen': 535, 'obwohl': 536, 'offensichtlich': 537, 'paar': 538, 'plus': 539, 'rahmen': 540, 'reden': 541, 'regierungsvorlage': 542, 'rund': 543, 'sage': 544, 'samt': 545, 'schaffen': 546, 'schließen': 547, 'schritt': 548, 'schule': 549, 'seine': 550, 'selber': 551, 'shutdown': 552, 'sinne': 553, 'sorgen': 554, 'soziale': 555, 'sozialen': 556, 'sperren': 557, 'staat': 558, 'stelle': 559, 'thema': 560, 'thomas': 561, 'treffen': 562, 'tun': 563, 'uhr': 564, 'umsatzersatzes': 565, 'umzusetzen': 566, 'unserem': 567, 'vergangenen': 568, 'verhängt': 569, 'verloren': 570, 'verluste': 571, 'verschlechtert': 572, 'vier': 573, 'vorgesehen': 574, 'wahrheit': 575, 'weiteren': 576, 'weiterhin': 577, 'weiß': 578, 'welche': 579, 'wien': 580, 'wirtschaftsministerin': 581, 'wohl': 582, 'wollten': 583, 'wort': 584, 'zurück': 585, 'zurückgegangen': 586, 'zwischen': 587, 'öffentlich': 588, 'öffentlichen': 589, 'österreicher': 590, 'österreichs': 591, 'übrigen': 592, '16': 593, '18': 594, '3': 595, '300': 596, '8': 597, 'aktuell': 598, 'alternativlos': 599, 'amazon': 600, 'anderes': 601, 'angesichts': 602, 'anhält': 603, 'april': 604, 'arbeitslosenzahlen': 605, 'aufge\\xadfordert': 606, 'aufholen': 607, 'aufrecht': 608, 'ausgegangen': 609, 'aussendung': 610, 'auswirkt': 611, 'auszubezahlen': 612, 'auszuschließen': 613, 'bedeutet': 614, 'befürchten': 615, 'beginnen': 616, 'beitrag': 617, 'bekannt': 618, 'beraten': 619, 'bereiche': 620, 'beschlos\\xadsen': 621, 'bezug': 622, 'bip': 623, 'bisschen': 624, 'blick': 625, 'blümel': 626, 'bringe': 627, 'buch': 628, 'bundesminister': 629, 'bundesvoranschlages': 630, 'bzw': 631, 'bücher': 632, 'coronalockdown': 633, 'covid': 634, 'damoklesschwert': 635, 'darum': 636, 'daten': 637, 'dauer': 638, 'dauern': 639, 'dies': 640, 'direkt': 641, 'diskutieren': 642, 'diskutiert': 643, 'doch': 644, 'dramatischen': 645, 'draußen': 646, 'drinnen': 647, 'durchgeführt': 648, 'ebenfalls': 649, 'eher': 650, 'einige': 651, 'eins': 652, 'entsprechenden': 653, 'entstanden': 654, 'erleben': 655, 'erreicht': 656, 'erschossen': 657, 'etwa': 658, 'europäischen': 659, 'familie': 660, 'februar': 661, 'finanzonline': 662, 'finanzämter': 663, 'finden': 664, 'flughafen': 665, 'freunden': 666, 'früher': 667, 'fünf': 668, 'gebraucht': 669, 'geehrte': 670, 'gehen': 671, 'geleistet': 672, 'gelungen': 673, 'gemeinsam': 674, 'genauso': 675, 'geregelt': 676, 'gering': 677, 'gesamte': 678, 'geschlossen': 679, 'gesehen': 680, 'gesetzt': 681, 'gesundheitssystems': 682, 'getreten': 683, 'gewusst': 684, 'gleich': 685, 'großen': 686, 'großes': 687, 'grundlage': 688, 'grüne': 689, 'gute': 690, 'hart': 691, 'hatten': 692, 'haus': 693, 'heimische': 694, 'hilfen': 695, 'hoffentlich': 696, 'ihm': 697, 'innerhalb': 698, 'investieren': 699, 'jedem': 700, 'jeden': 701, 'jedoch': 702, 'jetzigen': 703, 'job': 704, 'jänner': 705, 'kam': 706, 'kindern': 707, 'klarstellen': 708, 'kommunikation': 709, 'kulturschaffende': 710, 'kurzarbeit': 711, 'kümmern': 712, 'kürzlich': 713, 'lebensmitteln': 714, 'lesen': 715, 'liegen': 716, 'mahrer': 717, 'massive': 718, 'medial': 719, 'mitarbeiter': 720, 'musste': 721, 'möglichkeit': 722, 'möglichst': 723, 'nachhaltig': 724, 'neben': 725, 'neuerlichen': 726, 'normalität': 727, 'nächste': 728, 'oft': 729, 'online': 730, 'partei': 731, 'pflege': 732, 'planbarkeit': 733, 'profitieren': 734, 'prognose': 735, 'quartals': 736, 'rasche': 737, 'rechnen': 738, 'regelmäßige': 739, 'richtige': 740, 'schauen': 741, 'schaut': 742, 'schellhorn': 743, 'schließung': 744, 'schramböck': 745, 'schüler': 746, 'sehen': 747, 'sei': 748, 'seite': 749, 'sicher': 750, 'sofortige': 751, 'sogar': 752, 'sowohl': 753, 'spiel': 754, 'stattfinden': 755, 'steigen': 756, 'studien': 757, 'tagen': 758, 'teil': 759, 'teillockdown': 760, 'top': 761, 'tragen': 762, 'trifft': 763, 'ug': 764, 'umständen': 765, 'um\\xadsetzung': 766, 'ungefähr': 767, 'unseres': 768, 'unterfertigten': 769, 'unterschieden': 770, 'unterstützen': 771, 'veranstaltungen': 772, 'verantwortung': 773, 'vereinen': 774, 'vergessen': 775, 'verlassen': 776, 'vermeiden': 777, 'verordnete': 778, 'verordneten': 779, 'verschlafen': 780, 'versuchen': 781, 'verursacht': 782, 'vielen': 783, 'volksanwaltschaft': 784, 'vollen': 785, 'wann': 786, 'weit': 787, 'weitere': 788, 'weiterer': 789, 'welle': 790, 'wirtschaftsforscher': 791, 'wirtschaftsforschungsinstitut': 792, 'wirtschaftshilfen': 793, 'wirtschaftskammer': 794, 'wöginger': 795, 'zehn': 796, 'zuerst': 797, 'zulieferer': 798, 'zusammengeschossen': 799, 'zusätzlich': 800, 'zusätzliche': 801, 'österreicherinnen': 802, 'öster\\xadreich': 803, 'überlegt': 804, '‘': 805, '\"': 806, '%': 807, '...': 808, '0': 809, '09.okt': 810, '10': 811, '100': 812, '140': 813, '15': 814, '2,5': 815, '2.000': 816, '20': 817, '4,0': 818, '4,4': 819, '4,5': 820, '500.000': 821, '60': 822, '62': 823, '7': 824, '80prozentigen': 825, '[': 826, ']': 827, 'abgelehnt': 828, 'abgenommen': 829, 'absagen': 830, 'abschließend': 831, 'absehbar': 832, 'abstimmung': 833, 'abzusehen': 834, 'aktuelle': 835, 'andererseits': 836, 'angebot': 837, 'angefangen': 838, 'angenommen': 839, 'angstmacherei': 840, 'anlässlich': 841, 'anschober': 842, 'anspruch': 843, 'anstatt': 844, 'anträge': 845, 'antwort': 846, 'apa0140': 847, 'arbeitslose': 848, 'arbeitslosen': 849, 'arbeitslosigkeit': 850, 'arbeitsmarkpolitische': 851, 'argen': 852, 'aschbacher': 853, 'aufgabe': 854, 'aufgefangen': 855, 'aufgestellt': 856, 'aufholbar': 857, 'aufträge': 858, 'auf\\xadgrund': 859, 'ausgangslage': 860, 'ausgeführt': 861, 'ausgehen': 862, 'ausgelöst': 863, 'ausgeschlossen': 864, 'ausland': 865, 'ausschließen': 866, 'auszusenden': 867, 'badelt': 868, 'bauarbeiter': 869, 'bedanken': 870, 'bedenkt': 871, 'bedingt': 872, 'bedingungen': 873, 'beendet': 874, 'beendigung': 875, 'befindet': 876, 'befreundeter': 877, 'begleitende': 878, 'begleit\\xadmaß\\xadnahmen': 879, 'beispielhaft': 880, 'beisteuern': 881, 'bekämpfung': 882, 'bemühungen': 883, 'benötigen': 884, 'bereichen': 885, 'berichtige': 886, 'berichtigung': 887, 'beschluss': 888, 'beschränkungen': 889, 'beste': 890, 'bestehenden': 891, 'besteht': 892, 'bestellt': 893, 'bestimmt': 894, 'bestimmte': 895, 'besucher': 896, 'betretungsverbote': 897, 'betretungsverboten': 898, 'betrug': 899, 'bewirken': 900, 'bezeichnen': 901, 'beziehungsweise': 902, 'bildung': 903, 'bleibt': 904, 'branchenabhängigen': 905, 'brückl': 906, 'buchhändler': 907, 'budgetverhandlungen': 908, 'budgetären': 909, 'bundes\\xadminister': 910, 'bun\\xaddesregierung': 911, 'ca': 912, 'coronakurzarbeit': 913, 'dankbar': 914, 'danke': 915, 'dankeschön': 916, 'daraus': 917, 'dargelegt': 918, 'davor': 919, 'deren': 920, 'detail': 921, 'dienst': 922, 'dienstag': 923, 'diesbezüglich': 924, 'digitales': 925, 'digitalisierung': 926, 'dinge': 927, 'diskussionen': 928, 'disziplin': 929, 'dramatisch': 930, 'dramatische': 931, 'drastisch': 932, 'dringende': 933, 'dritte': 934, 'durchschlagen': 935, 'durfte': 936, 'dynamik': 937, 'echte': 938, 'effiziente': 939, 'eigenen': 940, 'einbrechen': 941, 'einerseits': 942, 'eingebrochenem': 943, 'eingefordert': 944, 'einig': 945, 'einzelhandel': 946, 'einzig': 947, 'einzigen': 948, 'endet': 949, 'endlich': 950, 'enthalten': 951, 'entschädigung': 952, 'entsprechende': 953, 'entstehen': 954, 'erfahrungen': 955, 'erfolgt': 956, 'erforderliche': 957, 'erholung': 958, 'erklärt': 959, 'ermöglicht': 960, 'ertragsanteile': 961, 'erwarten': 962, 'eure': 963, 'experten': 964, 'falls': 965, 'familienbeihilfe': 966, 'fangen': 967, 'feiern': 968, 'finanziell': 969, 'finanzmittel': 970, 'finanz\\xadminister': 971, 'finde': 972, 'fixkosten\\xadzuschuss': 973, 'fleisch': 974, 'folgender': 975, 'fonds': 976, 'fordere': 977, 'forschung': 978, 'fortgesetzt': 979, 'fr': 980, 'fragen': 981, 'freiheitliche': 982, 'freilich': 983, 'fälle': 984, 'führen': 985, 'führt': 986, 'gagen': 987, 'ganzen': 988, 'ganzer': 989, 'geblieben': 990, 'gedanken': 991, 'geehrten': 992, 'gefahr': 993, 'gefehlt': 994, 'gefor\\xaddert': 995, 'gefragt': 996, 'gefühl': 997, 'gefühlt': 998, 'gegenüber': 999, 'gehalten': 1000, 'gelassen': 1001, 'gelaufen': 1002, 'gelegenheit': 1003, 'gemeint': 1004, 'genommen': 1005, 'genom\\xadmen': 1006, 'genossen': 1007, 'genossinnen': 1008, 'genug': 1009, 'genützt': 1010, 'geringer': 1011, 'geschichte': 1012, 'geschrieben': 1013, 'geschätzt': 1014, 'gesetzliche': 1015, 'gestellt': 1016, 'gestiegen': 1017, 'gesundheit': 1018, 'gesundheitlich': 1019, 'gesundheitsminister': 1020, 'gewerbe': 1021, 'gewährung': 1022, 'gezeigt': 1023, 'ge\\xadstiegen': 1024, 'geändert': 1025, 'gott': 1026, 'greifen': 1027, 'gruppen': 1028, 'größte': 1029, 'größten': 1030, 'halbwegs': 1031, 'handel': 1032, 'handeln': 1033, 'hebein': 1034, 'heldinnen': 1035, 'her': 1036, 'herauszukommen': 1037, 'hermann': 1038, 'herrn': 1039, 'herrscht': 1040, 'herunten': 1041, 'herunterdrücken': 1042, 'heurige': 1043, 'heurigen': 1044, 'hilfspaketen': 1045, 'hilft': 1046, 'hin': 1047, 'hinaus': 1048, 'hinterfragt': 1049, 'hintergrund': 1050, 'hintertür': 1051, 'hoffnung': 1052, 'hohe': 1053, 'härtefallfonds': 1054, 'höhe': 1055, 'höher': 1056, 'ihres': 1057, 'infektionsgeschehen': 1058, 'infektionszahlen': 1059, 'infizierten': 1060, 'inland': 1061, 'inmitten': 1062, 'insofern': 1063, 'international': 1064, 'interview': 1065, 'irgendwie': 1066, 'israel': 1067, 'it': 1068, 'jahre': 1069, 'jahren': 1070, 'jahrzehnte': 1071, 'jeder': 1072, 'juli': 1073, 'kenntnis': 1074, 'kerbe': 1075, 'kinder\\xadbetreuung': 1076, 'kocher': 1077, 'kommenden': 1078, 'kompensation': 1079, 'konkreten': 1080, 'konnte': 1081, 'kontaktlosen': 1082, 'kopf': 1083, 'kultureinrichtungen': 1084, 'kulturminister': 1085, 'kurier': 1086, 'kurzfristig': 1087, 'könnten': 1088, 'lang': 1089, 'langsam': 1090, 'lehrer': 1091, 'lehrerinnen': 1092, 'lehrlingskosten': 1093, 'leicht': 1094, 'leistungen': 1095, 'lernen': 1096, 'letztlich': 1097, 'leuten': 1098, 'liebe': 1099, 'liegt': 1100, 'loacker': 1101, 'lockdownbonus': 1102, 'lokal': 1103, 'luftverschmutzung': 1104, 'ländern': 1105, 'ma': 1106, 'massiven': 1107, 'mediziner': 1108, 'mehrwertsteuersenkungen': 1109, 'meinung': 1110, 'menschenleben': 1111, 'men\\xadschen': 1112, 'mindestens': 1113, 'minus': 1114, 'mitarbeiterinnen': 1115, 'mittel': 1116, 'mitteln': 1117, 'momentan': 1118, 'mögliche': 1119, 'möglicherweise': 1120, 'müll': 1121, 'müsse': 1122, 'müsste': 1123, 'müssten': 1124, 'na': 1125, 'nachlesen': 1126, 'netto': 1127, 'neu': 1128, 'neßler': 1129, 'niveau': 1130, 'nochmals': 1131, 'normalisierung': 1132, 'not': 1133, 'notvergabe': 1134, 'notwendigkeit': 1135, 'nützen': 1136, 'onlinehandel': 1137, 'opposition': 1138, 'per': 1139, 'pflichtschulen': 1140, 'planen': 1141, 'plant': 1142, 'politik': 1143, 'polizisten': 1144, 'polizistinnen': 1145, 'positive': 1146, 'private': 1147, 'probleme': 1148, 'problemen': 1149, 'prognosezeitraum': 1150, 'prozentpunkte': 1151, 'präsident': 1152, 'puls': 1153, 'quartal': 1154, 'quasi': 1155, 'raab': 1156, 'radikale': 1157, 'raten': 1158, 'reagiert': 1159, 'realisiert': 1160, 'redet': 1161, 'reduktion': 1162, 'reduzieren': 1163, 'regierungsparteien': 1164, 'regie\\xadrungsvorlage': 1165, 'reichen': 1166, 'rendi': 1167, 'retten': 1168, 'ringen': 1169, 'rollen': 1170, 'rosa': 1171, 'rücken': 1172, 'saison': 1173, 'scharf': 1174, 'scharfen': 1175, 'schlag': 1176, 'schmeißen': 1177, 'schneller': 1178, 'schreiben': 1179, 'schul': 1180, 'schulden': 1181, 'schulschließungs-': 1182, 'schweden': 1183, 'schweiz': 1184, 'schwerer': 1185, 'schwierig': 1186, 'schwierige': 1187, 'schwierigkeiten': 1188, 'schäden': 1189, 'schön': 1190, 'sechs': 1191, 'selbstständige': 1192, 'selbstverständlich': 1193, 'senken': 1194, 'september': 1195, 'sichergestellt': 1196, 'sicherheit': 1197, 'sicht': 1198, 'sieben': 1199, 'signale': 1200, 'sodass': 1201, 'sogenannte': 1202, 'sogenannten': 1203, 'solche': 1204, 'sommermonate': 1205, 'sozusagen': 1206, 'spannend': 1207, 'sparen': 1208, 'spitäler': 1209, 'spreche': 1210, 'sprechen': 1211, 'spö': 1212, 'spürbar': 1213, 'staaten': 1214, 'starke': 1215, 'startet': 1216, 'stellt': 1217, 'stimmen': 1218, 'ständig': 1219, 'svs': 1220, 'sämtliche': 1221, 'tageszeitung': 1222, 'tiere': 1223, 'tonnen': 1224, 'tool': 1225, 'totallockdown': 1226, 'trinken': 1227, 'täglich': 1228, 'tätigen': 1229, 'umsatz': 1230, 'umsatzentschädigung': 1231, 'umsatzrückgang': 1232, 'umsatzsteuerminderung': 1233, 'umsetzung': 1234, 'umstellung': 1235, 'undenkbar': 1236, 'unklaren': 1237, 'unmittelbar': 1238, 'unnötige': 1239, 'unsicherheit': 1240, 'untergliederung': 1241, 'unternehmerinnen': 1242, 'unterrichten': 1243, 'unterstützt': 1244, 'verehrten': 1245, 'vereinbarte': 1246, 'vereine': 1247, 'vergangene': 1248, 'verhindert': 1249, 'verhinderung': 1250, 'verlangt': 1251, 'verlierer': 1252, 'verlängert': 1253, 'vermehrt': 1254, 'vernunft': 1255, 'verordnet': 1256, 'verordnungen': 1257, 'verschlechternden': 1258, 'verschulden': 1259, 'verschärft': 1260, 'versorgung': 1261, 'verständnis': 1262, 'verstärkt': 1263, 'versucht': 1264, 'vertreter': 1265, 'vieles': 1266, 'vierten': 1267, 'voller': 1268, 'vorgelegt': 1269, 'vorgeschlagen': 1270, 'vorher': 1271, 'vorhinein': 1272, 'vorlegt': 1273, 'vorstellen': 1274, 'vorzulegen': 1275, 'völligen': 1276, 'wagner': 1277, 'warnt': 1278, 'warnte': 1279, 'wegbrechen': 1280, 'weihnachten': 1281, 'weitergehen': 1282, 'weiterlaufen': 1283, 'weitreichende': 1284, 'welt': 1285, 'wenigen': 1286, 'wertschöpfungsverlust': 1287, 'westeuropa': 1288, 'wichtige': 1289, 'wichtiger': 1290, 'wiederholen': 1291, 'wiederum': 1292, 'wirken': 1293, 'wirkt': 1294, 'wirtschaftskrise': 1295, 'wirtschaftsleistung': 1296, 'wirtschaftspolitische': 1297, 'wissenschaft': 1298, 'wobei': 1299, 'wusste': 1300, 'zeigen': 1301, 'zeitung': 1302, 'zitiere': 1303, 'zitiert': 1304, 'zugrunde': 1305, 'zuletzt': 1306, 'zulieferfirmen': 1307, 'zumindest': 1308, 'zurückgehen': 1309, 'zusperren': 1310, 'zustimmung': 1311, 'ändern': 1312, 'ärzte': 1313, 'äußerst': 1314, 'öffentliche': 1315, 'öffnung': 1316, 'überbrückungsfonds': 1317, 'übrigens': 1318, '‚': 1319, '  ': 1320, '-initiativen': 1321, '-trägern': 1322, '-ver\\xadanstalter': 1323, '000er': 1324, '03.11.2020': 1325, '05.nov': 1326, '075': 1327, '095': 1328, '1,1': 1329, '1,6': 1330, '1,8': 1331, '107': 1332, '11': 1333, '11.30': 1334, '117': 1335, '12,4': 1336, '13.3': 1337, '14': 1338, '16.11': 1339, '16.3': 1340, '16.3.2020': 1341, '17': 1342, '183': 1343, '189.000': 1344, '19': 1345, '1950': 1346, '1956': 1347, '1967': 1348, '1979': 1349, '2.11.2020': 1350, '200': 1351, '2020/2021': 1352, '21': 1353, '22': 1354, '24': 1355, '25': 1356, '26': 1357, '28': 1358, '31': 1359, '32': 1360, '40': 1361, '450': 1362, '50': 1363, '500': 1364, '53,7': 1365, '55': 1366, '6.12': 1367, '600': 1368, '613': 1369, '7,7': 1370, '800': 1371, '82': 1372, '900': 1373, '936': 1374, 'a': 1375, 'abarbeitung': 1376, 'abbildung': 1377, 'abbringen': 1378, 'abdeckt': 1379, 'abfederung': 1380, 'abfertigungsgesetz': 1381, 'abgefedert': 1382, 'abgeflacht': 1383, 'abgehalten': 1384, 'abgeholt': 1385, 'abgeschafft': 1386, 'abgesehen': 1387, 'abgestritten': 1388, 'abgewiesen': 1389, 'abschluss': 1390, 'absicherung': 1391, 'absolute': 1392, 'abwandern': 1393, 'abwehren': 1394, 'abwicklung': 1395, 'abzuschaffen': 1396, 'abzuschätzen': 1397, 'abzuwenden': 1398, 'abzuwickeln': 1399, 'abänderungsantrag': 1400, 'abänderungsbudget': 1401, 'acht': 1402, 'adaptiert': 1403, 'adressiert': 1404, 'afrikanische': 1405, 'agiert': 1406, 'akontozahlung': 1407, 'aktienkurse': 1408, 'akzeptiert': 1409, 'alleingang': 1410, 'alleinige': 1411, 'allerdings': 1412, 'allerseelen': 1413, 'alt': 1414, 'alter': 1415, 'alternativen': 1416, 'amtskollegin': 1417, 'analog': 1418, 'anbelangt': 1419, 'anbietern': 1420, 'anblick': 1421, 'anderem': 1422, 'ande\\xadren': 1423, 'anfragen': 1424, 'angeblich': 1425, 'angeblichen': 1426, 'angebote': 1427, 'angeboten': 1428, 'angeführt': 1429, 'angehörigen': 1430, 'angelegtes': 1431, 'angeordnet': 1432, 'angesetzten': 1433, 'angesteckt': 1434, 'angestellt': 1435, 'angestiegen': 1436, 'angesucht': 1437, 'angewiesen': 1438, 'ange\\xadwiesen': 1439, 'angriff': 1440, 'ankommen': 1441, 'ankündigungen': 1442, 'anlaufenden': 1443, 'anmerken': 1444, 'annäherungsverbote': 1445, 'anonym': 1446, 'anpassungen': 1447, 'anrechnung': 1448, 'anregen': 1449, 'ans': 1450, 'ansatzweise': 1451, 'anschaue': 1452, 'anschaut': 1453, 'anscheinend': 1454, 'ansetzt': 1455, 'ansteigen': 1456, 'anstrengungen': 1457, 'ansuchen': 1458, 'ante': 1459, 'antragslos': 1460, 'antragslose': 1461, 'antragsteller': 1462, 'anzubieten': 1463, 'anzukündigen': 1464, 'apa0170': 1465, 'apotheken': 1466, 'arbeitenden': 1467, 'arbeitnehmer': 1468, 'arbeitnehmerinnen': 1469, 'arbeitslos': 1470, 'arbeitslosengeld': 1471, 'arbeitslosen\\xadzahlen': 1472, 'arbeitsmarkt': 1473, 'arbeitsmarktpolitik': 1474, 'arbeitsministerin': 1475, 'arbeitsplatz': 1476, 'arbeitsplätze': 1477, 'arbeitsprozesse': 1478, 'arbeitswelt': 1479, 'arbeitszeit': 1480, 'arbeits\\xadlosen\\xadzahlen': 1481, 'arme': 1482, 'armutsfalle': 1483, 'art': 1484, 'arzneimittelversorgung': 1485, 'arztpraxen': 1486, 'ar\\xadbeitslosigkeit': 1487, 'attes\\xadtiert': 1488, 'aufeinander': 1489, 'auffällt': 1490, 'aufgehoben': 1491, 'aufgepasst': 1492, 'aufgesetzt': 1493, 'aufhorchen': 1494, 'aufhört': 1495, 'auflachen': 1496, 'auflagen': 1497, 'aufmerksamkeit': 1498, 'aufrufe': 1499, 'aufrufen': 1500, 'aufs': 1501, 'aufseiten': 1502, 'aufwärtsgegangen': 1503, 'aufzufangen': 1504, 'aufzustellen': 1505, 'auf\\xadgefordert': 1506, 'augen': 1507, 'august': 1508, 'ausbezahlt': 1509, 'ausbreitung': 1510, 'ausbricht': 1511, 'ausdehnung': 1512, 'ausgangbeschränkungen': 1513, 'ausgangsbeschränkung': 1514, 'ausgangsbeschränkungen': 1515, 'ausgangsverbote': 1516, 'ausganssperren': 1517, 'ausgebrochen': 1518, 'ausgeht': 1519, 'ausgerechnet': 1520, 'ausgereicht': 1521, 'ausgerufen': 1522, 'ausgestalten': 1523, 'ausgeweitet': 1524, 'ausgewirkt': 1525, 'ausgezeichnet': 1526, 'ausgleich': 1527, 'ausgleichen': 1528, 'auslastungszahlen': 1529, 'ausliefern': 1530, 'ausländerbeschäftigungsge\\xadsetz': 1531, 'ausreichend': 1532, 'ausreichende': 1533, 'ausreichender': 1534, 'ausschuss': 1535, 'aussieht': 1536, 'aussteigen': 1537, 'ausstrahlt': 1538, 'auswirkung': 1539, 'auszunutzen': 1540, 'aus\\xadreichende': 1541, 'aus\\xadsehen': 1542, 'außen': 1543, 'außenministeriums': 1544, 'außenminis\\xadterium': 1545, 'außenstellen': 1546, 'außerhalb': 1547, 'barbara': 1548, 'basieren': 1549, 'basis': 1550, 'bauernhof': 1551, 'bauern\\xadfamilien': 1552, 'beamten': 1553, 'beamtinnen': 1554, 'beam\\xadten': 1555, 'beantragen': 1556, 'beantragt': 1557, 'bean\\xadtragen': 1558, 'bearbeitet': 1559, 'beatmungsgeräte': 1560, 'bedarf': 1561, 'bedeuten': 1562, 'bedeutung': 1563, 'bedrohung': 1564, 'befeuern': 1565, 'befindlichen': 1566, 'befristete': 1567, 'befürchtet': 1568, 'begegnen': 1569, 'beginnend': 1570, 'beginnt': 1571, 'begleiten': 1572, 'begleitet': 1573, 'begleitforschung': 1574, 'begleit\\xadforschung': 1575, 'begründet': 1576, 'begrüßen': 1577, 'begrüßte': 1578, 'behandeln': 1579, 'behandelt': 1580, 'behandlungsrückstau': 1581, 'behaupten': 1582, 'beherbergungsbetriebe': 1583, 'beherber\\xadgungs\\xadbetriebe': 1584, 'beherzt': 1585, 'behördenweg': 1586, 'behördlich': 1587, 'beiden': 1588, 'beigetragen': 1589, 'beihilfenrecht': 1590, 'beihilfen\\xadrecht\\xadlichen': 1591, 'beispielsweise': 1592, 'beitragen': 1593, 'beiträgt': 1594, 'bekanntlich': 1595, 'bekenntnis': 1596, 'beklagte': 1597, 'bekommt': 1598, 'belakowitsch': 1599, 'belastet': 1600, 'belastungen': 1601, 'belgien': 1602, 'belvedere': 1603, 'bemerkung': 1604, 'benötigtes': 1605, 'beobachten': 1606, 'beratung': 1607, 'beratungen': 1608, 'beratungsinformationen': 1609, 'berechnet': 1610, 'berechnungen': 1611, 'berechtigt': 1612, 'bereiten': 1613, 'bereitgestellt': 1614, 'bereitschaft': 1615, 'bereitzustellen': 1616, 'berichtet': 1617, 'berichtete': 1618, 'beruf': 1619, 'berufe': 1620, 'berufliche': 1621, 'berücksichtigen': 1622, 'berücksichtigung': 1623, 'berühren': 1624, 'beschlossene': 1625, 'beschwerden': 1626, 'beschäftigt': 1627, 'beschäftigtenstand': 1628, 'beschäftigung': 1629, 'beschäftigungspaket': 1630, 'besondere': 1631, 'besonderes': 1632, 'beson\\xadders': 1633, 'besorgniserregende': 1634, 'bessere': 1635, 'besseren': 1636, 'bestanden': 1637, 'besten': 1638, 'bestens': 1639, 'bestmögliche': 1640, 'bestätigt': 1641, 'besuche': 1642, 'besucherinnen': 1643, 'besucherrückgängen': 1644, 'betonen': 1645, 'betont': 1646, 'betrachtung': 1647, 'betrachtungszeitraum': 1648, 'betreiben': 1649, 'betretungsverbot': 1650, 'betretungsverbo\\xadte': 1651, 'betreuen': 1652, 'betreut': 1653, 'betrieben': 1654, 'betriebsschließungen': 1655, 'betroffene': 1656, 'betrof\\xadfen': 1657, 'beträchtlich': 1658, 'beurteilt': 1659, 'beur\\xadteilen': 1660, 'bevorstehenden': 1661, 'bevorsteht': 1662, 'bewegung': 1663, 'bewegungsmangel': 1664, 'beweist': 1665, 'bewertung': 1666, 'bewiesen': 1667, 'bewirkt': 1668, 'bewältigbar': 1669, 'bezahlten': 1670, 'beziffert': 1671, 'bezirken': 1672, 'bezogen': 1673, 'be\\xadginn': 1674, 'be\\xadlieben': 1675, 'be\\xadtreffen': 1676, 'be\\xadtroffenen': 1677, 'be\\xadvölkerung': 1678, 'bier': 1679, 'bieten': 1680, 'bilanz': 1681, 'bilden': 1682, 'bilder': 1683, 'bildungsbereich': 1684, 'bildungslockdown': 1685, 'bildungssystems': 1686, 'bildungsverluste': 1687, 'bildungszeit': 1688, 'binnen': 1689, 'birgit': 1690, 'bisher': 1691, 'bitten': 1692, 'blase': 1693, 'bleibe': 1694, 'blindflug': 1695, 'blödsinn': 1696, 'bombardieren': 1697, 'books': 1698, 'botschaften': 1699, 'botschafter': 1700, 'brau\\xadchen': 1701, 'brechen': 1702, 'brief': 1703, 'bringt': 1704, 'bruttoinlandsprodukt': 1705, 'bräuchte': 1706, 'bräuchten': 1707, 'buchhandel': 1708, 'buchhändlerin\\xadnen': 1709, 'buchkultur': 1710, 'budgetausschuss': 1711, 'budgetbericht': 1712, 'budgetdebatte': 1713, 'budgetpolitik': 1714, 'budgetrede': 1715, 'budgets': 1716, 'budget\\xadausschusses': 1717, 'budget\\xadhearing': 1718, 'budgetänderung': 1719, 'budgetär': 1720, 'bud\\xadgets': 1721, 'bud\\xadgetär': 1722, 'bund': 1723, 'bundesfinanzrahmengesetz': 1724, 'bundeskanzleramt': 1725, 'bundeskanzlers': 1726, 'bundesländerspezifische': 1727, 'bundesmuseen': 1728, 'bundesparteiobmann': 1729, 'bundesvoran\\xadschlages': 1730, 'bundes\\xadkanzler': 1731, 'buschenschanken': 1732, 'börsencrash': 1733, 'bürgerinnen': 1734, 'bürgermeisterkollegen': 1735, 'canberra': 1736, 'caritas': 1737, 'cash': 1738, 'chance': 1739, 'chancenindex': 1740, 'chaos': 1741, 'chef': 1742, 'chefcoronaleugner': 1743, 'china': 1744, 'christen': 1745, 'christoph': 1746, 'chronologie': 1747, 'circa': 1748, 'comebackzuschuss': 1749, 'computer-': 1750, 'coronaausnahmesituation': 1751, 'coronabedingtem': 1752, 'coronainfizierte': 1753, 'coronakommission': 1754, 'coronapandemie': 1755, 'coronapolitik': 1756, 'coronasituation': 1757, 'coronaviruspandemie': 1758, 'coronazeit': 1759, 'co\\xadronatoten': 1760, 'co\\xadvid-19': 1761, 'cyberinfrastruktur': 1762, 'dabei': 1763, 'dagegengestimmt': 1764, 'dahin': 1765, 'dahinter': 1766, 'danach': 1767, 'danebenliegt': 1768, 'danken': 1769, 'darangesetzt': 1770, 'dargestellt': 1771, 'darstellte': 1772, 'darzustellen': 1773, 'dasselbe': 1774, 'datenbasis': 1775, 'datenerhebungen': 1776, 'datenerhe\\xadbun\\xadgen': 1777, 'dazusagen': 1778, 'de': 1779, 'debatten': 1780, 'defizite': 1781, 'demnach': 1782, 'denkt': 1783, 'derartigen': 1784, 'derartiges': 1785, 'derer': 1786, 'derjenige': 1787, 'derselben': 1788, 'details': 1789, 'deutsch\\xadland': 1790, 'dich': 1791, 'dienen': 1792, 'dienstes': 1793, 'dienstrechtsgesetz': 1794, 'diesbezüg\\xadlich': 1795, 'diesmal': 1796, 'dietmar': 1797, 'die\\xadsem': 1798, 'digi\\xadtalisierung': 1799, 'dimension': 1800, 'dingen': 1801, 'diskursen': 1802, 'distance': 1803, 'doppelresidenz': 1804, 'doppelten': 1805, 'dop\\xadpelte': 1806, 'dorthin': 1807, 'dortigen': 1808, 'draht': 1809, 'dramas': 1810, 'draufhaben': 1811, 'dreier': 1812, 'dreifache': 1813, 'dringlichkeit': 1814, 'drittel': 1815, 'drohen': 1816, 'drohenden': 1817, 'drängt': 1818, 'du': 1819, 'durchaus': 1820, 'durchgehen': 1821, 'durchgekommen': 1822, 'durchgesetzt': 1823, 'durchgezogen': 1824, 'durchzubringen': 1825, 'durch\\xadführt': 1826, 'durch\\xadge\\xadführt': 1827, 'dürfen': 1828, 'e': 1829, 'ebendiesen': 1830, 'ebenso': 1831, 'effekt': 1832, 'effizienter': 1833, 'egal': 1834, 'eh': 1835, 'eigene': 1836, 'eigenes': 1837, 'eigenlob': 1838, 'eigentlichen': 1839, 'eigent\\xadlich': 1840, 'eigenver\\xadantwortung': 1841, 'einbringen': 1842, 'einbringt': 1843, 'einbruch': 1844, 'einbußen': 1845, 'eindeutig': 1846, 'eindruck': 1847, 'eindrucksvoll': 1848, 'eindrücklich': 1849, 'eindämmung': 1850, 'einfacher': 1851, 'einforderten': 1852, 'eingegangen': 1853, 'eingehe': 1854, 'eingehen': 1855, 'eingerichteten': 1856, 'eingeschränkt': 1857, 'eingeschränkten': 1858, 'eingesprungen': 1859, 'eingetretene': 1860, 'eingetroffen': 1861, 'eingriffe': 1862, 'einhalten': 1863, 'einhaltung': 1864, 'einhorn': 1865, 'einiges': 1866, 'einkaufsmöglichkeiten': 1867, 'einkommensverlust': 1868, 'einmalzahlung': 1869, 'einnahmenentfall': 1870, 'einpendeln': 1871, 'einreichmodalitäten': 1872, 'einschenken': 1873, 'einschneidend': 1874, 'einschränken': 1875, 'einschätzung': 1876, 'einstellen': 1877, 'einstellung': 1878, 'einstimmig': 1879, 'eintragungswoche': 1880, 'einverneh\\xadmen': 1881, 'einzelne': 1882, 'einzudämmen': 1883, 'einzuführen': 1884, 'einzugreifen': 1885, 'einzupreisen': 1886, 'ein\\xadfordern': 1887, 'elefant': 1888, 'elf': 1889, 'empfinden': 1890, 'empörung': 1891, 'endes': 1892, 'endgerät': 1893, 'energieträger': 1894, 'enorm': 1895, 'enorme': 1896, 'enormen': 1897, 'enten': 1898, 'entfernt': 1899, 'entgeltsicherungsge\\xadsetz': 1900, 'entlastung': 1901, 'entscheiden': 1902, 'entscheidung': 1903, 'entscheidungsstärke': 1904, 'entscheidungs\\xadfindung': 1905, 'entschei\\xaddungen': 1906, 'entschließen': 1907, 'entschließungsantrag': 1908, 'entschädigungsanspruch': 1909, 'entschädi\\xadgungs\\xadanspruch': 1910, 'entschä\\xaddigung': 1911, 'entsprechend': 1912, 'entspricht': 1913, 'entsteht': 1914, 'entwickelt': 1915, 'entwicklungen': 1916, 'entwick\\xadlungen': 1917, 'entwurf': 1918, 'epidemieg': 1919, 'epidemiegesetz': 1920, 'erfahrung': 1921, 'erfolgen': 1922, 'erforderlich': 1923, 'erforderlichen': 1924, 'erfreulich': 1925, 'erfüllen': 1926, 'erhalt': 1927, 'erhebliche': 1928, 'erhebung': 1929, 'erhöhen': 1930, 'erinnern': 1931, 'erinnert': 1932, 'erinnerung': 1933, 'erkennens': 1934, 'erkenntnissen': 1935, 'erkrankten': 1936, 'erlauben': 1937, 'erlebt': 1938, 'erledigen': 1939, 'erledigt': 1940, 'ermöglichen': 1941, 'ernst': 1942, 'erratisches': 1943, 'erreichbar': 1944, 'errichten': 1945, 'erschließt': 1946, 'erstes': 1947, 'ersuche': 1948, 'erwachsenen': 1949, 'erweiterten': 1950, 'erweiterung': 1951, 'erwirtschaften': 1952, 'erwischt': 1953, 'erwähnen': 1954, 'erwähnt': 1955, 'erwähnung': 1956, 'erzwungenen': 1957, 'essen': 1958, 'europa': 1959, 'europaweit': 1960, 'eu\\xadropäischen': 1961, 'event-': 1962, 'evidenz': 1963, 'evidenzbasierte': 1964, 'evidenz\\xadbasierte': 1965, 'existenz': 1966, 'existenzielle': 1967, 'expertise': 1968, 'exporte': 1969, 'exportgenehmigungen': 1970, 'externe': 1971, 'facto': 1972, 'fair': 1973, 'faktor': 1974, 'fallen': 1975, 'falsch': 1976, 'falschen': 1977, 'familienlastenausgleichsgesetz': 1978, 'familienleistungen': 1979, 'familienverbandes': 1980, 'familien\\xadbeihilfe': 1981, 'fantasiebudget': 1982, 'fantasiezahlen': 1983, 'fast': 1984, 'fatal': 1985, 'fatalen': 1986, 'fatz': 1987, 'fehlen': 1988, 'fehlende': 1989, 'fehler': 1990, 'fehlt': 1991, 'fest': 1992, 'festhalten': 1993, 'feststellung': 1994, 'film-': 1995, 'finanziellen': 1996, 'finanzierungsgesetz': 1997, 'finanzministers': 1998, 'finanzrahmen': 1999, 'finanzverwaltung': 2000, 'finanz\\xadministerium': 2001, 'findet': 2002, 'fiskus': 2003, 'fitnessstudiobetreiber': 2004, 'fixkostenzuschusses': 2005, 'fkz': 2006, 'fleißig': 2007, 'flächendeckende': 2008, 'flächendeckenden': 2009, 'folgekosten': 2010, 'folgende': 2011, 'folgendes': 2012, 'folgt': 2013, 'folgte': 2014, 'forderung': 2015, 'form': 2016, 'formate': 2017, 'formen': 2018, 'formulieren': 2019, 'formuliert': 2020, 'formulierung': 2021, 'fortzusetzen': 2022, 'fossile': 2023, 'frappant': 2024, 'frauenbild': 2025, 'frauenministerin': 2026, 'frauenmi\\xadnisterin': 2027, 'frechheit': 2028, 'freiberufler': 2029, 'freiheitsrechte': 2030, 'freischaffenden': 2031, 'freiwillige': 2032, 'frequente': 2033, 'frequenz': 2034, 'freue': 2035, 'freunderl': 2036, 'freundinnen': 2037, 'freundliches': 2038, 'freut': 2039, 'froh': 2040, 'frühjahres': 2041, 'frühling': 2042, 'fuchs': 2043, 'funktion': 2044, 'furchtbar': 2045, 'fällen': 2046, 'förderangebote': 2047, 'förderpaket': 2048, 'förderungen': 2049, 'för\\xadderungen': 2050, 'fühlen': 2051, 'führend': 2052, 'fürchterlichen': 2053, 'ganzes': 2054, 'ganzjahresmaskerade': 2055, 'gas': 2056, 'gastronomiebetrieben': 2057, 'gast\\xadronomie': 2058, 'geantwortet': 2059, 'gebe': 2060, 'gebieten': 2061, 'gebracht': 2062, 'gebrachte': 2063, 'gebun\\xadden': 2064, 'gebührt': 2065, 'gedacht': 2066, 'geehrter': 2067, 'geeignet': 2068, 'geeigneten': 2069, 'gefunden': 2070, 'geführte': 2071, 'gegeißelt': 2072, 'gegen': 2073, 'gegend': 2074, 'gegenteil': 2075, 'geglaubt': 2076, 'gehaltsgesetz': 2077, 'gehe': 2078, 'geheißen': 2079, 'gehend': 2080, 'gehofft': 2081, 'gekennzeichnet': 2082, 'geklärt': 2083, 'gekostet': 2084, 'gekündigt': 2085, 'gelernt': 2086, 'gelin\\xaddere': 2087, 'geltende': 2088, 'gemeindeamt': 2089, 'gemeinsamen': 2090, 'gemeinsames': 2091, 'genannt': 2092, 'generalsekretär': 2093, 'genereller': 2094, 'gene\\xadralsekretär': 2095, 'genießen': 2096, 'gepflegt': 2097, 'gepredigt': 2098, 'gerasselt': 2099, 'geredet': 2100, 'gerettet': 2101, 'gerichtet': 2102, 'geringe': 2103, 'geringeren': 2104, 'geringeres': 2105, 'gerne': 2106, 'gernot': 2107, 'gerutscht': 2108, 'gerüchte': 2109, 'gerückt': 2110, 'gesamter': 2111, 'gesamtes': 2112, 'geschaffen': 2113, 'gescheites': 2114, 'geschlechtern': 2115, 'geschlittert': 2116, 'geschlossenen': 2117, 'geschnürt': 2118, 'geschuldet': 2119, 'geschwiegen': 2120, 'geschwungen': 2121, 'geschätzte': 2122, 'gesellschaftlich': 2123, 'gesell\\xadschaftliche': 2124, 'gesessen': 2125, 'gesetze': 2126, 'gesetzlichen': 2127, 'gespart': 2128, 'gespräche': 2129, 'gesprächen': 2130, 'gestalten': 2131, 'gestiegenen': 2132, 'gestreut': 2133, 'gesund': 2134, 'gesundheitsbereich': 2135, 'gesundheitsberufen': 2136, 'gesundheitsbudget': 2137, 'gesundheitskrise': 2138, 'gesundheitsministerium': 2139, 'gesundheitsministeriums': 2140, 'gesundheitspolitisch': 2141, 'gesundheitspolitische': 2142, 'gesundheitswesen': 2143, 'gesundheitszustand': 2144, 'gesund\\xadheitsausschuss': 2145, 'gesund\\xadheitsversorgung': 2146, 'gesund\\xadheitszustand': 2147, 'gesund\\xadheits\\xadminister': 2148, 'getragen': 2149, 'getrennt': 2150, 'getwittert': 2151, 'gewalt': 2152, 'gewaltbereitschaft': 2153, 'gewalteskalationen': 2154, 'gewerbetreibenden': 2155, 'gewinnen': 2156, 'gewisser': 2157, 'gewissheit': 2158, 'gewählt': 2159, 'gewähren': 2160, 'gewährleisten': 2161, 'gewährleistet': 2162, 'gezielte': 2163, 'ge\\xadnannt': 2164, 'ge\\xadschlossenen': 2165, 'ge\\xadschlos\\xadsenen': 2166, 'ge\\xadsellschaft': 2167, 'ge\\xadsundheitswesen': 2168, 'ge\\xadwalt': 2169, 'ge\\xadzielte': 2170, 'gib': 2171, 'ging': 2172, 'glaskugel': 2173, 'glauben': 2174, 'glaubt': 2175, 'gnadenhöfen': 2176, 'gp': 2177, 'grenzschließung': 2178, 'grenzschließun\\xadgen': 2179, 'griff': 2180, 'grippalen': 2181, 'grippeepidemie': 2182, 'groß': 2183, 'großer': 2184, 'großflächi\\xadgen': 2185, 'großindustrie': 2186, 'großteil': 2187, 'großteils': 2188, 'grund-': 2189, 'grundkonsens': 2190, 'grundlegenden': 2191, 'grund\\xadsätzlich': 2192, 'größer': 2193, 'gründe': 2194, 'gründen': 2195, 'grünen': 2196, 'guten': 2197, 'gänge': 2198, 'halbjahr': 2199, 'halt': 2200, 'handelns': 2201, 'handels': 2202, 'handwerk': 2203, 'hantel': 2204, 'harald': 2205, 'harter': 2206, 'hatte': 2207, 'hauptausschuss': 2208, 'hauptausschus\\xadses': 2209, 'hauptlast': 2210, 'hauptprofiteur': 2211, 'haushalt': 2212, 'haustür': 2213, 'ha\\xadben': 2214, 'heftige': 2215, 'hehl': 2216, 'heimischer': 2217, 'heimi\\xadschen': 2218, 'helden': 2219, 'heraus': 2220, 'herausforderung': 2221, 'herausfor\\xadde\\xadrung': 2222, 'herausgekommen': 2223, 'herausgestellt': 2224, 'herauskommen': 2225, 'herauszubringen': 2226, 'hereingebrochen': 2227, 'hergeben': 2228, 'hergehen': 2229, 'herhalten': 2230, 'herrschte': 2231, 'herstellen': 2232, 'herumzutesten': 2233, 'herunterfahren': 2234, 'hervorragende': 2235, 'herzliches': 2236, 'her\\xadstellen': 2237, 'heutigem': 2238, 'heutigen': 2239, 'hey': 2240, 'he\\xadrumfantasiert': 2241, 'hiezu': 2242, 'hilfestellungen': 2243, 'hilfsfonds': 2244, 'hilfsmaßnahme': 2245, 'hilfsmaßnahmen': 2246, 'hilfsorganisationen': 2247, 'hinausgeht': 2248, 'hinein': 2249, 'hineingegangen': 2250, 'hineinzuschreiben': 2251, 'hingedeutet': 2252, 'hingestellt': 2253, 'hingewiesen': 2254, 'hinsicht': 2255, 'hinsichtlich': 2256, 'hinstellt': 2257, 'hinstolpert': 2258, 'hinterfragen': 2259, 'hinterlassen': 2260, 'hinweis': 2261, 'hinüberturnen': 2262, 'hochfährt': 2263, 'hochschulen': 2264, 'hoffe': 2265, 'hoher': 2266, 'home': 2267, 'homeoffice': 2268, 'homeschooling': 2269, 'hotels': 2270, 'humanitäre': 2271, 'hunderttausende': 2272, 'hut': 2273, 'hygienemaßnahmen': 2274, 'hält': 2275, 'hände': 2276, 'hängen': 2277, 'hängt': 2278, 'häusliche': 2279, 'häuslichen': 2280, 'höchstens': 2281, 'höchstnotwendige': 2282, 'höhepunkt': 2283, 'höllentempo': 2284, 'höre': 2285, 'hören': 2286, 'hört': 2287, 'hörte': 2288, 'iden': 2289, 'iger': 2290, 'immerhin': 2291, 'impfstoff': 2292, 'impfung': 2293, 'indem': 2294, 'indexierung': 2295, 'indikator': 2296, 'industrie': 2297, 'infekte': 2298, 'infektionsfälle': 2299, 'infektionskrankheit': 2300, 'infektionsrate': 2301, 'infektionsraten': 2302, 'infektionswerte': 2303, 'infizierte': 2304, 'informationen': 2305, 'infrastrukturellen': 2306, 'initiiert': 2307, 'inländischen': 2308, 'innenministerium': 2309, 'insgesamt': 2310, 'insolvenz': 2311, 'institutionen': 2312, 'integrieren': 2313, 'intensiv': 2314, 'intensivbetten': 2315, 'intensive': 2316, 'interdisziplinäre': 2317, 'interessant': 2318, 'interessante': 2319, 'interessanterweise': 2320, 'investition': 2321, 'investitions-': 2322, 'investment': 2323, 'inves\\xadtitionsanreizen': 2324, 'inves\\xadtitionsprämie': 2325, 'irgendeinem': 2326, 'irgendetwas': 2327, 'irgendet\\xadwas': 2328, 'irgendwelche': 2329, 'irgendwelchen': 2330, 'italienischen': 2331, 'jahresende': 2332, 'jahreswechsel': 2333, 'je': 2334, 'jede': 2335, 'jederzeit': 2336, 'jedes': 2337, 'jemand': 2338, 'jener': 2339, 'jobs': 2340, 'josef': 2341, 'journalist': 2342, 'jugendliche': 2343, 'jugendlichen': 2344, 'jugendnotruf': 2345, 'juliauszahlung': 2346, 'juni': 2347, 'jux': 2348, 'jährliches': 2349, 'kamen': 2350, 'kameras': 2351, 'kanada': 2352, 'kaniak': 2353, 'kannten': 2354, 'kapazitäten': 2355, 'kapazitäts\\xadgrenzen': 2356, 'karl': 2357, 'katastrophal': 2358, 'katastrophe': 2359, 'katastrophenhilfe': 2360, 'katholischen': 2361, 'kauf': 2362, 'kaum': 2363, 'keck': 2364, 'kehren': 2365, 'kehrt': 2366, 'keil': 2367, 'keinster': 2368, 'keller': 2369, 'kellnern': 2370, 'kennen': 2371, 'khm': 2372, 'kinder-': 2373, 'kinderrechte': 2374, 'klarheit': 2375, 'kleine': 2376, 'kleineren': 2377, 'klub\\xadobmann': 2378, 'kohle': 2379, 'kollateralschaden': 2380, 'kollegin\\xadnen': 2381, 'komme': 2382, 'kommenwird': 2383, 'kompensiert': 2384, 'kompe\\xadtenzverteilung': 2385, 'komplett': 2386, 'kompletten': 2387, 'kom\\xadmen': 2388, 'kom\\xadmission': 2389, 'konsequent': 2390, 'konsequente': 2391, 'konstant': 2392, 'kontakt': 2393, 'kontakte': 2394, 'kontaktlose': 2395, 'kontakt\\xadlose': 2396, 'kontext': 2397, 'kontraproduktiv': 2398, 'konzeption': 2399, 'kostendeckungen': 2400, 'kostenlos': 2401, 'kranken': 2402, 'krankenkassen': 2403, 'kreative': 2404, 'kriege': 2405, 'kriegen': 2406, 'krisen': 2407, 'krisenbewältigung': 2408, 'krisenbewältigungsmodus': 2409, 'krisenmanagement': 2410, 'kritik': 2411, 'kritische': 2412, 'kritischen': 2413, 'kritisieren': 2414, 'kritisiert': 2415, 'kuala': 2416, 'kucher': 2417, 'kuchl': 2418, 'kultur-': 2419, 'kulturbetriebe': 2420, 'kulturbetrieben': 2421, 'kulturbudget': 2422, 'kulturinstitutionen': 2423, 'kulturschaffenden': 2424, 'kultur\\xadbranche': 2425, 'kunden': 2426, 'kundenkontakte': 2427, 'kundinnen': 2428, 'kunsthistorische': 2429, 'kurzarbeitsregelung': 2430, 'kurze': 2431, 'kurzem': 2432, 'kurzfristigen': 2433, 'käme': 2434, 'kämpfen': 2435, 'könne': 2436, 'könnt': 2437, 'könn\\xadten': 2438, 'kündigungswelle': 2439, 'künstlerin': 2440, 'künstlersozialversicherung': 2441, 'künstlichen': 2442, 'kürze': 2443, 'landegenehmigungen': 2444, 'landen': 2445, 'landeten': 2446, 'langen': 2447, 'langfristig': 2448, 'langfristigen': 2449, 'lasse': 2450, 'lasten': 2451, 'lauer': 2452, 'laufen': 2453, 'laufenden': 2454, 'learning': 2455, 'lebenseinkommen': 2456, 'lebensmittel': 2457, 'lebenssituation': 2458, 'lehnt': 2459, 'lehre': 2460, 'lehrer_innen': 2461, 'lehrpersonal': 2462, 'leichten': 2463, 'leiden': 2464, 'leistungsträgerinnen': 2465, 'leis\\xadtungsfähig': 2466, 'letztendlich': 2467, 'letzter': 2468, 'level': 2469, 'licht-': 2470, 'lieber': 2471, 'lieblingslokal': 2472, 'ließen': 2473, 'lifte': 2474, 'lima': 2475, 'literatur': 2476, 'live': 2477, 'lockdownforderer': 2478, 'lockdownkompensation': 2479, 'lockdownkrise': 2480, 'lockdownmodus': 2481, 'lockdownszenario': 2482, 'lockdownverlierern': 2483, 'lockdownzeit': 2484, 'lockdownzeiten': 2485, 'lockdown\\xadverlierer': 2486, 'lockdownähnlichen': 2487, 'lockert': 2488, 'lockerungen': 2489, 'lock\\xaddownphase': 2490, 'lokale': 2491, 'lokals': 2492, 'lumpur': 2493, 'lustig': 2494, 'lächeln': 2495, 'länger': 2496, 'lässt': 2497, 'lösen': 2498, 'lösung': 2499, 'machbar': 2500, 'machbaren': 2501, 'machtfülle': 2502, 'mag': 2503, 'mahnte': 2504, 'mai': 2505, 'makulatur': 2506, 'management': 2507, 'mann': 2508, 'mantel': 2509, 'man\\xadche': 2510, 'man\\xadcherlei': 2511, 'marke': 2512, 'martin': 2513, 'maskenball': 2514, 'maskenpflicht': 2515, 'massenimpfung': 2516, 'massentests': 2517, 'massentestung': 2518, 'material': 2519, 'materialkosten': 2520, 'matznetter': 2521, 'maximal': 2522, 'ma\\xadchen': 2523, 'maßgeblich': 2524, 'maßnahmengesetz': 2525, 'maßnahmengesetzes': 2526, 'maßnah\\xadmen': 2527, 'maß\\xadnahmen': 2528, 'medizinerinnen': 2529, 'medizinischen': 2530, 'medizinisches': 2531, 'medizinprodukten': 2532, 'mehrfach': 2533, 'mehrheitsfraktion': 2534, 'mehrzahl': 2535, 'meilenweit': 2536, 'meinem': 2537, 'meinen': 2538, 'meinungen': 2539, 'meinungsfreiheit': 2540, 'meinungshoheit': 2541, 'meisten': 2542, 'meldete': 2543, 'menge': 2544, 'menschen\\xadexperiment': 2545, 'merken': 2546, 'merkt': 2547, 'methode': 2548, 'mikrofon': 2549, 'min': 2550, 'mindeststandards': 2551, 'minimieren': 2552, 'minizahlung': 2553, 'minuten': 2554, 'min\\xaddeststandards': 2555, 'misstraut': 2556, 'missverständnisse': 2557, 'miteinander': 2558, 'miteinbeziehung': 2559, 'mitgegangen': 2560, 'mitgeteilt': 2561, 'mitgetragen': 2562, 'mitge\\xadstimmt': 2563, 'mitgliedern': 2564, 'mitleidenschaft': 2565, 'mitteilen': 2566, 'mittel-': 2567, 'mitternacht': 2568, 'mittleren': 2569, 'mittler\\xadweile': 2570, 'mittwoch': 2571, 'mitzuüberweisen': 2572, 'mi\\xadnus': 2573, 'modellierung': 2574, 'modellierungsversuchen': 2575, 'moder\\xadnem': 2576, 'momentanen': 2577, 'monatelang': 2578, 'motto': 2579, 'multiarbeit': 2580, 'museum': 2581, 'mutter': 2582, 'mythen': 2583, 'möglichen': 2584, 'möglicher': 2585, 'möglichkeiten': 2586, 'nachbarländer': 2587, 'nachdenkt': 2588, 'nachfolgende': 2589, 'nachgehen': 2590, 'nachgeliefert': 2591, 'nachhinein': 2592, 'nachstehenden': 2593, 'nachstehender': 2594, 'nachste\\xadhenden': 2595, 'nacht': 2596, 'nachverfolgung': 2597, 'nachvollziehbarer': 2598, 'nach\\xadste\\xadhen\\xadden': 2599, 'nase': 2600, 'nationalbank': 2601, 'nationalrates': 2602, 'nebenjob': 2603, 'negative': 2604, 'negativen': 2605, 'nehmen': 2606, 'netz': 2607, 'neuausrichtung': 2608, 'neuer': 2609, 'neues': 2610, 'neuinfektionen': 2611, 'nieder': 2612, 'niedergelassener': 2613, 'niedrige': 2614, 'niemals': 2615, 'niemandem': 2616, 'nord-': 2617, 'nordamerika': 2618, 'nor\\xadmalität': 2619, 'notmaßnahmenverordnung': 2620, 'notwen\\xaddigkeit': 2621, 'not\\xadwendig': 2622, 'nov': 2623, 'novellierung': 2624, 'novem\\xadber': 2625, 'npo': 2626, 'nullpunkt': 2627, 'nunmehr': 2628, 'nächstes': 2629, 'nächtigungen': 2630, 'nötig': 2631, 'nötigen': 2632, 'oberstufe': 2633, 'oberstufen': 2634, 'obfrau': 2635, 'of': 2636, 'oftmals': 2637, 'ohnehin': 2638, 'operationen': 2639, 'opfer': 2640, 'oppositionsparteien': 2641, 'oppo\\xadsition': 2642, 'oppo\\xadsitionsparteien': 2643, 'orf': 2644, 'organisiert': 2645, 'orga\\xadni\\xadsationen': 2646, 'originalverpackt': 2647, 'ort': 2648, 'ortschaften': 2649, 'ost-': 2650, 'ots': 2651, 'paare': 2652, 'paket': 2653, 'pamela': 2654, 'pannen': 2655, 'pan\\xaddemie': 2656, 'papierkübel': 2657, 'parlament': 2658, 'parlaments': 2659, 'parlamentsklubs': 2660, 'parteipolitischen': 2661, 'passieren': 2662, 'pauschalität': 2663, 'peking': 2664, 'pension': 2665, 'pensionsraub': 2666, 'perfide': 2667, 'permanent': 2668, 'personal': 2669, 'perspektive': 2670, 'perspek\\xadtiven': 2671, 'persönlich': 2672, 'phantasiert': 2673, 'phasen': 2674, 'planungsarbeiten': 2675, 'plattform': 2676, 'pleite-': 2677, 'plenums': 2678, 'plätze': 2679, 'plötzlich': 2680, 'plötzliche': 2681, 'podium': 2682, 'politiker': 2683, 'politischen': 2684, 'polizeistaatlich': 2685, 'pop': 2686, 'portas': 2687, 'positiven': 2688, 'positives': 2689, 'preis': 2690, 'preise': 2691, 'presse': 2692, 'pressekonferenzen': 2693, 'pres\\xadseaussendungen': 2694, 'privatbereich': 2695, 'privaten': 2696, 'privatleben': 2697, 'pro': 2698, 'problem': 2699, 'produktion': 2700, 'produktionen': 2701, 'produzieren': 2702, 'prof.': 2703, 'profiteure': 2704, 'projektaufgaben': 2705, 'projektbezogen': 2706, 'prominenten': 2707, 'protokoll': 2708, 'prozentigen': 2709, 'pro\\xadzentigen': 2710, 'prämien': 2711, 'präsentiert': 2712, 'präsidenten': 2713, 'präventivmaßnahme': 2714, 'psychische': 2715, 'psychologischen': 2716, 'puls-4': 2717, 'pult': 2718, 'punkte': 2719, 'pöttinger': 2720, 'quaken': 2721, 'qualität': 2722, 'quartiere': 2723, 'quer\\xadschnittstestungen': 2724, 'rascher': 2725, 'rat': 2726, 'ratz': 2727, 'reaktion': 2728, 'realistische': 2729, 'rechnet': 2730, 'rechnungen': 2731, 'rechte': 2732, 'rechtsanspruch': 2733, 'rechtzeitig': 2734, 'rechtzeitigen': 2735, 'redner': 2736, 'regeln': 2737, 'regelung': 2738, 'regen': 2739, 'regierungsexperte': 2740, 'regierungslinie': 2741, 'regierungsseite': 2742, 'region': 2743, 'regionalen': 2744, 'reguläre': 2745, 'rehabilitationen': 2746, 'reiben': 2747, 'reibungslos': 2748, 'reinen': 2749, 'reise': 2750, 'reisebeschränkungen': 2751, 'reisebüros': 2752, 'reisewarnungen': 2753, 'reisewar\\xadnungen': 2754, 'relativ': 2755, 'renate': 2756, 'rennt': 2757, 'reparatur': 2758, 'repara\\xadtur': 2759, 'ressortministern': 2760, 'return': 2761, 'revidiert': 2762, 'rezession': 2763, 'richtet': 2764, 'richtigen': 2765, 'richtigerweise': 2766, 'richtiges': 2767, 'richtigstellen': 2768, 'richtung': 2769, 'rigorosen': 2770, 'risikoszenario': 2771, 'rolle': 2772, 'ruhe': 2773, 'runden': 2774, 'runtergefahren': 2775, 'rückgang': 2776, 'rückholaktion': 2777, 'rückschritte': 2778, 'rückverweis': 2779, 'rückwirkend': 2780, 'rückwirkung': 2781, 'sache': 2782, 'sachen': 2783, 'sachliche': 2784, 'sagst': 2785, 'salamitaktik': 2786, 'sammeln': 2787, 'sanfte': 2788, 'sanften': 2789, 'sanfter': 2790, 'sanitäter': 2791, 'sanitäterinnen': 2792, 'satz': 2793, 'sa\\xadgen': 2794, 'schaden': 2795, 'schadenersatz': 2796, 'schadenersatzes': 2797, 'schallmauer': 2798, 'schallmeiner': 2799, 'schall\\xadmauer': 2800, 'scharfer': 2801, 'schau\\xaden': 2802, 'scha\\xadden': 2803, 'scheichelbauer': 2804, 'schlagartig': 2805, 'schlage': 2806, 'schlange': 2807, 'schlecht': 2808, 'schlechter': 2809, 'schlechtwetterentschädigungsgesetz': 2810, 'schließlich': 2811, 'schließt': 2812, 'schließ\\xadlich': 2813, 'schloss': 2814, 'schlug': 2815, 'schluss': 2816, 'schlussappell': 2817, 'schlussberatungen': 2818, 'schlägt': 2819, 'schmerzhaft': 2820, 'schmidhofer': 2821, 'schnelle': 2822, 'schnellen': 2823, 'schnellstmögliche': 2824, 'schnöde': 2825, 'schooling': 2826, 'schrittweise': 2827, 'schrittweisen': 2828, 'schullockdown': 2829, 'schul\\xadlockdown': 2830, 'schuster': 2831, 'schwarzen': 2832, 'schwebt': 2833, 'schwer': 2834, 'schwe\\xadre': 2835, 'schwierigkeit': 2836, 'schwierigsten': 2837, 'schwächere': 2838, 'schwätzen': 2839, 'schätzungen': 2840, 'schönes': 2841, 'schöngerechnet': 2842, 'schüler_innen': 2843, 'schülerinnen': 2844, 'sebastian': 2845, 'sechswöchigen': 2846, 'sehe': 2847, 'seid': 2848, 'seien': 2849, 'seilbahnbereich': 2850, 'seinem': 2851, 'seiner': 2852, 'seines': 2853, 'seitdem': 2854, 'seiten': 2855, 'seitens': 2856, 'sekundarstufe': 2857, 'selben': 2858, 'selbstständigen': 2859, 'sensibilisierungs\\xadkampagne': 2860, 'sepp': 2861, 'setzt': 2862, 'sichergehen': 2863, 'sicherstellt': 2864, 'signalisieren': 2865, 'signalisiert': 2866, 'signifikant': 2867, 'simulationsexperte': 2868, 'sinken': 2869, 'sinnbefreit': 2870, 'sinnvoll': 2871, 'sitzung': 2872, 'skigebiete': 2873, 'sofortigen': 2874, 'sofortiger': 2875, 'soft': 2876, 'solch': 2877, 'solchen': 2878, 'soldaten': 2879, 'soldatinnen': 2880, 'solide': 2881, 'solides': 2882, 'solle': 2883, 'sollte': 2884, 'sollten': 2885, 'sonderberichte': 2886, 'sonderbetreuung': 2887, 'sonderbetreuungszeit': 2888, 'sonderbetreuungszeiten': 2889, 'sonderprüfungen': 2890, 'sonstige': 2891, 'sorge': 2892, 'sorgt': 2893, 'sozial': 2894, 'sozialausschuss': 2895, 'soziales': 2896, 'sozialversicherung': 2897, 'sozialversicherungen': 2898, 'so\\xaddass': 2899, 'spaltung': 2900, 'sparring\\xadpartner': 2901, 'sparte': 2902, 'spaß': 2903, 'spezielle': 2904, 'spielen': 2905, 'spitzenkandidatinnen': 2906, 'spitzensport': 2907, 'spitzen\\xadmonaten': 2908, 'spitälern': 2909, 'sport': 2910, 'sprache': 2911, 'springende': 2912, 'spuren': 2913, 'spät': 2914, 'später': 2915, 'spätestens': 2916, 'spä\\xadter': 2917, 'spüren': 2918, 'staates': 2919, 'staatshilfen': 2920, 'staat\\xadlichen': 2921, 'stadium': 2922, 'standen': 2923, 'starten': 2924, 'statt': 2925, 'stattdessen': 2926, 'stattfindet': 2927, 'stattgefunden': 2928, 'statt\\xadfindet': 2929, 'statt\\xadgefunden': 2930, 'stau\\xaddamm': 2931, 'stecken': 2932, 'steigenden': 2933, 'steigerung': 2934, 'steigt': 2935, 'steiner': 2936, 'stellenabbau\\xadprogram\\xadme': 2937, 'steuer': 2938, 'steuerliche': 2939, 'stimmt': 2940, 'stim\\xadmen': 2941, 'stoß': 2942, 'strategie': 2943, 'straucheln': 2944, 'straße': 2945, 'strengeren': 2946, 'strengster': 2947, 'stresses': 2948, 'streut': 2949, 'studie': 2950, 'studierende': 2951, 'stunden': 2952, 'stur': 2953, 'sturmgewehr': 2954, 'stärker': 2955, 'stärksten': 2956, 'stütze': 2957, 'substanziell': 2958, 'suchen': 2959, 'super': 2960, 'susanne': 2961, 'systematisch': 2962, 'systemrelevante': 2963, 'system\\xaderhalterinnen': 2964, 'sämtlicher': 2965, 'sätze': 2966, 'säule': 2967, 'säulen': 2968, 'südamerika': 2969, 'südosteuropa': 2970, 'tagaus': 2971, 'tagein': 2972, 'tagesordnungspunkt': 2973, 'takes': 2974, 'tatsächliche': 2975, 'tatsächlichen': 2976, 'tech\\xadnisch': 2977, 'teilnahme': 2978, 'teilnimmt': 2979, 'teilweise': 2980, 'teilzeit': 2981, 'teil\\xadweise': 2982, 'telefonisch': 2983, 'testprogramme': 2984, 'testungen': 2985, 'themas': 2986, 'tiefschlaf': 2987, 'tierwohlge\\xadrechtes': 2988, 'tirana': 2989, 'tirol': 2990, 'tiroler': 2991, 'titelte': 2992, 'todesfälle': 2993, 'todesfällen': 2994, 'todesstoß': 2995, 'tollerei': 2996, 'tontechniker': 2997, 'tore': 2998, 'totale': 2999, 'tote': 3000, 'toten': 3001, 'tourismusausschusses': 3002, 'tourismussprecherin': 3003, 'tradition': 3004, 'traditionelle': 3005, 'traditionellen': 3006, 'trauen': 3007, 'treffe': 3008, 'tritt': 3009, 'trotz': 3010, 'trump': 3011, 'tu': 3012, 'tut': 3013, 'tv': 3014, 'tägliche': 3015, 'tätig': 3016, 'tätigkeit': 3017, 'türkisen': 3018, 'u.a': 3019, 'umfangreich': 3020, 'umfassenden': 3021, 'umfassender': 3022, 'umfeld': 3023, 'umformen': 3024, 'umfragewerte': 3025, 'umgegangen': 3026, 'umgehen': 3027, 'umgesetzt': 3028, 'umsatzausfall': 3029, 'umsatzeinbruch': 3030, 'umsatzes': 3031, 'umsatzteile': 3032, 'umsatz\\xadersatz': 3033, 'umso': 3034, 'umsätze': 3035, 'umwelteinflüsse': 3036, 'umweltförderungsgesetz': 3037, 'umzustellen': 3038, 'unausgegorenen': 3039, 'unbedingt': 3040, 'unbegreiflicher': 3041, 'unbemerkte': 3042, 'unbürokratisch': 3043, 'unbürokratische': 3044, 'unbüro\\xadkratisch': 3045, 'unerklärlichen': 3046, 'unge\\xadrecht': 3047, 'unglaublichen': 3048, 'ungleichheit': 3049, 'union': 3050, 'unmit\\xadtelbar': 3051, 'unnötiger': 3052, 'unpfändbarkeit': 3053, 'unplanbarkeit': 3054, 'unschuldig': 3055, 'unschuldige': 3056, 'unseren': 3057, 'unse\\xadre': 3058, 'untergehen': 3059, 'unternehmensunterstützungen': 3060, 'unternehmer_innen': 3061, 'unternehmerlohn': 3062, 'unternehmern': 3063, 'unternommen': 3064, 'unterricht': 3065, 'unterrichtsstunden': 3066, 'untersagt': 3067, 'unterschiedliche': 3068, 'unterschiedlichste': 3069, 'unterschriften': 3070, 'unterstufe': 3071, 'unterstützungen': 3072, 'unterstützungseinrichtungen': 3073, 'untersuchungen': 3074, 'unterwegs': 3075, 'unterzeichnenden': 3076, 'unter\\xadricht': 3077, 'unter\\xadschiede': 3078, 'unter\\xadstützen': 3079, 'untätigkeit': 3080, 'unverantwortlich': 3081, 'unverzüglich': 3082, 'unzulänglich': 3083, 'unzählige': 3084, 'unübersehbar': 3085, 'up': 3086, 'urlaub': 3087, 'urlaubs-': 3088, 'usa': 3089, 'varianten': 3090, 'veraltet': 3091, 'verankert': 3092, 'veranstaltungs': 3093, 'veranstaltungsbranche': 3094, 'verantwortlich': 3095, 'verantwor\\xadtungsträger': 3096, 'verbessert': 3097, 'verbindungen': 3098, 'verboten': 3099, 'verbreitet': 3100, 'verbunden': 3101, 'verbundenen': 3102, 'verdienstentgang': 3103, 'vereinfachen': 3104, 'vereinsmitglieder': 3105, 'verfassungswidrig': 3106, 'vergangen': 3107, 'verhandeln': 3108, 'verharmlost': 3109, 'verheerend': 3110, 'verhin\\xaddert': 3111, 'verhängen': 3112, 'verhängung': 3113, 'verkaufen': 3114, 'verkündeten': 3115, 'verlas\\xadsen': 3116, 'verlegen': 3117, 'verliehen': 3118, 'verlieren': 3119, 'verliererinnen': 3120, 'verlorene': 3121, 'verlustausgleich': 3122, 'verlängern': 3123, 'verlängerung': 3124, 'verlässlichkeit': 3125, 'vermeintliche': 3126, 'vermisst': 3127, 'vermutlich': 3128, 'verordnungsermächtigungen': 3129, 'versagens': 3130, 'versagerinnen': 3131, 'versammeln': 3132, 'versandhandel': 3133, 'versandkosten': 3134, 'verschafft': 3135, 'verschieben': 3136, 'verschlechtern': 3137, 'verschoben': 3138, 'verschärfen': 3139, 'verschärfter': 3140, 'verschärfung': 3141, 'versorgungsengpässe': 3142, 'verspätet': 3143, 'verstanden': 3144, 'verstehen': 3145, 'verstärkung': 3146, 'versäumnisse': 3147, 'versäumt': 3148, 'vertragsbediensteten': 3149, 'vertrauten': 3150, 'verunsichern': 3151, 'verunsicherung': 3152, 'vervierfacht': 3153, 'verwalten': 3154, 'verwechselt': 3155, 'verwirklichen': 3156, 'verzapft': 3157, 'verzug': 3158, 'ver\\xadfügung': 3159, 'ver\\xadordnete': 3160, 'ver\\xadsprochen': 3161, 'vieler': 3162, 'vielfach': 3163, 'vielfaches': 3164, 'vielfalt': 3165, 'vielzahl': 3166, 'viermal': 3167, 'virusleugner': 3168, 'vizekanzler': 3169, 'volksbegehren': 3170, 'volkspartei': 3171, 'volksschule': 3172, 'volkswirtschaft': 3173, 'volks\\xadwirtschaft': 3174, 'voll': 3175, 'vollkommen': 3176, 'vonseiten': 3177, 'vorab': 3178, 'vorankommen': 3179, 'vorbei': 3180, 'vorbereitet': 3181, 'vorbereitungsmaßnahmen': 3182, 'vorbildland': 3183, 'vorgelegte': 3184, 'vorgenommen': 3185, 'vorgesetzten': 3186, 'vorhin': 3187, 'vorjahr': 3188, 'vorjahresmonats': 3189, 'vorjahres\\xadzeitraum': 3190, 'vorlage': 3191, 'vorrednerin': 3192, 'vorreiter': 3193, 'vorschau': 3194, 'vorschlag': 3195, 'vorschlägt': 3196, 'vorsichtige': 3197, 'vorteil': 3198, 'vorteile': 3199, 'vorwurf': 3200, 'vor\\xadherrschen': 3201, 'völlig': 3202, 'völliger': 3203, 'wachstum': 3204, 'wachstumsprognosen': 3205, 'wagen': 3206, 'wahnsinn': 3207, 'wahr': 3208, 'wahrgenommen': 3209, 'wahr\\xadscheinlich': 3210, 'wand': 3211, 'wandel': 3212, 'warnen': 3213, 'weggeblieben': 3214, 'weggebrochen': 3215, 'wegge\\xadbrochen': 3216, 'wegnehmen': 3217, 'wegweisungen': 3218, 'wehgetan': 3219, 'wehrlos': 3220, 'weihnachts\\xadgeschäft': 3221, 'wein': 3222, 'weiterentwickelt': 3223, 'weitergeht': 3224, 'weiters': 3225, 'weiter\\xadgelaufen': 3226, 'welcher': 3227, 'welches': 3228, 'weltweiten': 3229, 'welt\\xadweiten': 3230, 'wen': 3231, 'wenden': 3232, 'wenig': 3233, 'wenige': 3234, 'wenigstens': 3235, 'werde': 3236, 'wert': 3237, 'wertschöpfungsketten': 3238, 'wert\\xadschöpfungskette': 3239, 'wesentlich': 3240, 'wesentliche': 3241, 'wesentlichen': 3242, 'wesent\\xadlichen': 3243, 'westbahn': 3244, 'we\\xadge': 3245, 'whatever': 3246, 'wichtigen': 3247, 'wichtigsten': 3248, 'widersprechen': 3249, 'wiederherzustellen': 3250, 'wien.orf.at': 3251, 'wiener': 3252, 'will': 3253, 'willkommen': 3254, 'willkürlich': 3255, 'winter': 3256, 'wintertourismus': 3257, 'wirkenden': 3258, 'wirklichkeit': 3259, 'wirksam': 3260, 'wirksame': 3261, 'wirksamkeit': 3262, 'wirkung': 3263, 'wirtschaftsausschuss': 3264, 'wirtschaftsexperte': 3265, 'wirtschaftsforschern': 3266, 'wirtschaftsprognosen': 3267, 'wirtschaftsreparaturpaket': 3268, 'wirtschaftssystem': 3269, 'wirt\\xadschaft': 3270, 'wissend': 3271, 'wissenschaftliche': 3272, 'wissenschaftsfreiheit': 3273, 'wissensstand': 3274, 'wissentlich': 3275, 'wko': 3276, 'wkö': 3277, 'wohle': 3278, 'wollt': 3279, 'worten': 3280, 'wozu': 3281, 'wucht': 3282, 'wunderschön': 3283, 'wunsch': 3284, 'wurm': 3285, 'wussten': 3286, 'wäh\\xadrend': 3287, 'wände': 3288, 'wöchentlich': 3289, 'wünschen': 3290, 'würden': 3291, 'wüssten': 3292, 'zadić': 3293, 'zahlenmäßig': 3294, 'zahlenwerk': 3295, 'zahlung': 3296, 'zah\\xadlenwerk': 3297, 'zeche': 3298, 'zehntausende': 3299, 'zeigt': 3300, 'zeitge\\xadrecht': 3301, 'zeitgleich': 3302, 'zeitungen': 3303, 'zeitverwendungsstudie': 3304, 'zentral': 3305, 'zerreißen': 3306, 'zertrümmert': 3307, 'zickzackkurs': 3308, 'ziehen': 3309, 'zielgruppen': 3310, 'ziemlich': 3311, 'zitieren': 3312, 'zittern': 3313, 'zoom': 3314, 'zudrehen': 3315, 'zufolge': 3316, 'zugestimmt': 3317, 'zugleich': 3318, 'zugute': 3319, 'zulieferbetrieben': 3320, 'zumutbaren': 3321, 'zunimmt': 3322, 'zunächst': 3323, 'zurecht': 3324, 'zurechtrücken': 3325, 'zurückgelehnt': 3326, 'zurückgenommen': 3327, 'zurückkehren': 3328, 'zurückrutschen': 3329, 'zurückversetzt': 3330, 'zusammen': 3331, 'zusammenbringen': 3332, 'zusammengebracht': 3333, 'zusammengekom\\xadmen': 3334, 'zusammenspiel': 3335, 'zusam\\xadmenbruch': 3336, 'zusam\\xadmenhang': 3337, 'zuschriften': 3338, 'zuschuss': 3339, 'zustrom': 3340, 'zustände': 3341, 'zuständen': 3342, 'zuständig': 3343, 'zusätzlichen': 3344, 'zu\\xadrückgehen': 3345, 'zwei\\xadten': 3346, '\\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0 ': 3347, 'ähnlich': 3348, 'ähnliche': 3349, 'ähnliches': 3350, 'ärztinnen': 3351, 'äu\\xadßerst': 3352, 'äußersten': 3353, 'öbb': 3354, 'öffnete': 3355, 'öfters': 3356, 'ökonomisch': 3357, 'österreichern': 3358, 'österreiche\\xadrinnen': 3359, 'österreichi\\xadschen': 3360, 'österreichweit': 3361, 'ös\\xadterreich': 3362, 'überall': 3363, 'überarbeiten': 3364, 'überaus': 3365, 'überbordenden': 3366, 'überbrückungsfinanzierung': 3367, 'überholt': 3368, 'überlagert': 3369, 'überlastung': 3370, 'übernehmen': 3371, 'übernervöse': 3372, 'überprüfung': 3373, 'überschreiten': 3374, 'überschritten': 3375, 'überstanden': 3376, 'überwiegenden': 3377, 'überzeugt': 3378, 'über\\xadgriffe': 3379, 'über\\xadlastet': 3380, 'üblich': 3381, '”': 3382, '•': 3383})\n"
     ]
    }
   ],
   "source": [
    "#initialize glove embeddings\n",
    "# the custom vectors we use in de_glove_deepsetai.txt have 300 dimensions\n",
    "de_embeddings = torchtext.vocab.Vectors(name = 'vectors/de_glove_deepsetai.txt')\n",
    "TEXT.build_vocab(training_data,min_freq=1,vectors = de_embeddings)\n",
    "LABEL.build_vocab(training_data)\n",
    "\n",
    "#No. of unique tokens in text\n",
    "print(\"Size of TEXT vocabulary:\",len(TEXT.vocab))\n",
    "\n",
    "#No. of unique tokens in label\n",
    "print(\"Size of LABEL vocabulary:\",len(LABEL.vocab))\n",
    "\n",
    "#Commonly used words\n",
    "print(TEXT.vocab.freqs.most_common(10))\n",
    "\n",
    "#Word dictionary\n",
    "print(TEXT.vocab.stoi)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "#check whether cuda is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class classifier(nn.Module):\n",
    "\n",
    "    #define all the layers used in model\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers,\n",
    "                 bidirectional, dropout):\n",
    "\n",
    "        #Constructor\n",
    "        super().__init__()\n",
    "\n",
    "        #embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        #lstm layer\n",
    "        self.lstm = nn.LSTM(embedding_dim,\n",
    "                           hidden_dim,\n",
    "                           num_layers=n_layers,\n",
    "                           bidirectional=bidirectional,\n",
    "                           dropout=dropout,\n",
    "                           batch_first=True)\n",
    "\n",
    "        #dense layer\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "\n",
    "        #activation function\n",
    "        self.act = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, text, text_lengths):\n",
    "\n",
    "        #text = [batch size,sent_length]\n",
    "        embedded = self.embedding(text)\n",
    "        #embedded = [batch size, sent_len, emb dim]\n",
    "\n",
    "        #packed sequence\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths,batch_first=True)\n",
    "\n",
    "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
    "        #hidden = [batch size, num layers * num directions,hid dim]\n",
    "        #cell = [batch size, num layers * num directions,hid dim]\n",
    "\n",
    "        #concat the final forward and backward hidden state\n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n",
    "\n",
    "        #hidden = [batch size, hid dim * num directions]\n",
    "        dense_outputs=self.fc(hidden)\n",
    "\n",
    "        #Final activation function\n",
    "        outputs=self.act(dense_outputs)\n",
    "\n",
    "        return outputs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "#define hyperparameters\n",
    "size_of_vocab = len(TEXT.vocab)\n",
    "embedding_dim = 300  # the custom vectors we use in de_glove_deepsetai.txt have 300 dimensions\n",
    "num_hidden_nodes = 32\n",
    "num_output_nodes = 3  # we have 3 different labels (-/o/+) so we need 3 output nodes.\n",
    "num_layers = 2\n",
    "bidirection = True\n",
    "dropout = 0.2\n",
    "\n",
    "#instantiate the model\n",
    "model = classifier(size_of_vocab, embedding_dim, num_hidden_nodes,num_output_nodes, num_layers,\n",
    "                   bidirectional = True, dropout = dropout)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "#define metric\n",
    "def binary_accuracy(preds, y):\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.argmax(preds, dim=-1)\n",
    "\n",
    "    correct = (rounded_preds == y)\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "\n",
    "    #initialize every epoch\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    #set the model in training phase\n",
    "    model.train()\n",
    "\n",
    "    for batch in iterator:\n",
    "\n",
    "        #resets the gradients after every batch\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #retrieve text and no. of words\n",
    "        text, text_lengths = batch.text\n",
    "\n",
    "        # predictions has list of likelihoods for the 3 classes for each sample.\n",
    "        predictions = model(text, text_lengths)\n",
    "\n",
    "        #compute the loss\n",
    "        loss = criterion(predictions, batch.label.type(torch.LongTensor))\n",
    "\n",
    "        #compute the binary accuracy\n",
    "        acc = binary_accuracy(predictions, batch.label.type(torch.LongTensor))\n",
    "\n",
    "        #backpropage the loss and compute the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        #update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        #loss and accuracy\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "\n",
    "    #initialize every epoch\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    #deactivating dropout layers\n",
    "    model.eval()\n",
    "\n",
    "    #deactivates autograd\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for batch in iterator:\n",
    "\n",
    "            #retrieve text and no. of words\n",
    "            text, text_lengths = batch.text\n",
    "\n",
    "            #convert to 1d tensor\n",
    "            predictions = model(text, text_lengths)\n",
    "\n",
    "            #compute loss and accuracy\n",
    "            loss = criterion(predictions, batch.label.type(torch.LongTensor))\n",
    "            acc = binary_accuracy(predictions, batch.label.type(torch.LongTensor))\n",
    "\n",
    "            #keep track of loss and accuracy\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "def random_train_test_split(split=0.8):\n",
    "    #set batch size\n",
    "    BATCH_SIZE = 16\n",
    "    neg_train = int(len(negative) * 0.8)\n",
    "    neut_train = int(len(neutral) * 0.8)\n",
    "    pos_train = int(len(positive) * 0.8)\n",
    "\n",
    "    import random\n",
    "    random.shuffle(positive)\n",
    "    random.shuffle(negative)\n",
    "    random.shuffle(neutral)\n",
    "    train_data = data.Dataset(positive[:pos_train] + negative[:neg_train] + neutral[:neut_train], fields)\n",
    "    valid_data = data.Dataset(positive[pos_train:] + negative[neg_train:] + neutral[neut_train:], fields)\n",
    "    #Load an iterator\n",
    "    train_iterator, valid_iterator = data.BucketIterator.splits(\n",
    "        (train_data, valid_data),\n",
    "        batch_size = BATCH_SIZE,\n",
    "        sort_key = lambda x: len(x.text),\n",
    "        sort_within_batch=True,\n",
    "        shuffle=True,\n",
    "        device = device)\n",
    "    return train_iterator, valid_iterator, neg_train, neut_train, pos_train\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 1.100 | Train Acc: 18.75%\n",
      "\t Val. Loss: 1.098 |  Val. Acc: 25.89%\n",
      "\tTrain Loss: 1.094 | Train Acc: 57.00%\n",
      "\t Val. Loss: 1.098 |  Val. Acc: 52.68%\n",
      "\tTrain Loss: 1.082 | Train Acc: 62.75%\n",
      "\t Val. Loss: 1.096 |  Val. Acc: 46.43%\n",
      "\tTrain Loss: 1.013 | Train Acc: 71.25%\n",
      "\t Val. Loss: 1.058 |  Val. Acc: 51.79%\n",
      "\tTrain Loss: 0.850 | Train Acc: 82.75%\n",
      "\t Val. Loss: 1.058 |  Val. Acc: 43.75%\n",
      "\tTrain Loss: 0.702 | Train Acc: 92.75%\n",
      "\t Val. Loss: 1.058 |  Val. Acc: 57.14%\n",
      "\tTrain Loss: 0.613 | Train Acc: 97.50%\n",
      "\t Val. Loss: 1.068 |  Val. Acc: 58.04%\n",
      "\tTrain Loss: 0.589 | Train Acc: 98.00%\n",
      "\t Val. Loss: 1.118 |  Val. Acc: 47.32%\n",
      "\tTrain Loss: 0.572 | Train Acc: 99.25%\n",
      "\t Val. Loss: 1.076 |  Val. Acc: 58.04%\n",
      "\tTrain Loss: 0.566 | Train Acc: 99.25%\n",
      "\t Val. Loss: 1.087 |  Val. Acc: 54.46%\n",
      "\tTrain Loss: 0.563 | Train Acc: 99.50%\n",
      "\t Val. Loss: 1.071 |  Val. Acc: 58.04%\n",
      "\tTrain Loss: 0.559 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.057 |  Val. Acc: 57.14%\n",
      "\tTrain Loss: 0.558 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.076 |  Val. Acc: 55.36%\n",
      "\tTrain Loss: 0.556 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.052 |  Val. Acc: 58.04%\n",
      "\tTrain Loss: 0.556 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.057 |  Val. Acc: 58.04%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.047 |  Val. Acc: 59.82%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.047 |  Val. Acc: 58.93%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.044 |  Val. Acc: 58.93%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.044 |  Val. Acc: 58.93%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.037 |  Val. Acc: 59.82%\n",
      "\tTrain Loss: 1.100 | Train Acc: 18.00%\n",
      "\t Val. Loss: 1.098 |  Val. Acc: 19.64%\n",
      "\tTrain Loss: 1.093 | Train Acc: 42.00%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 52.68%\n",
      "\tTrain Loss: 1.072 | Train Acc: 59.00%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 47.32%\n",
      "\tTrain Loss: 0.994 | Train Acc: 67.00%\n",
      "\t Val. Loss: 1.076 |  Val. Acc: 39.29%\n",
      "\tTrain Loss: 0.874 | Train Acc: 78.25%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 48.21%\n",
      "\tTrain Loss: 0.749 | Train Acc: 87.75%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 46.43%\n",
      "\tTrain Loss: 0.662 | Train Acc: 92.75%\n",
      "\t Val. Loss: 1.112 |  Val. Acc: 53.57%\n",
      "\tTrain Loss: 0.621 | Train Acc: 96.25%\n",
      "\t Val. Loss: 1.151 |  Val. Acc: 50.89%\n",
      "\tTrain Loss: 0.581 | Train Acc: 99.00%\n",
      "\t Val. Loss: 1.177 |  Val. Acc: 48.21%\n",
      "\tTrain Loss: 0.569 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.116 |  Val. Acc: 48.21%\n",
      "\tTrain Loss: 0.564 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.133 |  Val. Acc: 48.21%\n",
      "\tTrain Loss: 0.560 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.154 |  Val. Acc: 47.32%\n",
      "\tTrain Loss: 0.558 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.134 |  Val. Acc: 50.00%\n",
      "\tTrain Loss: 0.557 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.128 |  Val. Acc: 50.89%\n",
      "\tTrain Loss: 0.556 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.139 |  Val. Acc: 48.21%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.151 |  Val. Acc: 47.32%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.153 |  Val. Acc: 47.32%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.156 |  Val. Acc: 47.32%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.159 |  Val. Acc: 46.43%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.160 |  Val. Acc: 47.32%\n",
      "\tTrain Loss: 1.099 | Train Acc: 28.75%\n",
      "\t Val. Loss: 1.096 |  Val. Acc: 50.89%\n",
      "\tTrain Loss: 1.093 | Train Acc: 60.00%\n",
      "\t Val. Loss: 1.095 |  Val. Acc: 46.43%\n",
      "\tTrain Loss: 1.075 | Train Acc: 62.50%\n",
      "\t Val. Loss: 1.090 |  Val. Acc: 53.57%\n",
      "\tTrain Loss: 0.966 | Train Acc: 75.25%\n",
      "\t Val. Loss: 1.080 |  Val. Acc: 53.57%\n",
      "\tTrain Loss: 0.818 | Train Acc: 85.75%\n",
      "\t Val. Loss: 1.050 |  Val. Acc: 56.25%\n",
      "\tTrain Loss: 0.707 | Train Acc: 94.25%\n",
      "\t Val. Loss: 1.086 |  Val. Acc: 54.46%\n",
      "\tTrain Loss: 0.658 | Train Acc: 95.25%\n",
      "\t Val. Loss: 1.068 |  Val. Acc: 44.64%\n",
      "\tTrain Loss: 0.602 | Train Acc: 99.00%\n",
      "\t Val. Loss: 1.019 |  Val. Acc: 53.57%\n",
      "\tTrain Loss: 0.578 | Train Acc: 99.25%\n",
      "\t Val. Loss: 1.018 |  Val. Acc: 56.25%\n",
      "\tTrain Loss: 0.573 | Train Acc: 99.25%\n",
      "\t Val. Loss: 1.011 |  Val. Acc: 57.14%\n",
      "\tTrain Loss: 0.568 | Train Acc: 99.50%\n",
      "\t Val. Loss: 1.016 |  Val. Acc: 54.46%\n",
      "\tTrain Loss: 0.563 | Train Acc: 99.50%\n",
      "\t Val. Loss: 1.009 |  Val. Acc: 57.14%\n",
      "\tTrain Loss: 0.561 | Train Acc: 99.50%\n",
      "\t Val. Loss: 1.075 |  Val. Acc: 59.82%\n",
      "\tTrain Loss: 0.559 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.069 |  Val. Acc: 58.04%\n",
      "\tTrain Loss: 0.566 | Train Acc: 99.25%\n",
      "\t Val. Loss: 1.025 |  Val. Acc: 58.93%\n",
      "\tTrain Loss: 0.560 | Train Acc: 99.50%\n",
      "\t Val. Loss: 1.076 |  Val. Acc: 58.93%\n",
      "\tTrain Loss: 0.556 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.056 |  Val. Acc: 48.21%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.059 |  Val. Acc: 55.36%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.053 |  Val. Acc: 56.25%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.053 |  Val. Acc: 55.36%\n",
      "\tTrain Loss: 1.100 | Train Acc: 17.75%\n",
      "\t Val. Loss: 1.098 |  Val. Acc: 26.79%\n",
      "\tTrain Loss: 1.095 | Train Acc: 36.25%\n",
      "\t Val. Loss: 1.098 |  Val. Acc: 47.32%\n",
      "\tTrain Loss: 1.079 | Train Acc: 65.75%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 49.11%\n",
      "\tTrain Loss: 1.001 | Train Acc: 67.50%\n",
      "\t Val. Loss: 1.086 |  Val. Acc: 38.39%\n",
      "\tTrain Loss: 0.866 | Train Acc: 80.25%\n",
      "\t Val. Loss: 1.069 |  Val. Acc: 47.32%\n",
      "\tTrain Loss: 0.719 | Train Acc: 94.50%\n",
      "\t Val. Loss: 1.077 |  Val. Acc: 46.43%\n",
      "\tTrain Loss: 0.625 | Train Acc: 98.00%\n",
      "\t Val. Loss: 1.055 |  Val. Acc: 52.68%\n",
      "\tTrain Loss: 0.588 | Train Acc: 99.50%\n",
      "\t Val. Loss: 1.058 |  Val. Acc: 52.68%\n",
      "\tTrain Loss: 0.569 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 49.11%\n",
      "\tTrain Loss: 0.564 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.042 |  Val. Acc: 46.43%\n",
      "\tTrain Loss: 0.563 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.078 |  Val. Acc: 50.00%\n",
      "\tTrain Loss: 0.559 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.077 |  Val. Acc: 50.00%\n",
      "\tTrain Loss: 0.557 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.085 |  Val. Acc: 49.11%\n",
      "\tTrain Loss: 0.556 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.087 |  Val. Acc: 49.11%\n",
      "\tTrain Loss: 0.556 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.084 |  Val. Acc: 49.11%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.084 |  Val. Acc: 50.00%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.083 |  Val. Acc: 50.89%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.079 |  Val. Acc: 50.89%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.077 |  Val. Acc: 50.89%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.075 |  Val. Acc: 52.68%\n",
      "\tTrain Loss: 1.099 | Train Acc: 56.50%\n",
      "\t Val. Loss: 1.098 |  Val. Acc: 58.04%\n",
      "\tTrain Loss: 1.093 | Train Acc: 56.50%\n",
      "\t Val. Loss: 1.098 |  Val. Acc: 58.04%\n",
      "\tTrain Loss: 1.070 | Train Acc: 60.25%\n",
      "\t Val. Loss: 1.087 |  Val. Acc: 57.14%\n",
      "\tTrain Loss: 0.965 | Train Acc: 74.50%\n",
      "\t Val. Loss: 1.077 |  Val. Acc: 57.14%\n",
      "\tTrain Loss: 0.897 | Train Acc: 77.00%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 45.54%\n",
      "\tTrain Loss: 0.757 | Train Acc: 88.75%\n",
      "\t Val. Loss: 1.074 |  Val. Acc: 60.71%\n",
      "\tTrain Loss: 0.655 | Train Acc: 96.25%\n",
      "\t Val. Loss: 1.064 |  Val. Acc: 56.25%\n",
      "\tTrain Loss: 0.601 | Train Acc: 98.25%\n",
      "\t Val. Loss: 1.074 |  Val. Acc: 58.04%\n",
      "\tTrain Loss: 0.577 | Train Acc: 99.25%\n",
      "\t Val. Loss: 1.053 |  Val. Acc: 58.04%\n",
      "\tTrain Loss: 0.569 | Train Acc: 99.50%\n",
      "\t Val. Loss: 1.095 |  Val. Acc: 53.57%\n",
      "\tTrain Loss: 0.562 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.079 |  Val. Acc: 52.68%\n",
      "\tTrain Loss: 0.560 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.078 |  Val. Acc: 57.14%\n",
      "\tTrain Loss: 0.558 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.090 |  Val. Acc: 54.46%\n",
      "\tTrain Loss: 0.556 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 55.36%\n",
      "\tTrain Loss: 0.556 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.086 |  Val. Acc: 55.36%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.087 |  Val. Acc: 55.36%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 54.46%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.087 |  Val. Acc: 55.36%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.086 |  Val. Acc: 55.36%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.087 |  Val. Acc: 54.46%\n",
      "\tTrain Loss: 1.100 | Train Acc: 34.25%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 50.89%\n",
      "\tTrain Loss: 1.095 | Train Acc: 57.25%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 50.89%\n",
      "\tTrain Loss: 1.083 | Train Acc: 61.25%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 45.54%\n",
      "\tTrain Loss: 1.023 | Train Acc: 64.50%\n",
      "\t Val. Loss: 1.110 |  Val. Acc: 43.75%\n",
      "\tTrain Loss: 0.844 | Train Acc: 83.00%\n",
      "\t Val. Loss: 1.135 |  Val. Acc: 40.18%\n",
      "\tTrain Loss: 0.704 | Train Acc: 92.50%\n",
      "\t Val. Loss: 1.087 |  Val. Acc: 36.61%\n",
      "\tTrain Loss: 0.626 | Train Acc: 96.75%\n",
      "\t Val. Loss: 1.131 |  Val. Acc: 42.86%\n",
      "\tTrain Loss: 0.589 | Train Acc: 98.25%\n",
      "\t Val. Loss: 1.129 |  Val. Acc: 42.86%\n",
      "\tTrain Loss: 0.575 | Train Acc: 99.50%\n",
      "\t Val. Loss: 1.121 |  Val. Acc: 50.00%\n",
      "\tTrain Loss: 0.570 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.125 |  Val. Acc: 51.79%\n",
      "\tTrain Loss: 0.566 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.082 |  Val. Acc: 46.43%\n",
      "\tTrain Loss: 0.570 | Train Acc: 98.75%\n",
      "\t Val. Loss: 1.116 |  Val. Acc: 48.21%\n",
      "\tTrain Loss: 0.568 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.098 |  Val. Acc: 53.57%\n",
      "\tTrain Loss: 0.563 | Train Acc: 99.50%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 45.54%\n",
      "\tTrain Loss: 0.557 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.129 |  Val. Acc: 47.32%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.126 |  Val. Acc: 50.89%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.129 |  Val. Acc: 50.89%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.130 |  Val. Acc: 50.89%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.131 |  Val. Acc: 50.00%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.130 |  Val. Acc: 49.11%\n",
      "\tTrain Loss: 1.099 | Train Acc: 56.25%\n",
      "\t Val. Loss: 1.096 |  Val. Acc: 58.04%\n",
      "\tTrain Loss: 1.093 | Train Acc: 57.00%\n",
      "\t Val. Loss: 1.094 |  Val. Acc: 59.82%\n",
      "\tTrain Loss: 1.078 | Train Acc: 59.25%\n",
      "\t Val. Loss: 1.085 |  Val. Acc: 61.61%\n",
      "\tTrain Loss: 1.005 | Train Acc: 68.50%\n",
      "\t Val. Loss: 1.054 |  Val. Acc: 51.79%\n",
      "\tTrain Loss: 0.887 | Train Acc: 80.00%\n",
      "\t Val. Loss: 1.047 |  Val. Acc: 49.11%\n",
      "\tTrain Loss: 0.777 | Train Acc: 91.25%\n",
      "\t Val. Loss: 1.018 |  Val. Acc: 52.68%\n",
      "\tTrain Loss: 0.677 | Train Acc: 96.50%\n",
      "\t Val. Loss: 1.028 |  Val. Acc: 42.86%\n",
      "\tTrain Loss: 0.621 | Train Acc: 99.25%\n",
      "\t Val. Loss: 1.045 |  Val. Acc: 56.25%\n",
      "\tTrain Loss: 0.584 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.056 |  Val. Acc: 51.79%\n",
      "\tTrain Loss: 0.569 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.039 |  Val. Acc: 53.57%\n",
      "\tTrain Loss: 0.563 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.052 |  Val. Acc: 51.79%\n",
      "\tTrain Loss: 0.560 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.050 |  Val. Acc: 52.68%\n",
      "\tTrain Loss: 0.558 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.058 |  Val. Acc: 53.57%\n",
      "\tTrain Loss: 0.557 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.056 |  Val. Acc: 53.57%\n",
      "\tTrain Loss: 0.556 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.061 |  Val. Acc: 53.57%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.066 |  Val. Acc: 55.36%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.066 |  Val. Acc: 56.25%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.068 |  Val. Acc: 54.46%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.063 |  Val. Acc: 55.36%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.065 |  Val. Acc: 55.36%\n",
      "\tTrain Loss: 1.100 | Train Acc: 25.75%\n",
      "\t Val. Loss: 1.098 |  Val. Acc: 50.00%\n",
      "\tTrain Loss: 1.094 | Train Acc: 57.25%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 53.57%\n",
      "\tTrain Loss: 1.084 | Train Acc: 58.50%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 50.00%\n",
      "\tTrain Loss: 1.031 | Train Acc: 64.50%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 31.25%\n",
      "\tTrain Loss: 0.886 | Train Acc: 76.00%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 47.32%\n",
      "\tTrain Loss: 0.742 | Train Acc: 89.75%\n",
      "\t Val. Loss: 1.132 |  Val. Acc: 40.18%\n",
      "\tTrain Loss: 0.646 | Train Acc: 96.25%\n",
      "\t Val. Loss: 1.168 |  Val. Acc: 33.04%\n",
      "\tTrain Loss: 0.599 | Train Acc: 98.75%\n",
      "\t Val. Loss: 1.132 |  Val. Acc: 43.75%\n",
      "\tTrain Loss: 0.581 | Train Acc: 99.00%\n",
      "\t Val. Loss: 1.156 |  Val. Acc: 43.75%\n",
      "\tTrain Loss: 0.570 | Train Acc: 99.25%\n",
      "\t Val. Loss: 1.157 |  Val. Acc: 42.86%\n",
      "\tTrain Loss: 0.564 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.163 |  Val. Acc: 43.75%\n",
      "\tTrain Loss: 0.562 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.152 |  Val. Acc: 40.18%\n",
      "\tTrain Loss: 0.559 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.165 |  Val. Acc: 43.75%\n",
      "\tTrain Loss: 0.559 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.167 |  Val. Acc: 45.54%\n",
      "\tTrain Loss: 0.557 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.157 |  Val. Acc: 37.50%\n",
      "\tTrain Loss: 0.556 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.150 |  Val. Acc: 39.29%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.145 |  Val. Acc: 43.75%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.143 |  Val. Acc: 44.64%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.143 |  Val. Acc: 43.75%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.146 |  Val. Acc: 43.75%\n",
      "\tTrain Loss: 1.099 | Train Acc: 43.75%\n",
      "\t Val. Loss: 1.096 |  Val. Acc: 38.39%\n",
      "\tTrain Loss: 1.094 | Train Acc: 55.25%\n",
      "\t Val. Loss: 1.096 |  Val. Acc: 57.14%\n",
      "\tTrain Loss: 1.076 | Train Acc: 59.50%\n",
      "\t Val. Loss: 1.091 |  Val. Acc: 57.14%\n",
      "\tTrain Loss: 1.009 | Train Acc: 68.00%\n",
      "\t Val. Loss: 1.086 |  Val. Acc: 43.75%\n",
      "\tTrain Loss: 0.873 | Train Acc: 77.25%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 49.11%\n",
      "\tTrain Loss: 0.719 | Train Acc: 91.50%\n",
      "\t Val. Loss: 1.137 |  Val. Acc: 42.86%\n",
      "\tTrain Loss: 0.637 | Train Acc: 97.50%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 41.07%\n",
      "\tTrain Loss: 0.615 | Train Acc: 96.25%\n",
      "\t Val. Loss: 1.168 |  Val. Acc: 50.89%\n",
      "\tTrain Loss: 0.584 | Train Acc: 98.25%\n",
      "\t Val. Loss: 1.128 |  Val. Acc: 50.00%\n",
      "\tTrain Loss: 0.568 | Train Acc: 99.50%\n",
      "\t Val. Loss: 1.158 |  Val. Acc: 48.21%\n",
      "\tTrain Loss: 0.563 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.124 |  Val. Acc: 46.43%\n",
      "\tTrain Loss: 0.560 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.138 |  Val. Acc: 49.11%\n",
      "\tTrain Loss: 0.558 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.137 |  Val. Acc: 49.11%\n",
      "\tTrain Loss: 0.557 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.141 |  Val. Acc: 49.11%\n",
      "\tTrain Loss: 0.556 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.145 |  Val. Acc: 47.32%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.149 |  Val. Acc: 46.43%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.149 |  Val. Acc: 46.43%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.151 |  Val. Acc: 46.43%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.154 |  Val. Acc: 46.43%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.157 |  Val. Acc: 46.43%\n",
      "\tTrain Loss: 1.099 | Train Acc: 26.00%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 40.18%\n",
      "\tTrain Loss: 1.093 | Train Acc: 47.75%\n",
      "\t Val. Loss: 1.095 |  Val. Acc: 56.25%\n",
      "\tTrain Loss: 1.073 | Train Acc: 57.25%\n",
      "\t Val. Loss: 1.085 |  Val. Acc: 44.64%\n",
      "\tTrain Loss: 0.990 | Train Acc: 69.25%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 35.71%\n",
      "\tTrain Loss: 0.844 | Train Acc: 75.00%\n",
      "\t Val. Loss: 1.042 |  Val. Acc: 62.50%\n",
      "\tTrain Loss: 0.731 | Train Acc: 88.75%\n",
      "\t Val. Loss: 1.050 |  Val. Acc: 54.46%\n",
      "\tTrain Loss: 0.667 | Train Acc: 93.25%\n",
      "\t Val. Loss: 0.996 |  Val. Acc: 58.93%\n",
      "\tTrain Loss: 0.624 | Train Acc: 94.50%\n",
      "\t Val. Loss: 0.977 |  Val. Acc: 61.61%\n",
      "\tTrain Loss: 0.595 | Train Acc: 97.75%\n",
      "\t Val. Loss: 0.986 |  Val. Acc: 58.93%\n",
      "\tTrain Loss: 0.575 | Train Acc: 99.25%\n",
      "\t Val. Loss: 0.975 |  Val. Acc: 67.86%\n",
      "\tTrain Loss: 0.568 | Train Acc: 99.50%\n",
      "\t Val. Loss: 1.010 |  Val. Acc: 55.36%\n",
      "\tTrain Loss: 0.568 | Train Acc: 99.25%\n",
      "\t Val. Loss: 1.032 |  Val. Acc: 49.11%\n",
      "\tTrain Loss: 0.562 | Train Acc: 99.75%\n",
      "\t Val. Loss: 0.986 |  Val. Acc: 62.50%\n",
      "\tTrain Loss: 0.558 | Train Acc: 99.75%\n",
      "\t Val. Loss: 0.991 |  Val. Acc: 63.39%\n",
      "\tTrain Loss: 0.557 | Train Acc: 99.75%\n",
      "\t Val. Loss: 0.991 |  Val. Acc: 63.39%\n",
      "\tTrain Loss: 0.556 | Train Acc: 99.75%\n",
      "\t Val. Loss: 0.989 |  Val. Acc: 64.29%\n",
      "\tTrain Loss: 0.556 | Train Acc: 100.00%\n",
      "\t Val. Loss: 0.984 |  Val. Acc: 64.29%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 0.983 |  Val. Acc: 64.29%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 0.983 |  Val. Acc: 65.18%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 0.980 |  Val. Acc: 64.29%\n",
      "\tTrain Loss: 1.100 | Train Acc: 33.50%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 53.57%\n",
      "\tTrain Loss: 1.095 | Train Acc: 56.25%\n",
      "\t Val. Loss: 1.098 |  Val. Acc: 53.57%\n",
      "\tTrain Loss: 1.082 | Train Acc: 58.75%\n",
      "\t Val. Loss: 1.095 |  Val. Acc: 54.46%\n",
      "\tTrain Loss: 0.996 | Train Acc: 70.00%\n",
      "\t Val. Loss: 1.112 |  Val. Acc: 47.32%\n",
      "\tTrain Loss: 0.810 | Train Acc: 85.75%\n",
      "\t Val. Loss: 1.094 |  Val. Acc: 38.39%\n",
      "\tTrain Loss: 0.711 | Train Acc: 90.00%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 53.57%\n",
      "\tTrain Loss: 0.620 | Train Acc: 96.50%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 50.00%\n",
      "\tTrain Loss: 0.584 | Train Acc: 98.75%\n",
      "\t Val. Loss: 1.122 |  Val. Acc: 52.68%\n",
      "\tTrain Loss: 0.571 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.096 |  Val. Acc: 47.32%\n",
      "\tTrain Loss: 0.566 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.080 |  Val. Acc: 50.00%\n",
      "\tTrain Loss: 0.564 | Train Acc: 99.50%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 43.75%\n",
      "\tTrain Loss: 0.561 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 50.00%\n",
      "\tTrain Loss: 0.559 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.132 |  Val. Acc: 51.79%\n",
      "\tTrain Loss: 0.557 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.128 |  Val. Acc: 50.89%\n",
      "\tTrain Loss: 0.557 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.124 |  Val. Acc: 50.89%\n",
      "\tTrain Loss: 0.556 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 51.79%\n",
      "\tTrain Loss: 0.556 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.119 |  Val. Acc: 51.79%\n",
      "\tTrain Loss: 0.555 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.118 |  Val. Acc: 51.79%\n",
      "\tTrain Loss: 0.555 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 51.79%\n",
      "\tTrain Loss: 0.555 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 54.46%\n",
      "\tTrain Loss: 1.099 | Train Acc: 56.00%\n",
      "\t Val. Loss: 1.095 |  Val. Acc: 60.71%\n",
      "\tTrain Loss: 1.094 | Train Acc: 57.50%\n",
      "\t Val. Loss: 1.094 |  Val. Acc: 55.36%\n",
      "\tTrain Loss: 1.076 | Train Acc: 60.75%\n",
      "\t Val. Loss: 1.089 |  Val. Acc: 52.68%\n",
      "\tTrain Loss: 0.977 | Train Acc: 69.25%\n",
      "\t Val. Loss: 1.090 |  Val. Acc: 33.93%\n",
      "\tTrain Loss: 0.807 | Train Acc: 85.00%\n",
      "\t Val. Loss: 1.081 |  Val. Acc: 53.57%\n",
      "\tTrain Loss: 0.688 | Train Acc: 94.50%\n",
      "\t Val. Loss: 1.108 |  Val. Acc: 40.18%\n",
      "\tTrain Loss: 0.632 | Train Acc: 97.50%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 50.00%\n",
      "\tTrain Loss: 0.591 | Train Acc: 99.00%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 48.21%\n",
      "\tTrain Loss: 0.578 | Train Acc: 99.00%\n",
      "\t Val. Loss: 1.065 |  Val. Acc: 58.04%\n",
      "\tTrain Loss: 0.569 | Train Acc: 99.50%\n",
      "\t Val. Loss: 1.094 |  Val. Acc: 50.89%\n",
      "\tTrain Loss: 0.565 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.073 |  Val. Acc: 55.36%\n",
      "\tTrain Loss: 0.565 | Train Acc: 99.50%\n",
      "\t Val. Loss: 1.098 |  Val. Acc: 50.89%\n",
      "\tTrain Loss: 0.560 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 50.89%\n",
      "\tTrain Loss: 0.559 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 50.00%\n",
      "\tTrain Loss: 0.558 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.108 |  Val. Acc: 47.32%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.094 |  Val. Acc: 49.11%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 48.21%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 49.11%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 48.21%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 49.11%\n",
      "\tTrain Loss: 1.101 | Train Acc: 25.00%\n",
      "\t Val. Loss: 1.098 |  Val. Acc: 25.89%\n",
      "\tTrain Loss: 1.096 | Train Acc: 42.50%\n",
      "\t Val. Loss: 1.095 |  Val. Acc: 54.46%\n",
      "\tTrain Loss: 1.081 | Train Acc: 63.50%\n",
      "\t Val. Loss: 1.091 |  Val. Acc: 22.32%\n",
      "\tTrain Loss: 0.998 | Train Acc: 66.75%\n",
      "\t Val. Loss: 1.095 |  Val. Acc: 50.00%\n",
      "\tTrain Loss: 0.813 | Train Acc: 86.75%\n",
      "\t Val. Loss: 1.130 |  Val. Acc: 45.54%\n",
      "\tTrain Loss: 0.682 | Train Acc: 94.75%\n",
      "\t Val. Loss: 1.118 |  Val. Acc: 48.21%\n",
      "\tTrain Loss: 0.630 | Train Acc: 97.00%\n",
      "\t Val. Loss: 1.124 |  Val. Acc: 47.32%\n",
      "\tTrain Loss: 0.596 | Train Acc: 98.50%\n",
      "\t Val. Loss: 1.138 |  Val. Acc: 52.68%\n",
      "\tTrain Loss: 0.583 | Train Acc: 98.75%\n",
      "\t Val. Loss: 1.139 |  Val. Acc: 41.96%\n",
      "\tTrain Loss: 0.572 | Train Acc: 99.50%\n",
      "\t Val. Loss: 1.138 |  Val. Acc: 50.89%\n",
      "\tTrain Loss: 0.565 | Train Acc: 99.50%\n",
      "\t Val. Loss: 1.145 |  Val. Acc: 50.89%\n",
      "\tTrain Loss: 0.562 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.151 |  Val. Acc: 46.43%\n",
      "\tTrain Loss: 0.559 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.137 |  Val. Acc: 37.50%\n",
      "\tTrain Loss: 0.557 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.159 |  Val. Acc: 41.07%\n",
      "\tTrain Loss: 0.556 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.161 |  Val. Acc: 41.07%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.159 |  Val. Acc: 41.07%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.162 |  Val. Acc: 41.07%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.160 |  Val. Acc: 41.07%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.161 |  Val. Acc: 41.07%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.161 |  Val. Acc: 40.18%\n",
      "\tTrain Loss: 1.100 | Train Acc: 46.25%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 52.68%\n",
      "\tTrain Loss: 1.093 | Train Acc: 58.00%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 49.11%\n",
      "\tTrain Loss: 1.076 | Train Acc: 58.25%\n",
      "\t Val. Loss: 1.098 |  Val. Acc: 34.82%\n",
      "\tTrain Loss: 0.978 | Train Acc: 66.50%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 34.82%\n",
      "\tTrain Loss: 0.833 | Train Acc: 84.00%\n",
      "\t Val. Loss: 1.058 |  Val. Acc: 45.54%\n",
      "\tTrain Loss: 0.705 | Train Acc: 93.75%\n",
      "\t Val. Loss: 1.076 |  Val. Acc: 49.11%\n",
      "\tTrain Loss: 0.628 | Train Acc: 97.50%\n",
      "\t Val. Loss: 1.078 |  Val. Acc: 48.21%\n",
      "\tTrain Loss: 0.600 | Train Acc: 98.75%\n",
      "\t Val. Loss: 1.071 |  Val. Acc: 46.43%\n",
      "\tTrain Loss: 0.576 | Train Acc: 99.50%\n",
      "\t Val. Loss: 1.085 |  Val. Acc: 49.11%\n",
      "\tTrain Loss: 0.565 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.073 |  Val. Acc: 50.89%\n",
      "\tTrain Loss: 0.562 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.087 |  Val. Acc: 47.32%\n",
      "\tTrain Loss: 0.559 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.078 |  Val. Acc: 48.21%\n",
      "\tTrain Loss: 0.558 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.078 |  Val. Acc: 49.11%\n",
      "\tTrain Loss: 0.557 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.125 |  Val. Acc: 55.36%\n",
      "\tTrain Loss: 0.585 | Train Acc: 98.50%\n",
      "\t Val. Loss: 1.086 |  Val. Acc: 41.07%\n",
      "\tTrain Loss: 0.561 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.113 |  Val. Acc: 48.21%\n",
      "\tTrain Loss: 0.557 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.085 |  Val. Acc: 50.00%\n",
      "\tTrain Loss: 0.556 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.087 |  Val. Acc: 50.89%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 53.57%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 52.68%\n",
      "\tTrain Loss: 1.100 | Train Acc: 30.50%\n",
      "\t Val. Loss: 1.096 |  Val. Acc: 53.57%\n",
      "\tTrain Loss: 1.096 | Train Acc: 59.25%\n",
      "\t Val. Loss: 1.094 |  Val. Acc: 54.46%\n",
      "\tTrain Loss: 1.082 | Train Acc: 64.25%\n",
      "\t Val. Loss: 1.083 |  Val. Acc: 59.82%\n",
      "\tTrain Loss: 1.003 | Train Acc: 74.75%\n",
      "\t Val. Loss: 1.109 |  Val. Acc: 29.46%\n",
      "\tTrain Loss: 0.874 | Train Acc: 77.25%\n",
      "\t Val. Loss: 1.063 |  Val. Acc: 47.32%\n",
      "\tTrain Loss: 0.775 | Train Acc: 85.75%\n",
      "\t Val. Loss: 1.081 |  Val. Acc: 52.68%\n",
      "\tTrain Loss: 0.693 | Train Acc: 94.50%\n",
      "\t Val. Loss: 1.084 |  Val. Acc: 42.86%\n",
      "\tTrain Loss: 0.610 | Train Acc: 99.25%\n",
      "\t Val. Loss: 1.068 |  Val. Acc: 52.68%\n",
      "\tTrain Loss: 0.580 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.081 |  Val. Acc: 50.89%\n",
      "\tTrain Loss: 0.566 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.076 |  Val. Acc: 51.79%\n",
      "\tTrain Loss: 0.561 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.102 |  Val. Acc: 50.00%\n",
      "\tTrain Loss: 0.559 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.075 |  Val. Acc: 51.79%\n",
      "\tTrain Loss: 0.558 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.110 |  Val. Acc: 52.68%\n",
      "\tTrain Loss: 0.557 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 52.68%\n",
      "\tTrain Loss: 0.556 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 51.79%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.110 |  Val. Acc: 52.68%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.110 |  Val. Acc: 50.89%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.112 |  Val. Acc: 51.79%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.111 |  Val. Acc: 51.79%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.109 |  Val. Acc: 50.89%\n",
      "\tTrain Loss: 1.101 | Train Acc: 24.25%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 23.21%\n",
      "\tTrain Loss: 1.094 | Train Acc: 47.25%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 52.68%\n",
      "\tTrain Loss: 1.080 | Train Acc: 59.50%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 51.79%\n",
      "\tTrain Loss: 1.010 | Train Acc: 64.00%\n",
      "\t Val. Loss: 1.109 |  Val. Acc: 59.82%\n",
      "\tTrain Loss: 0.868 | Train Acc: 77.25%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 44.64%\n",
      "\tTrain Loss: 0.747 | Train Acc: 89.50%\n",
      "\t Val. Loss: 1.126 |  Val. Acc: 59.82%\n",
      "\tTrain Loss: 0.674 | Train Acc: 94.00%\n",
      "\t Val. Loss: 1.083 |  Val. Acc: 45.54%\n",
      "\tTrain Loss: 0.632 | Train Acc: 94.25%\n",
      "\t Val. Loss: 1.122 |  Val. Acc: 43.75%\n",
      "\tTrain Loss: 0.586 | Train Acc: 98.50%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 52.68%\n",
      "\tTrain Loss: 0.573 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.108 |  Val. Acc: 49.11%\n",
      "\tTrain Loss: 0.564 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 50.00%\n",
      "\tTrain Loss: 0.560 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.118 |  Val. Acc: 50.89%\n",
      "\tTrain Loss: 0.558 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.123 |  Val. Acc: 50.00%\n",
      "\tTrain Loss: 0.557 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.125 |  Val. Acc: 50.89%\n",
      "\tTrain Loss: 0.556 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.127 |  Val. Acc: 50.89%\n",
      "\tTrain Loss: 0.556 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.129 |  Val. Acc: 50.89%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.131 |  Val. Acc: 50.89%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.133 |  Val. Acc: 50.89%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.134 |  Val. Acc: 50.89%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.135 |  Val. Acc: 50.00%\n",
      "\tTrain Loss: 1.100 | Train Acc: 33.00%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 54.46%\n",
      "\tTrain Loss: 1.095 | Train Acc: 57.25%\n",
      "\t Val. Loss: 1.098 |  Val. Acc: 55.36%\n",
      "\tTrain Loss: 1.084 | Train Acc: 59.50%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 55.36%\n",
      "\tTrain Loss: 1.024 | Train Acc: 66.75%\n",
      "\t Val. Loss: 1.077 |  Val. Acc: 50.89%\n",
      "\tTrain Loss: 0.886 | Train Acc: 78.75%\n",
      "\t Val. Loss: 1.121 |  Val. Acc: 45.54%\n",
      "\tTrain Loss: 0.746 | Train Acc: 91.50%\n",
      "\t Val. Loss: 1.129 |  Val. Acc: 45.54%\n",
      "\tTrain Loss: 0.643 | Train Acc: 96.50%\n",
      "\t Val. Loss: 1.138 |  Val. Acc: 43.75%\n",
      "\tTrain Loss: 0.593 | Train Acc: 99.00%\n",
      "\t Val. Loss: 1.115 |  Val. Acc: 51.79%\n",
      "\tTrain Loss: 0.575 | Train Acc: 99.50%\n",
      "\t Val. Loss: 1.124 |  Val. Acc: 47.32%\n",
      "\tTrain Loss: 0.579 | Train Acc: 98.75%\n",
      "\t Val. Loss: 1.161 |  Val. Acc: 51.79%\n",
      "\tTrain Loss: 0.573 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.167 |  Val. Acc: 44.64%\n",
      "\tTrain Loss: 0.564 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.145 |  Val. Acc: 43.75%\n",
      "\tTrain Loss: 0.560 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.165 |  Val. Acc: 43.75%\n",
      "\tTrain Loss: 0.557 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.160 |  Val. Acc: 46.43%\n",
      "\tTrain Loss: 0.556 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.148 |  Val. Acc: 48.21%\n",
      "\tTrain Loss: 0.556 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.143 |  Val. Acc: 49.11%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.142 |  Val. Acc: 48.21%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.141 |  Val. Acc: 49.11%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.140 |  Val. Acc: 50.00%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.142 |  Val. Acc: 50.00%\n",
      "\tTrain Loss: 1.099 | Train Acc: 44.25%\n",
      "\t Val. Loss: 1.094 |  Val. Acc: 59.82%\n",
      "\tTrain Loss: 1.093 | Train Acc: 54.75%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 60.71%\n",
      "\tTrain Loss: 1.074 | Train Acc: 64.25%\n",
      "\t Val. Loss: 1.081 |  Val. Acc: 59.82%\n",
      "\tTrain Loss: 0.983 | Train Acc: 68.50%\n",
      "\t Val. Loss: 1.081 |  Val. Acc: 59.82%\n",
      "\tTrain Loss: 0.909 | Train Acc: 75.25%\n",
      "\t Val. Loss: 1.056 |  Val. Acc: 58.04%\n",
      "\tTrain Loss: 0.763 | Train Acc: 89.25%\n",
      "\t Val. Loss: 1.020 |  Val. Acc: 59.82%\n",
      "\tTrain Loss: 0.674 | Train Acc: 94.00%\n",
      "\t Val. Loss: 1.030 |  Val. Acc: 53.57%\n",
      "\tTrain Loss: 0.615 | Train Acc: 98.00%\n",
      "\t Val. Loss: 1.006 |  Val. Acc: 62.50%\n",
      "\tTrain Loss: 0.584 | Train Acc: 99.50%\n",
      "\t Val. Loss: 1.004 |  Val. Acc: 60.71%\n",
      "\tTrain Loss: 0.570 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.010 |  Val. Acc: 59.82%\n",
      "\tTrain Loss: 0.564 | Train Acc: 100.00%\n",
      "\t Val. Loss: 0.996 |  Val. Acc: 63.39%\n",
      "\tTrain Loss: 0.561 | Train Acc: 100.00%\n",
      "\t Val. Loss: 0.998 |  Val. Acc: 61.61%\n",
      "\tTrain Loss: 0.559 | Train Acc: 100.00%\n",
      "\t Val. Loss: 0.994 |  Val. Acc: 60.71%\n",
      "\tTrain Loss: 0.558 | Train Acc: 100.00%\n",
      "\t Val. Loss: 0.996 |  Val. Acc: 59.82%\n",
      "\tTrain Loss: 0.557 | Train Acc: 100.00%\n",
      "\t Val. Loss: 0.999 |  Val. Acc: 61.61%\n",
      "\tTrain Loss: 0.557 | Train Acc: 100.00%\n",
      "\t Val. Loss: 0.994 |  Val. Acc: 62.50%\n",
      "\tTrain Loss: 0.558 | Train Acc: 100.00%\n",
      "\t Val. Loss: 0.994 |  Val. Acc: 61.61%\n",
      "\tTrain Loss: 0.595 | Train Acc: 97.25%\n",
      "\t Val. Loss: 1.031 |  Val. Acc: 52.68%\n",
      "\tTrain Loss: 0.586 | Train Acc: 97.75%\n",
      "\t Val. Loss: 1.026 |  Val. Acc: 55.36%\n",
      "\tTrain Loss: 0.562 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.031 |  Val. Acc: 54.46%\n",
      "\tTrain Loss: 1.101 | Train Acc: 18.25%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 16.07%\n",
      "\tTrain Loss: 1.095 | Train Acc: 33.25%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 48.21%\n",
      "\tTrain Loss: 1.080 | Train Acc: 58.50%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 54.46%\n",
      "\tTrain Loss: 1.012 | Train Acc: 70.50%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 33.93%\n",
      "\tTrain Loss: 0.888 | Train Acc: 77.00%\n",
      "\t Val. Loss: 1.109 |  Val. Acc: 50.00%\n",
      "\tTrain Loss: 0.758 | Train Acc: 88.25%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 39.29%\n",
      "\tTrain Loss: 0.651 | Train Acc: 96.50%\n",
      "\t Val. Loss: 1.098 |  Val. Acc: 42.86%\n",
      "\tTrain Loss: 0.599 | Train Acc: 98.25%\n",
      "\t Val. Loss: 1.076 |  Val. Acc: 53.57%\n",
      "\tTrain Loss: 0.577 | Train Acc: 99.50%\n",
      "\t Val. Loss: 1.070 |  Val. Acc: 49.11%\n",
      "\tTrain Loss: 0.566 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.049 |  Val. Acc: 49.11%\n",
      "\tTrain Loss: 0.563 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.068 |  Val. Acc: 57.14%\n",
      "\tTrain Loss: 0.560 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.073 |  Val. Acc: 55.36%\n",
      "\tTrain Loss: 0.559 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.059 |  Val. Acc: 53.57%\n",
      "\tTrain Loss: 0.557 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.072 |  Val. Acc: 53.57%\n",
      "\tTrain Loss: 0.556 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.071 |  Val. Acc: 52.68%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.070 |  Val. Acc: 53.57%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.067 |  Val. Acc: 52.68%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.065 |  Val. Acc: 53.57%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.064 |  Val. Acc: 53.57%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.064 |  Val. Acc: 52.68%\n",
      "\tTrain Loss: 1.101 | Train Acc: 46.75%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 57.14%\n",
      "\tTrain Loss: 1.095 | Train Acc: 56.50%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 51.79%\n",
      "\tTrain Loss: 1.086 | Train Acc: 57.50%\n",
      "\t Val. Loss: 1.096 |  Val. Acc: 50.89%\n",
      "\tTrain Loss: 1.027 | Train Acc: 70.00%\n",
      "\t Val. Loss: 1.115 |  Val. Acc: 41.96%\n",
      "\tTrain Loss: 0.851 | Train Acc: 84.25%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 37.50%\n",
      "\tTrain Loss: 0.734 | Train Acc: 91.00%\n",
      "\t Val. Loss: 1.134 |  Val. Acc: 52.68%\n",
      "\tTrain Loss: 0.648 | Train Acc: 96.50%\n",
      "\t Val. Loss: 1.143 |  Val. Acc: 50.00%\n",
      "\tTrain Loss: 0.609 | Train Acc: 98.25%\n",
      "\t Val. Loss: 1.160 |  Val. Acc: 42.86%\n",
      "\tTrain Loss: 0.579 | Train Acc: 99.50%\n",
      "\t Val. Loss: 1.169 |  Val. Acc: 48.21%\n",
      "\tTrain Loss: 0.569 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.190 |  Val. Acc: 47.32%\n",
      "\tTrain Loss: 0.562 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.174 |  Val. Acc: 50.00%\n",
      "\tTrain Loss: 0.559 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.174 |  Val. Acc: 49.11%\n",
      "\tTrain Loss: 0.558 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.168 |  Val. Acc: 44.64%\n",
      "\tTrain Loss: 0.558 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.184 |  Val. Acc: 48.21%\n",
      "\tTrain Loss: 0.556 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.186 |  Val. Acc: 47.32%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.185 |  Val. Acc: 49.11%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.183 |  Val. Acc: 48.21%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.185 |  Val. Acc: 48.21%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.186 |  Val. Acc: 48.21%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.188 |  Val. Acc: 48.21%\n",
      "\tTrain Loss: 1.099 | Train Acc: 29.50%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 30.36%\n",
      "\tTrain Loss: 1.095 | Train Acc: 58.50%\n",
      "\t Val. Loss: 1.096 |  Val. Acc: 51.79%\n",
      "\tTrain Loss: 1.084 | Train Acc: 61.00%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 56.25%\n",
      "\tTrain Loss: 1.014 | Train Acc: 68.75%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 57.14%\n",
      "\tTrain Loss: 0.872 | Train Acc: 82.25%\n",
      "\t Val. Loss: 1.109 |  Val. Acc: 55.36%\n",
      "\tTrain Loss: 0.724 | Train Acc: 92.75%\n",
      "\t Val. Loss: 1.060 |  Val. Acc: 57.14%\n",
      "\tTrain Loss: 0.632 | Train Acc: 97.50%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 50.89%\n",
      "\tTrain Loss: 0.596 | Train Acc: 99.00%\n",
      "\t Val. Loss: 1.028 |  Val. Acc: 54.46%\n",
      "\tTrain Loss: 0.586 | Train Acc: 99.50%\n",
      "\t Val. Loss: 1.065 |  Val. Acc: 55.36%\n",
      "\tTrain Loss: 0.580 | Train Acc: 99.25%\n",
      "\t Val. Loss: 1.074 |  Val. Acc: 59.82%\n",
      "\tTrain Loss: 0.566 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.064 |  Val. Acc: 58.04%\n",
      "\tTrain Loss: 0.563 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.040 |  Val. Acc: 58.93%\n",
      "\tTrain Loss: 0.561 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.060 |  Val. Acc: 57.14%\n",
      "\tTrain Loss: 0.560 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.037 |  Val. Acc: 59.82%\n",
      "\tTrain Loss: 0.559 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.056 |  Val. Acc: 58.93%\n",
      "\tTrain Loss: 0.559 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.037 |  Val. Acc: 57.14%\n",
      "\tTrain Loss: 0.558 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.050 |  Val. Acc: 57.14%\n",
      "\tTrain Loss: 0.558 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.001 |  Val. Acc: 59.82%\n",
      "\tTrain Loss: 0.558 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.004 |  Val. Acc: 59.82%\n",
      "\tTrain Loss: 0.558 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.018 |  Val. Acc: 57.14%\n",
      "\tTrain Loss: 1.100 | Train Acc: 21.75%\n",
      "\t Val. Loss: 1.098 |  Val. Acc: 25.89%\n",
      "\tTrain Loss: 1.093 | Train Acc: 58.50%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 55.36%\n",
      "\tTrain Loss: 1.075 | Train Acc: 60.25%\n",
      "\t Val. Loss: 1.087 |  Val. Acc: 49.11%\n",
      "\tTrain Loss: 0.973 | Train Acc: 80.75%\n",
      "\t Val. Loss: 1.067 |  Val. Acc: 46.43%\n",
      "\tTrain Loss: 0.816 | Train Acc: 90.00%\n",
      "\t Val. Loss: 1.089 |  Val. Acc: 44.64%\n",
      "\tTrain Loss: 0.716 | Train Acc: 94.50%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 50.00%\n",
      "\tTrain Loss: 0.635 | Train Acc: 98.25%\n",
      "\t Val. Loss: 1.113 |  Val. Acc: 50.89%\n",
      "\tTrain Loss: 0.592 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.086 |  Val. Acc: 40.18%\n",
      "\tTrain Loss: 0.577 | Train Acc: 99.50%\n",
      "\t Val. Loss: 1.146 |  Val. Acc: 45.54%\n",
      "\tTrain Loss: 0.566 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.135 |  Val. Acc: 46.43%\n",
      "\tTrain Loss: 0.561 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.139 |  Val. Acc: 43.75%\n",
      "\tTrain Loss: 0.559 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.143 |  Val. Acc: 43.75%\n",
      "\tTrain Loss: 0.557 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.139 |  Val. Acc: 45.54%\n",
      "\tTrain Loss: 0.556 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.141 |  Val. Acc: 46.43%\n",
      "\tTrain Loss: 0.556 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.144 |  Val. Acc: 46.43%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.151 |  Val. Acc: 46.43%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.159 |  Val. Acc: 46.43%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.163 |  Val. Acc: 46.43%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.134 |  Val. Acc: 38.39%\n",
      "\tTrain Loss: 0.556 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.162 |  Val. Acc: 43.75%\n",
      "\tTrain Loss: 1.099 | Train Acc: 55.50%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 55.36%\n",
      "\tTrain Loss: 1.094 | Train Acc: 56.75%\n",
      "\t Val. Loss: 1.098 |  Val. Acc: 55.36%\n",
      "\tTrain Loss: 1.081 | Train Acc: 59.50%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 53.57%\n",
      "\tTrain Loss: 0.991 | Train Acc: 76.75%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 55.36%\n",
      "\tTrain Loss: 0.837 | Train Acc: 85.00%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 48.21%\n",
      "\tTrain Loss: 0.698 | Train Acc: 94.00%\n",
      "\t Val. Loss: 1.084 |  Val. Acc: 49.11%\n",
      "\tTrain Loss: 0.633 | Train Acc: 96.00%\n",
      "\t Val. Loss: 1.086 |  Val. Acc: 58.04%\n",
      "\tTrain Loss: 0.593 | Train Acc: 99.00%\n",
      "\t Val. Loss: 1.072 |  Val. Acc: 58.04%\n",
      "\tTrain Loss: 0.573 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.083 |  Val. Acc: 60.71%\n",
      "\tTrain Loss: 0.565 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.081 |  Val. Acc: 58.93%\n",
      "\tTrain Loss: 0.561 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 51.79%\n",
      "\tTrain Loss: 0.560 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.089 |  Val. Acc: 57.14%\n",
      "\tTrain Loss: 0.559 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.080 |  Val. Acc: 60.71%\n",
      "\tTrain Loss: 0.557 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.077 |  Val. Acc: 57.14%\n",
      "\tTrain Loss: 0.556 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.081 |  Val. Acc: 58.93%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.079 |  Val. Acc: 58.93%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.079 |  Val. Acc: 60.71%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.078 |  Val. Acc: 59.82%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.077 |  Val. Acc: 59.82%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.077 |  Val. Acc: 58.04%\n",
      "\tTrain Loss: 1.101 | Train Acc: 18.75%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 18.75%\n",
      "\tTrain Loss: 1.095 | Train Acc: 38.75%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 45.54%\n",
      "\tTrain Loss: 1.083 | Train Acc: 49.25%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 48.21%\n",
      "\tTrain Loss: 1.008 | Train Acc: 70.75%\n",
      "\t Val. Loss: 1.119 |  Val. Acc: 41.07%\n",
      "\tTrain Loss: 0.855 | Train Acc: 83.00%\n",
      "\t Val. Loss: 1.142 |  Val. Acc: 47.32%\n",
      "\tTrain Loss: 0.703 | Train Acc: 95.25%\n",
      "\t Val. Loss: 1.096 |  Val. Acc: 43.75%\n",
      "\tTrain Loss: 0.631 | Train Acc: 98.25%\n",
      "\t Val. Loss: 1.118 |  Val. Acc: 40.18%\n",
      "\tTrain Loss: 0.611 | Train Acc: 98.00%\n",
      "\t Val. Loss: 1.116 |  Val. Acc: 46.43%\n",
      "\tTrain Loss: 0.585 | Train Acc: 99.25%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 49.11%\n",
      "\tTrain Loss: 0.570 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.113 |  Val. Acc: 52.68%\n",
      "\tTrain Loss: 0.564 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 50.00%\n",
      "\tTrain Loss: 0.562 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.086 |  Val. Acc: 46.43%\n",
      "\tTrain Loss: 0.559 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.098 |  Val. Acc: 48.21%\n",
      "\tTrain Loss: 0.558 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.092 |  Val. Acc: 48.21%\n",
      "\tTrain Loss: 0.557 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 48.21%\n",
      "\tTrain Loss: 0.556 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.102 |  Val. Acc: 49.11%\n",
      "\tTrain Loss: 0.556 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.095 |  Val. Acc: 50.00%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.090 |  Val. Acc: 50.00%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.094 |  Val. Acc: 47.32%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.086 |  Val. Acc: 53.57%\n",
      "\tTrain Loss: 1.099 | Train Acc: 22.50%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 34.82%\n",
      "\tTrain Loss: 1.093 | Train Acc: 59.50%\n",
      "\t Val. Loss: 1.098 |  Val. Acc: 55.36%\n",
      "\tTrain Loss: 1.079 | Train Acc: 64.00%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 50.89%\n",
      "\tTrain Loss: 1.000 | Train Acc: 71.50%\n",
      "\t Val. Loss: 1.069 |  Val. Acc: 51.79%\n",
      "\tTrain Loss: 0.836 | Train Acc: 82.75%\n",
      "\t Val. Loss: 1.112 |  Val. Acc: 53.57%\n",
      "\tTrain Loss: 0.718 | Train Acc: 91.25%\n",
      "\t Val. Loss: 1.062 |  Val. Acc: 44.64%\n",
      "\tTrain Loss: 0.651 | Train Acc: 95.50%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 41.96%\n",
      "\tTrain Loss: 0.596 | Train Acc: 98.75%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 53.57%\n",
      "\tTrain Loss: 0.572 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 55.36%\n",
      "\tTrain Loss: 0.564 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.136 |  Val. Acc: 53.57%\n",
      "\tTrain Loss: 0.561 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.124 |  Val. Acc: 55.36%\n",
      "\tTrain Loss: 0.558 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.127 |  Val. Acc: 55.36%\n",
      "\tTrain Loss: 0.557 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.130 |  Val. Acc: 55.36%\n",
      "\tTrain Loss: 0.556 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.132 |  Val. Acc: 55.36%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.132 |  Val. Acc: 55.36%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.130 |  Val. Acc: 55.36%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.130 |  Val. Acc: 55.36%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.131 |  Val. Acc: 55.36%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.132 |  Val. Acc: 55.36%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.132 |  Val. Acc: 55.36%\n",
      "\tTrain Loss: 1.100 | Train Acc: 35.00%\n",
      "\t Val. Loss: 1.098 |  Val. Acc: 54.46%\n",
      "\tTrain Loss: 1.094 | Train Acc: 56.75%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 52.68%\n",
      "\tTrain Loss: 1.082 | Train Acc: 59.25%\n",
      "\t Val. Loss: 1.095 |  Val. Acc: 53.57%\n",
      "\tTrain Loss: 1.020 | Train Acc: 68.75%\n",
      "\t Val. Loss: 1.080 |  Val. Acc: 54.46%\n",
      "\tTrain Loss: 0.871 | Train Acc: 82.75%\n",
      "\t Val. Loss: 1.079 |  Val. Acc: 46.43%\n",
      "\tTrain Loss: 0.733 | Train Acc: 91.25%\n",
      "\t Val. Loss: 1.085 |  Val. Acc: 54.46%\n",
      "\tTrain Loss: 0.641 | Train Acc: 98.50%\n",
      "\t Val. Loss: 1.094 |  Val. Acc: 56.25%\n",
      "\tTrain Loss: 0.588 | Train Acc: 99.25%\n",
      "\t Val. Loss: 1.075 |  Val. Acc: 55.36%\n",
      "\tTrain Loss: 0.574 | Train Acc: 99.50%\n",
      "\t Val. Loss: 1.087 |  Val. Acc: 51.79%\n",
      "\tTrain Loss: 0.570 | Train Acc: 99.50%\n",
      "\t Val. Loss: 1.090 |  Val. Acc: 56.25%\n",
      "\tTrain Loss: 0.566 | Train Acc: 99.50%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 54.46%\n",
      "\tTrain Loss: 0.562 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.077 |  Val. Acc: 53.57%\n",
      "\tTrain Loss: 0.559 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.090 |  Val. Acc: 56.25%\n",
      "\tTrain Loss: 0.558 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.086 |  Val. Acc: 56.25%\n",
      "\tTrain Loss: 0.557 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.086 |  Val. Acc: 57.14%\n",
      "\tTrain Loss: 0.557 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.085 |  Val. Acc: 58.04%\n",
      "\tTrain Loss: 0.556 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.082 |  Val. Acc: 58.04%\n",
      "\tTrain Loss: 0.556 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.073 |  Val. Acc: 63.39%\n",
      "\tTrain Loss: 0.556 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.074 |  Val. Acc: 62.50%\n",
      "\tTrain Loss: 0.555 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.074 |  Val. Acc: 62.50%\n",
      "\tTrain Loss: 1.100 | Train Acc: 55.25%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 55.36%\n",
      "\tTrain Loss: 1.094 | Train Acc: 57.00%\n",
      "\t Val. Loss: 1.096 |  Val. Acc: 55.36%\n",
      "\tTrain Loss: 1.083 | Train Acc: 60.25%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 56.25%\n",
      "\tTrain Loss: 1.016 | Train Acc: 71.50%\n",
      "\t Val. Loss: 1.096 |  Val. Acc: 19.64%\n",
      "\tTrain Loss: 0.908 | Train Acc: 74.50%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 32.14%\n",
      "\tTrain Loss: 0.747 | Train Acc: 92.50%\n",
      "\t Val. Loss: 1.115 |  Val. Acc: 36.61%\n",
      "\tTrain Loss: 0.643 | Train Acc: 97.25%\n",
      "\t Val. Loss: 1.134 |  Val. Acc: 41.07%\n",
      "\tTrain Loss: 0.591 | Train Acc: 99.25%\n",
      "\t Val. Loss: 1.130 |  Val. Acc: 39.29%\n",
      "\tTrain Loss: 0.577 | Train Acc: 99.25%\n",
      "\t Val. Loss: 1.133 |  Val. Acc: 34.82%\n",
      "\tTrain Loss: 0.568 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.165 |  Val. Acc: 41.96%\n",
      "\tTrain Loss: 0.562 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.180 |  Val. Acc: 41.07%\n",
      "\tTrain Loss: 0.560 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.183 |  Val. Acc: 41.07%\n",
      "\tTrain Loss: 0.559 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.180 |  Val. Acc: 41.07%\n",
      "\tTrain Loss: 0.558 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.179 |  Val. Acc: 45.54%\n",
      "\tTrain Loss: 0.557 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.180 |  Val. Acc: 45.54%\n",
      "\tTrain Loss: 0.556 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.170 |  Val. Acc: 46.43%\n",
      "\tTrain Loss: 0.557 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.197 |  Val. Acc: 41.07%\n",
      "\tTrain Loss: 0.559 | Train Acc: 99.50%\n",
      "\t Val. Loss: 1.125 |  Val. Acc: 42.86%\n",
      "\tTrain Loss: 0.556 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.147 |  Val. Acc: 44.64%\n",
      "\tTrain Loss: 0.555 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.124 |  Val. Acc: 48.21%\n",
      "\tTrain Loss: 1.100 | Train Acc: 46.50%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 57.14%\n",
      "\tTrain Loss: 1.093 | Train Acc: 57.25%\n",
      "\t Val. Loss: 1.094 |  Val. Acc: 58.04%\n",
      "\tTrain Loss: 1.077 | Train Acc: 62.75%\n",
      "\t Val. Loss: 1.086 |  Val. Acc: 48.21%\n",
      "\tTrain Loss: 0.984 | Train Acc: 70.25%\n",
      "\t Val. Loss: 1.057 |  Val. Acc: 49.11%\n",
      "\tTrain Loss: 0.788 | Train Acc: 86.75%\n",
      "\t Val. Loss: 1.081 |  Val. Acc: 50.89%\n",
      "\tTrain Loss: 0.664 | Train Acc: 94.25%\n",
      "\t Val. Loss: 1.072 |  Val. Acc: 44.64%\n",
      "\tTrain Loss: 0.629 | Train Acc: 96.50%\n",
      "\t Val. Loss: 1.107 |  Val. Acc: 39.29%\n",
      "\tTrain Loss: 0.597 | Train Acc: 98.25%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 54.46%\n",
      "\tTrain Loss: 0.577 | Train Acc: 99.25%\n",
      "\t Val. Loss: 1.108 |  Val. Acc: 49.11%\n",
      "\tTrain Loss: 0.568 | Train Acc: 99.25%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 50.00%\n",
      "\tTrain Loss: 0.562 | Train Acc: 99.50%\n",
      "\t Val. Loss: 1.123 |  Val. Acc: 50.89%\n",
      "\tTrain Loss: 0.559 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 50.89%\n",
      "\tTrain Loss: 0.558 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 51.79%\n",
      "\tTrain Loss: 0.557 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.116 |  Val. Acc: 51.79%\n",
      "\tTrain Loss: 0.556 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.121 |  Val. Acc: 52.68%\n",
      "\tTrain Loss: 0.588 | Train Acc: 97.25%\n",
      "\t Val. Loss: 1.132 |  Val. Acc: 40.18%\n",
      "\tTrain Loss: 0.569 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.090 |  Val. Acc: 49.11%\n",
      "\tTrain Loss: 0.559 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.112 |  Val. Acc: 40.18%\n",
      "\tTrain Loss: 0.556 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.102 |  Val. Acc: 50.89%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.113 |  Val. Acc: 47.32%\n",
      "\tTrain Loss: 1.100 | Train Acc: 31.00%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 43.75%\n",
      "\tTrain Loss: 1.093 | Train Acc: 59.25%\n",
      "\t Val. Loss: 1.098 |  Val. Acc: 49.11%\n",
      "\tTrain Loss: 1.075 | Train Acc: 66.25%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 46.43%\n",
      "\tTrain Loss: 0.996 | Train Acc: 67.50%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 39.29%\n",
      "\tTrain Loss: 0.853 | Train Acc: 81.00%\n",
      "\t Val. Loss: 1.083 |  Val. Acc: 41.07%\n",
      "\tTrain Loss: 0.732 | Train Acc: 90.75%\n",
      "\t Val. Loss: 1.071 |  Val. Acc: 52.68%\n",
      "\tTrain Loss: 0.630 | Train Acc: 97.25%\n",
      "\t Val. Loss: 1.067 |  Val. Acc: 50.89%\n",
      "\tTrain Loss: 0.600 | Train Acc: 98.75%\n",
      "\t Val. Loss: 1.063 |  Val. Acc: 47.32%\n",
      "\tTrain Loss: 0.583 | Train Acc: 99.50%\n",
      "\t Val. Loss: 1.073 |  Val. Acc: 36.61%\n",
      "\tTrain Loss: 0.590 | Train Acc: 97.50%\n",
      "\t Val. Loss: 1.081 |  Val. Acc: 50.89%\n",
      "\tTrain Loss: 0.567 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.069 |  Val. Acc: 54.46%\n",
      "\tTrain Loss: 0.561 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.066 |  Val. Acc: 54.46%\n",
      "\tTrain Loss: 0.558 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.062 |  Val. Acc: 51.79%\n",
      "\tTrain Loss: 0.557 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.054 |  Val. Acc: 52.68%\n",
      "\tTrain Loss: 0.556 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.048 |  Val. Acc: 53.57%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.054 |  Val. Acc: 52.68%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.051 |  Val. Acc: 52.68%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.052 |  Val. Acc: 53.57%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.058 |  Val. Acc: 52.68%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.064 |  Val. Acc: 52.68%\n",
      "\tTrain Loss: 1.099 | Train Acc: 43.75%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 52.68%\n",
      "\tTrain Loss: 1.094 | Train Acc: 57.00%\n",
      "\t Val. Loss: 1.098 |  Val. Acc: 52.68%\n",
      "\tTrain Loss: 1.084 | Train Acc: 57.75%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 51.79%\n",
      "\tTrain Loss: 1.015 | Train Acc: 71.25%\n",
      "\t Val. Loss: 1.084 |  Val. Acc: 40.18%\n",
      "\tTrain Loss: 0.877 | Train Acc: 82.25%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 44.64%\n",
      "\tTrain Loss: 0.741 | Train Acc: 91.00%\n",
      "\t Val. Loss: 1.085 |  Val. Acc: 39.29%\n",
      "\tTrain Loss: 0.633 | Train Acc: 98.25%\n",
      "\t Val. Loss: 1.116 |  Val. Acc: 44.64%\n",
      "\tTrain Loss: 0.586 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 50.00%\n",
      "\tTrain Loss: 0.570 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.120 |  Val. Acc: 50.00%\n",
      "\tTrain Loss: 0.563 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.118 |  Val. Acc: 51.79%\n",
      "\tTrain Loss: 0.560 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.124 |  Val. Acc: 49.11%\n",
      "\tTrain Loss: 0.558 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.116 |  Val. Acc: 52.68%\n",
      "\tTrain Loss: 0.557 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.127 |  Val. Acc: 50.89%\n",
      "\tTrain Loss: 0.556 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.128 |  Val. Acc: 50.89%\n",
      "\tTrain Loss: 0.556 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.128 |  Val. Acc: 51.79%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.124 |  Val. Acc: 49.11%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.122 |  Val. Acc: 49.11%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.111 |  Val. Acc: 52.68%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.116 |  Val. Acc: 50.00%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.121 |  Val. Acc: 50.89%\n",
      "\tTrain Loss: 1.099 | Train Acc: 50.25%\n",
      "\t Val. Loss: 1.098 |  Val. Acc: 55.36%\n",
      "\tTrain Loss: 1.092 | Train Acc: 56.75%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 55.36%\n",
      "\tTrain Loss: 1.067 | Train Acc: 63.50%\n",
      "\t Val. Loss: 1.114 |  Val. Acc: 53.57%\n",
      "\tTrain Loss: 0.955 | Train Acc: 74.25%\n",
      "\t Val. Loss: 1.112 |  Val. Acc: 44.64%\n",
      "\tTrain Loss: 0.800 | Train Acc: 86.75%\n",
      "\t Val. Loss: 1.134 |  Val. Acc: 46.43%\n",
      "\tTrain Loss: 0.673 | Train Acc: 93.75%\n",
      "\t Val. Loss: 1.166 |  Val. Acc: 51.79%\n",
      "\tTrain Loss: 0.624 | Train Acc: 97.00%\n",
      "\t Val. Loss: 1.185 |  Val. Acc: 44.64%\n",
      "\tTrain Loss: 0.600 | Train Acc: 97.25%\n",
      "\t Val. Loss: 1.136 |  Val. Acc: 45.54%\n",
      "\tTrain Loss: 0.577 | Train Acc: 99.00%\n",
      "\t Val. Loss: 1.155 |  Val. Acc: 42.86%\n",
      "\tTrain Loss: 0.567 | Train Acc: 99.50%\n",
      "\t Val. Loss: 1.175 |  Val. Acc: 45.54%\n",
      "\tTrain Loss: 0.564 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.168 |  Val. Acc: 36.61%\n",
      "\tTrain Loss: 0.562 | Train Acc: 99.50%\n",
      "\t Val. Loss: 1.210 |  Val. Acc: 41.07%\n",
      "\tTrain Loss: 0.560 | Train Acc: 99.50%\n",
      "\t Val. Loss: 1.200 |  Val. Acc: 41.96%\n",
      "\tTrain Loss: 0.563 | Train Acc: 99.00%\n",
      "\t Val. Loss: 1.177 |  Val. Acc: 42.86%\n",
      "\tTrain Loss: 0.558 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.173 |  Val. Acc: 37.50%\n",
      "\tTrain Loss: 0.590 | Train Acc: 98.75%\n",
      "\t Val. Loss: 1.169 |  Val. Acc: 40.18%\n",
      "\tTrain Loss: 0.566 | Train Acc: 99.50%\n",
      "\t Val. Loss: 1.169 |  Val. Acc: 40.18%\n",
      "\tTrain Loss: 0.558 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.174 |  Val. Acc: 41.96%\n",
      "\tTrain Loss: 0.556 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.177 |  Val. Acc: 40.18%\n",
      "\tTrain Loss: 0.556 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.175 |  Val. Acc: 41.07%\n",
      "\tTrain Loss: 1.099 | Train Acc: 45.25%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 54.46%\n",
      "\tTrain Loss: 1.092 | Train Acc: 58.00%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 52.68%\n",
      "\tTrain Loss: 1.078 | Train Acc: 61.75%\n",
      "\t Val. Loss: 1.110 |  Val. Acc: 48.21%\n",
      "\tTrain Loss: 0.995 | Train Acc: 65.25%\n",
      "\t Val. Loss: 1.121 |  Val. Acc: 44.64%\n",
      "\tTrain Loss: 0.859 | Train Acc: 78.50%\n",
      "\t Val. Loss: 1.151 |  Val. Acc: 39.29%\n",
      "\tTrain Loss: 0.757 | Train Acc: 85.50%\n",
      "\t Val. Loss: 1.145 |  Val. Acc: 37.50%\n",
      "\tTrain Loss: 0.671 | Train Acc: 93.00%\n",
      "\t Val. Loss: 1.149 |  Val. Acc: 35.71%\n",
      "\tTrain Loss: 0.627 | Train Acc: 95.25%\n",
      "\t Val. Loss: 1.139 |  Val. Acc: 41.96%\n",
      "\tTrain Loss: 0.590 | Train Acc: 99.00%\n",
      "\t Val. Loss: 1.133 |  Val. Acc: 36.61%\n",
      "\tTrain Loss: 0.580 | Train Acc: 98.50%\n",
      "\t Val. Loss: 1.147 |  Val. Acc: 35.71%\n",
      "\tTrain Loss: 0.568 | Train Acc: 99.50%\n",
      "\t Val. Loss: 1.166 |  Val. Acc: 31.25%\n",
      "\tTrain Loss: 0.564 | Train Acc: 99.50%\n",
      "\t Val. Loss: 1.151 |  Val. Acc: 35.71%\n",
      "\tTrain Loss: 0.561 | Train Acc: 99.50%\n",
      "\t Val. Loss: 1.124 |  Val. Acc: 40.18%\n",
      "\tTrain Loss: 0.559 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.144 |  Val. Acc: 35.71%\n",
      "\tTrain Loss: 0.558 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.135 |  Val. Acc: 38.39%\n",
      "\tTrain Loss: 0.557 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.139 |  Val. Acc: 36.61%\n",
      "\tTrain Loss: 0.556 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.131 |  Val. Acc: 41.07%\n",
      "\tTrain Loss: 0.556 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.140 |  Val. Acc: 38.39%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.141 |  Val. Acc: 35.71%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.134 |  Val. Acc: 39.29%\n",
      "\tTrain Loss: 1.098 | Train Acc: 47.50%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 58.04%\n",
      "\tTrain Loss: 1.092 | Train Acc: 56.50%\n",
      "\t Val. Loss: 1.098 |  Val. Acc: 58.04%\n",
      "\tTrain Loss: 1.073 | Train Acc: 63.75%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 43.75%\n",
      "\tTrain Loss: 0.968 | Train Acc: 74.75%\n",
      "\t Val. Loss: 1.111 |  Val. Acc: 37.50%\n",
      "\tTrain Loss: 0.815 | Train Acc: 84.75%\n",
      "\t Val. Loss: 1.172 |  Val. Acc: 42.86%\n",
      "\tTrain Loss: 0.717 | Train Acc: 92.50%\n",
      "\t Val. Loss: 1.160 |  Val. Acc: 45.54%\n",
      "\tTrain Loss: 0.638 | Train Acc: 96.75%\n",
      "\t Val. Loss: 1.144 |  Val. Acc: 47.32%\n",
      "\tTrain Loss: 0.591 | Train Acc: 99.00%\n",
      "\t Val. Loss: 1.154 |  Val. Acc: 45.54%\n",
      "\tTrain Loss: 0.570 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.155 |  Val. Acc: 45.54%\n",
      "\tTrain Loss: 0.565 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.157 |  Val. Acc: 37.50%\n",
      "\tTrain Loss: 0.562 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.116 |  Val. Acc: 56.25%\n",
      "\tTrain Loss: 0.560 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.127 |  Val. Acc: 53.57%\n",
      "\tTrain Loss: 0.559 | Train Acc: 99.50%\n",
      "\t Val. Loss: 1.108 |  Val. Acc: 56.25%\n",
      "\tTrain Loss: 0.598 | Train Acc: 95.50%\n",
      "\t Val. Loss: 1.153 |  Val. Acc: 38.39%\n",
      "\tTrain Loss: 0.604 | Train Acc: 97.50%\n",
      "\t Val. Loss: 1.145 |  Val. Acc: 46.43%\n",
      "\tTrain Loss: 0.565 | Train Acc: 99.50%\n",
      "\t Val. Loss: 1.130 |  Val. Acc: 53.57%\n",
      "\tTrain Loss: 0.557 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.112 |  Val. Acc: 49.11%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.115 |  Val. Acc: 49.11%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.117 |  Val. Acc: 50.00%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.119 |  Val. Acc: 51.79%\n",
      "\tTrain Loss: 1.099 | Train Acc: 44.50%\n",
      "\t Val. Loss: 1.096 |  Val. Acc: 59.82%\n",
      "\tTrain Loss: 1.093 | Train Acc: 56.00%\n",
      "\t Val. Loss: 1.094 |  Val. Acc: 50.00%\n",
      "\tTrain Loss: 1.079 | Train Acc: 59.50%\n",
      "\t Val. Loss: 1.085 |  Val. Acc: 58.93%\n",
      "\tTrain Loss: 1.009 | Train Acc: 65.25%\n",
      "\t Val. Loss: 1.074 |  Val. Acc: 39.29%\n",
      "\tTrain Loss: 0.843 | Train Acc: 83.50%\n",
      "\t Val. Loss: 1.045 |  Val. Acc: 58.04%\n",
      "\tTrain Loss: 0.694 | Train Acc: 94.25%\n",
      "\t Val. Loss: 1.048 |  Val. Acc: 51.79%\n",
      "\tTrain Loss: 0.619 | Train Acc: 98.00%\n",
      "\t Val. Loss: 1.024 |  Val. Acc: 60.71%\n",
      "\tTrain Loss: 0.594 | Train Acc: 99.25%\n",
      "\t Val. Loss: 1.039 |  Val. Acc: 58.04%\n",
      "\tTrain Loss: 0.575 | Train Acc: 99.50%\n",
      "\t Val. Loss: 1.030 |  Val. Acc: 49.11%\n",
      "\tTrain Loss: 0.565 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.033 |  Val. Acc: 50.00%\n",
      "\tTrain Loss: 0.561 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.095 |  Val. Acc: 50.00%\n",
      "\tTrain Loss: 0.558 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.053 |  Val. Acc: 51.79%\n",
      "\tTrain Loss: 0.557 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.068 |  Val. Acc: 53.57%\n",
      "\tTrain Loss: 0.556 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.067 |  Val. Acc: 52.68%\n",
      "\tTrain Loss: 0.556 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.070 |  Val. Acc: 54.46%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.068 |  Val. Acc: 55.36%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.067 |  Val. Acc: 51.79%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.066 |  Val. Acc: 52.68%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.067 |  Val. Acc: 50.89%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.065 |  Val. Acc: 52.68%\n",
      "\tTrain Loss: 1.099 | Train Acc: 56.25%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 55.36%\n",
      "\tTrain Loss: 1.094 | Train Acc: 56.75%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 55.36%\n",
      "\tTrain Loss: 1.081 | Train Acc: 59.25%\n",
      "\t Val. Loss: 1.102 |  Val. Acc: 53.57%\n",
      "\tTrain Loss: 1.002 | Train Acc: 69.00%\n",
      "\t Val. Loss: 1.078 |  Val. Acc: 47.32%\n",
      "\tTrain Loss: 0.836 | Train Acc: 82.50%\n",
      "\t Val. Loss: 1.067 |  Val. Acc: 49.11%\n",
      "\tTrain Loss: 0.695 | Train Acc: 93.00%\n",
      "\t Val. Loss: 1.116 |  Val. Acc: 60.71%\n",
      "\tTrain Loss: 0.622 | Train Acc: 97.75%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 50.00%\n",
      "\tTrain Loss: 0.584 | Train Acc: 99.25%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 50.00%\n",
      "\tTrain Loss: 0.575 | Train Acc: 99.50%\n",
      "\t Val. Loss: 1.098 |  Val. Acc: 51.79%\n",
      "\tTrain Loss: 0.570 | Train Acc: 99.50%\n",
      "\t Val. Loss: 1.089 |  Val. Acc: 47.32%\n",
      "\tTrain Loss: 0.563 | Train Acc: 99.50%\n",
      "\t Val. Loss: 1.091 |  Val. Acc: 48.21%\n",
      "\tTrain Loss: 0.560 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.111 |  Val. Acc: 44.64%\n",
      "\tTrain Loss: 0.559 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.110 |  Val. Acc: 45.54%\n",
      "\tTrain Loss: 0.558 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.109 |  Val. Acc: 45.54%\n",
      "\tTrain Loss: 0.557 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.108 |  Val. Acc: 43.75%\n",
      "\tTrain Loss: 0.557 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.105 |  Val. Acc: 45.54%\n",
      "\tTrain Loss: 0.556 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 44.64%\n",
      "\tTrain Loss: 0.556 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.103 |  Val. Acc: 46.43%\n",
      "\tTrain Loss: 0.556 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 46.43%\n",
      "\tTrain Loss: 0.555 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 46.43%\n",
      "\tTrain Loss: 1.099 | Train Acc: 48.50%\n",
      "\t Val. Loss: 1.096 |  Val. Acc: 58.04%\n",
      "\tTrain Loss: 1.093 | Train Acc: 58.50%\n",
      "\t Val. Loss: 1.096 |  Val. Acc: 46.43%\n",
      "\tTrain Loss: 1.073 | Train Acc: 61.25%\n",
      "\t Val. Loss: 1.104 |  Val. Acc: 21.43%\n",
      "\tTrain Loss: 1.009 | Train Acc: 58.75%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 32.14%\n",
      "\tTrain Loss: 0.881 | Train Acc: 76.00%\n",
      "\t Val. Loss: 1.112 |  Val. Acc: 48.21%\n",
      "\tTrain Loss: 0.789 | Train Acc: 86.25%\n",
      "\t Val. Loss: 1.098 |  Val. Acc: 57.14%\n",
      "\tTrain Loss: 0.705 | Train Acc: 91.25%\n",
      "\t Val. Loss: 1.086 |  Val. Acc: 54.46%\n",
      "\tTrain Loss: 0.639 | Train Acc: 94.75%\n",
      "\t Val. Loss: 1.058 |  Val. Acc: 44.64%\n",
      "\tTrain Loss: 0.614 | Train Acc: 95.75%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 37.50%\n",
      "\tTrain Loss: 0.591 | Train Acc: 98.25%\n",
      "\t Val. Loss: 1.060 |  Val. Acc: 53.57%\n",
      "\tTrain Loss: 0.574 | Train Acc: 99.25%\n",
      "\t Val. Loss: 1.022 |  Val. Acc: 62.50%\n",
      "\tTrain Loss: 0.563 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.023 |  Val. Acc: 59.82%\n",
      "\tTrain Loss: 0.560 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.031 |  Val. Acc: 59.82%\n",
      "\tTrain Loss: 0.558 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.022 |  Val. Acc: 61.61%\n",
      "\tTrain Loss: 0.557 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.034 |  Val. Acc: 58.93%\n",
      "\tTrain Loss: 0.556 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.041 |  Val. Acc: 58.04%\n",
      "\tTrain Loss: 0.556 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.038 |  Val. Acc: 58.04%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.035 |  Val. Acc: 58.04%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.030 |  Val. Acc: 58.93%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.020 |  Val. Acc: 58.93%\n",
      "\tTrain Loss: 1.099 | Train Acc: 56.25%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 58.04%\n",
      "\tTrain Loss: 1.093 | Train Acc: 56.00%\n",
      "\t Val. Loss: 1.098 |  Val. Acc: 58.04%\n",
      "\tTrain Loss: 1.076 | Train Acc: 62.00%\n",
      "\t Val. Loss: 1.094 |  Val. Acc: 51.79%\n",
      "\tTrain Loss: 0.992 | Train Acc: 70.75%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 53.57%\n",
      "\tTrain Loss: 0.839 | Train Acc: 82.75%\n",
      "\t Val. Loss: 1.083 |  Val. Acc: 52.68%\n",
      "\tTrain Loss: 0.695 | Train Acc: 93.00%\n",
      "\t Val. Loss: 1.074 |  Val. Acc: 60.71%\n",
      "\tTrain Loss: 0.622 | Train Acc: 97.75%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 50.89%\n",
      "\tTrain Loss: 0.595 | Train Acc: 97.50%\n",
      "\t Val. Loss: 1.086 |  Val. Acc: 57.14%\n",
      "\tTrain Loss: 0.579 | Train Acc: 99.50%\n",
      "\t Val. Loss: 1.109 |  Val. Acc: 48.21%\n",
      "\tTrain Loss: 0.569 | Train Acc: 99.50%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 56.25%\n",
      "\tTrain Loss: 0.565 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.098 |  Val. Acc: 52.68%\n",
      "\tTrain Loss: 0.560 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.078 |  Val. Acc: 56.25%\n",
      "\tTrain Loss: 0.558 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.082 |  Val. Acc: 55.36%\n",
      "\tTrain Loss: 0.557 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.085 |  Val. Acc: 53.57%\n",
      "\tTrain Loss: 0.556 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 53.57%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 54.46%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 54.46%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.087 |  Val. Acc: 55.36%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.087 |  Val. Acc: 54.46%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.087 |  Val. Acc: 54.46%\n",
      "\tTrain Loss: 1.100 | Train Acc: 20.75%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 24.11%\n",
      "\tTrain Loss: 1.095 | Train Acc: 52.00%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 53.57%\n",
      "\tTrain Loss: 1.079 | Train Acc: 57.00%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 54.46%\n",
      "\tTrain Loss: 1.003 | Train Acc: 62.75%\n",
      "\t Val. Loss: 1.070 |  Val. Acc: 41.07%\n",
      "\tTrain Loss: 0.868 | Train Acc: 73.00%\n",
      "\t Val. Loss: 1.057 |  Val. Acc: 58.93%\n",
      "\tTrain Loss: 0.770 | Train Acc: 87.00%\n",
      "\t Val. Loss: 1.052 |  Val. Acc: 41.96%\n",
      "\tTrain Loss: 0.710 | Train Acc: 91.00%\n",
      "\t Val. Loss: 1.051 |  Val. Acc: 54.46%\n",
      "\tTrain Loss: 0.648 | Train Acc: 96.00%\n",
      "\t Val. Loss: 1.056 |  Val. Acc: 51.79%\n",
      "\tTrain Loss: 0.606 | Train Acc: 98.25%\n",
      "\t Val. Loss: 1.046 |  Val. Acc: 53.57%\n",
      "\tTrain Loss: 0.583 | Train Acc: 99.25%\n",
      "\t Val. Loss: 1.029 |  Val. Acc: 51.79%\n",
      "\tTrain Loss: 0.568 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.043 |  Val. Acc: 55.36%\n",
      "\tTrain Loss: 0.562 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.037 |  Val. Acc: 54.46%\n",
      "\tTrain Loss: 0.559 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.035 |  Val. Acc: 52.68%\n",
      "\tTrain Loss: 0.557 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.044 |  Val. Acc: 52.68%\n",
      "\tTrain Loss: 0.556 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.045 |  Val. Acc: 51.79%\n",
      "\tTrain Loss: 0.556 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.038 |  Val. Acc: 54.46%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.040 |  Val. Acc: 52.68%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.039 |  Val. Acc: 53.57%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.040 |  Val. Acc: 52.68%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.040 |  Val. Acc: 53.57%\n",
      "\tTrain Loss: 1.100 | Train Acc: 18.50%\n",
      "\t Val. Loss: 1.098 |  Val. Acc: 18.75%\n",
      "\tTrain Loss: 1.094 | Train Acc: 45.00%\n",
      "\t Val. Loss: 1.095 |  Val. Acc: 50.00%\n",
      "\tTrain Loss: 1.080 | Train Acc: 55.50%\n",
      "\t Val. Loss: 1.087 |  Val. Acc: 53.57%\n",
      "\tTrain Loss: 1.008 | Train Acc: 63.75%\n",
      "\t Val. Loss: 1.083 |  Val. Acc: 41.07%\n",
      "\tTrain Loss: 0.889 | Train Acc: 74.25%\n",
      "\t Val. Loss: 1.067 |  Val. Acc: 50.89%\n",
      "\tTrain Loss: 0.755 | Train Acc: 89.25%\n",
      "\t Val. Loss: 1.050 |  Val. Acc: 56.25%\n",
      "\tTrain Loss: 0.651 | Train Acc: 94.25%\n",
      "\t Val. Loss: 1.047 |  Val. Acc: 49.11%\n",
      "\tTrain Loss: 0.604 | Train Acc: 98.25%\n",
      "\t Val. Loss: 1.053 |  Val. Acc: 49.11%\n",
      "\tTrain Loss: 0.580 | Train Acc: 99.50%\n",
      "\t Val. Loss: 1.065 |  Val. Acc: 51.79%\n",
      "\tTrain Loss: 0.571 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.031 |  Val. Acc: 52.68%\n",
      "\tTrain Loss: 0.564 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.056 |  Val. Acc: 51.79%\n",
      "\tTrain Loss: 0.560 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.028 |  Val. Acc: 53.57%\n",
      "\tTrain Loss: 0.557 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.040 |  Val. Acc: 50.89%\n",
      "\tTrain Loss: 0.556 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.042 |  Val. Acc: 52.68%\n",
      "\tTrain Loss: 0.556 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.043 |  Val. Acc: 50.89%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.043 |  Val. Acc: 50.89%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.048 |  Val. Acc: 50.89%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.049 |  Val. Acc: 50.89%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.043 |  Val. Acc: 50.89%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.042 |  Val. Acc: 50.89%\n",
      "\tTrain Loss: 1.100 | Train Acc: 37.75%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 52.68%\n",
      "\tTrain Loss: 1.094 | Train Acc: 50.50%\n",
      "\t Val. Loss: 1.098 |  Val. Acc: 54.46%\n",
      "\tTrain Loss: 1.081 | Train Acc: 58.75%\n",
      "\t Val. Loss: 1.094 |  Val. Acc: 52.68%\n",
      "\tTrain Loss: 1.006 | Train Acc: 71.00%\n",
      "\t Val. Loss: 1.069 |  Val. Acc: 50.00%\n",
      "\tTrain Loss: 0.856 | Train Acc: 81.50%\n",
      "\t Val. Loss: 1.065 |  Val. Acc: 48.21%\n",
      "\tTrain Loss: 0.750 | Train Acc: 87.25%\n",
      "\t Val. Loss: 1.091 |  Val. Acc: 50.89%\n",
      "\tTrain Loss: 0.663 | Train Acc: 95.50%\n",
      "\t Val. Loss: 1.063 |  Val. Acc: 48.21%\n",
      "\tTrain Loss: 0.602 | Train Acc: 99.00%\n",
      "\t Val. Loss: 1.069 |  Val. Acc: 46.43%\n",
      "\tTrain Loss: 0.577 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.051 |  Val. Acc: 48.21%\n",
      "\tTrain Loss: 0.567 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.049 |  Val. Acc: 49.11%\n",
      "\tTrain Loss: 0.567 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.084 |  Val. Acc: 51.79%\n",
      "\tTrain Loss: 0.561 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.079 |  Val. Acc: 56.25%\n",
      "\tTrain Loss: 0.558 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.079 |  Val. Acc: 50.89%\n",
      "\tTrain Loss: 0.557 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.083 |  Val. Acc: 52.68%\n",
      "\tTrain Loss: 0.556 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.087 |  Val. Acc: 54.46%\n",
      "\tTrain Loss: 0.556 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.084 |  Val. Acc: 55.36%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.084 |  Val. Acc: 56.25%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.084 |  Val. Acc: 56.25%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.083 |  Val. Acc: 57.14%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.083 |  Val. Acc: 57.14%\n",
      "\tTrain Loss: 1.099 | Train Acc: 56.50%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 52.68%\n",
      "\tTrain Loss: 1.094 | Train Acc: 56.50%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 51.79%\n",
      "\tTrain Loss: 1.080 | Train Acc: 60.00%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 49.11%\n",
      "\tTrain Loss: 1.002 | Train Acc: 77.25%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 39.29%\n",
      "\tTrain Loss: 0.856 | Train Acc: 82.00%\n",
      "\t Val. Loss: 1.115 |  Val. Acc: 48.21%\n",
      "\tTrain Loss: 0.696 | Train Acc: 96.50%\n",
      "\t Val. Loss: 1.132 |  Val. Acc: 41.96%\n",
      "\tTrain Loss: 0.613 | Train Acc: 98.00%\n",
      "\t Val. Loss: 1.147 |  Val. Acc: 35.71%\n",
      "\tTrain Loss: 0.586 | Train Acc: 99.25%\n",
      "\t Val. Loss: 1.142 |  Val. Acc: 35.71%\n",
      "\tTrain Loss: 0.571 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.082 |  Val. Acc: 41.07%\n",
      "\tTrain Loss: 0.565 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.137 |  Val. Acc: 43.75%\n",
      "\tTrain Loss: 0.561 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.163 |  Val. Acc: 41.96%\n",
      "\tTrain Loss: 0.559 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.158 |  Val. Acc: 46.43%\n",
      "\tTrain Loss: 0.559 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.163 |  Val. Acc: 42.86%\n",
      "\tTrain Loss: 0.557 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.174 |  Val. Acc: 44.64%\n",
      "\tTrain Loss: 0.556 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.172 |  Val. Acc: 46.43%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.174 |  Val. Acc: 46.43%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.178 |  Val. Acc: 46.43%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.178 |  Val. Acc: 45.54%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.180 |  Val. Acc: 48.21%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.176 |  Val. Acc: 45.54%\n",
      "\tTrain Loss: 1.100 | Train Acc: 36.50%\n",
      "\t Val. Loss: 1.096 |  Val. Acc: 52.68%\n",
      "\tTrain Loss: 1.093 | Train Acc: 58.25%\n",
      "\t Val. Loss: 1.095 |  Val. Acc: 53.57%\n",
      "\tTrain Loss: 1.076 | Train Acc: 64.50%\n",
      "\t Val. Loss: 1.089 |  Val. Acc: 53.57%\n",
      "\tTrain Loss: 0.988 | Train Acc: 69.75%\n",
      "\t Val. Loss: 1.070 |  Val. Acc: 51.79%\n",
      "\tTrain Loss: 0.810 | Train Acc: 85.25%\n",
      "\t Val. Loss: 1.069 |  Val. Acc: 42.86%\n",
      "\tTrain Loss: 0.690 | Train Acc: 94.75%\n",
      "\t Val. Loss: 1.040 |  Val. Acc: 63.39%\n",
      "\tTrain Loss: 0.622 | Train Acc: 97.50%\n",
      "\t Val. Loss: 1.066 |  Val. Acc: 49.11%\n",
      "\tTrain Loss: 0.588 | Train Acc: 98.50%\n",
      "\t Val. Loss: 1.068 |  Val. Acc: 50.00%\n",
      "\tTrain Loss: 0.576 | Train Acc: 99.25%\n",
      "\t Val. Loss: 1.069 |  Val. Acc: 55.36%\n",
      "\tTrain Loss: 0.565 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.073 |  Val. Acc: 53.57%\n",
      "\tTrain Loss: 0.560 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.060 |  Val. Acc: 55.36%\n",
      "\tTrain Loss: 0.558 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.055 |  Val. Acc: 53.57%\n",
      "\tTrain Loss: 0.557 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.063 |  Val. Acc: 54.46%\n",
      "\tTrain Loss: 0.556 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.052 |  Val. Acc: 55.36%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.048 |  Val. Acc: 55.36%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.045 |  Val. Acc: 55.36%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.043 |  Val. Acc: 53.57%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.039 |  Val. Acc: 53.57%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.037 |  Val. Acc: 53.57%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.037 |  Val. Acc: 53.57%\n",
      "\tTrain Loss: 1.099 | Train Acc: 55.50%\n",
      "\t Val. Loss: 1.098 |  Val. Acc: 56.25%\n",
      "\tTrain Loss: 1.095 | Train Acc: 56.25%\n",
      "\t Val. Loss: 1.096 |  Val. Acc: 58.04%\n",
      "\tTrain Loss: 1.085 | Train Acc: 53.25%\n",
      "\t Val. Loss: 1.095 |  Val. Acc: 55.36%\n",
      "\tTrain Loss: 1.025 | Train Acc: 63.75%\n",
      "\t Val. Loss: 1.059 |  Val. Acc: 54.46%\n",
      "\tTrain Loss: 0.885 | Train Acc: 73.75%\n",
      "\t Val. Loss: 1.071 |  Val. Acc: 49.11%\n",
      "\tTrain Loss: 0.756 | Train Acc: 87.00%\n",
      "\t Val. Loss: 1.037 |  Val. Acc: 49.11%\n",
      "\tTrain Loss: 0.681 | Train Acc: 91.00%\n",
      "\t Val. Loss: 1.062 |  Val. Acc: 58.04%\n",
      "\tTrain Loss: 0.610 | Train Acc: 97.75%\n",
      "\t Val. Loss: 1.051 |  Val. Acc: 56.25%\n",
      "\tTrain Loss: 0.586 | Train Acc: 97.75%\n",
      "\t Val. Loss: 1.048 |  Val. Acc: 45.54%\n",
      "\tTrain Loss: 0.579 | Train Acc: 99.00%\n",
      "\t Val. Loss: 1.026 |  Val. Acc: 54.46%\n",
      "\tTrain Loss: 0.567 | Train Acc: 99.50%\n",
      "\t Val. Loss: 1.056 |  Val. Acc: 56.25%\n",
      "\tTrain Loss: 0.562 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.056 |  Val. Acc: 55.36%\n",
      "\tTrain Loss: 0.559 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.042 |  Val. Acc: 57.14%\n",
      "\tTrain Loss: 0.556 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.034 |  Val. Acc: 56.25%\n",
      "\tTrain Loss: 0.556 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.037 |  Val. Acc: 56.25%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.037 |  Val. Acc: 57.14%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.037 |  Val. Acc: 58.04%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.037 |  Val. Acc: 58.93%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.038 |  Val. Acc: 58.93%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.038 |  Val. Acc: 58.93%\n",
      "\tTrain Loss: 1.100 | Train Acc: 51.50%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 55.36%\n",
      "\tTrain Loss: 1.093 | Train Acc: 56.25%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 51.79%\n",
      "\tTrain Loss: 1.074 | Train Acc: 61.00%\n",
      "\t Val. Loss: 1.110 |  Val. Acc: 55.36%\n",
      "\tTrain Loss: 1.020 | Train Acc: 61.75%\n",
      "\t Val. Loss: 1.123 |  Val. Acc: 24.11%\n",
      "\tTrain Loss: 0.891 | Train Acc: 76.50%\n",
      "\t Val. Loss: 1.140 |  Val. Acc: 55.36%\n",
      "\tTrain Loss: 0.768 | Train Acc: 84.25%\n",
      "\t Val. Loss: 1.136 |  Val. Acc: 58.93%\n",
      "\tTrain Loss: 0.689 | Train Acc: 90.75%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 45.54%\n",
      "\tTrain Loss: 0.620 | Train Acc: 97.25%\n",
      "\t Val. Loss: 1.119 |  Val. Acc: 54.46%\n",
      "\tTrain Loss: 0.596 | Train Acc: 98.75%\n",
      "\t Val. Loss: 1.090 |  Val. Acc: 52.68%\n",
      "\tTrain Loss: 0.568 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 56.25%\n",
      "\tTrain Loss: 0.563 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.113 |  Val. Acc: 41.07%\n",
      "\tTrain Loss: 0.564 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 51.79%\n",
      "\tTrain Loss: 0.559 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 47.32%\n",
      "\tTrain Loss: 0.558 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.096 |  Val. Acc: 54.46%\n",
      "\tTrain Loss: 0.557 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.110 |  Val. Acc: 54.46%\n",
      "\tTrain Loss: 0.556 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.068 |  Val. Acc: 55.36%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 57.14%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.096 |  Val. Acc: 57.14%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.094 |  Val. Acc: 57.14%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.091 |  Val. Acc: 60.71%\n",
      "\tTrain Loss: 1.100 | Train Acc: 18.50%\n",
      "\t Val. Loss: 1.098 |  Val. Acc: 25.89%\n",
      "\tTrain Loss: 1.094 | Train Acc: 56.50%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 55.36%\n",
      "\tTrain Loss: 1.082 | Train Acc: 60.25%\n",
      "\t Val. Loss: 1.095 |  Val. Acc: 54.46%\n",
      "\tTrain Loss: 1.010 | Train Acc: 68.25%\n",
      "\t Val. Loss: 1.063 |  Val. Acc: 41.07%\n",
      "\tTrain Loss: 0.865 | Train Acc: 83.00%\n",
      "\t Val. Loss: 1.072 |  Val. Acc: 55.36%\n",
      "\tTrain Loss: 0.721 | Train Acc: 93.25%\n",
      "\t Val. Loss: 1.074 |  Val. Acc: 57.14%\n",
      "\tTrain Loss: 0.649 | Train Acc: 95.75%\n",
      "\t Val. Loss: 1.016 |  Val. Acc: 57.14%\n",
      "\tTrain Loss: 0.608 | Train Acc: 98.50%\n",
      "\t Val. Loss: 1.012 |  Val. Acc: 56.25%\n",
      "\tTrain Loss: 0.581 | Train Acc: 99.50%\n",
      "\t Val. Loss: 1.051 |  Val. Acc: 50.89%\n",
      "\tTrain Loss: 0.572 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.042 |  Val. Acc: 51.79%\n",
      "\tTrain Loss: 0.564 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.060 |  Val. Acc: 61.61%\n",
      "\tTrain Loss: 0.560 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.071 |  Val. Acc: 56.25%\n",
      "\tTrain Loss: 0.558 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.066 |  Val. Acc: 60.71%\n",
      "\tTrain Loss: 0.557 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.061 |  Val. Acc: 59.82%\n",
      "\tTrain Loss: 0.556 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.059 |  Val. Acc: 60.71%\n",
      "\tTrain Loss: 0.556 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.061 |  Val. Acc: 58.93%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.063 |  Val. Acc: 58.93%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.064 |  Val. Acc: 59.82%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.066 |  Val. Acc: 58.93%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.067 |  Val. Acc: 58.04%\n",
      "\tTrain Loss: 1.100 | Train Acc: 24.75%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 29.46%\n",
      "\tTrain Loss: 1.093 | Train Acc: 59.25%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 50.89%\n",
      "\tTrain Loss: 1.071 | Train Acc: 64.50%\n",
      "\t Val. Loss: 1.110 |  Val. Acc: 28.57%\n",
      "\tTrain Loss: 0.975 | Train Acc: 68.25%\n",
      "\t Val. Loss: 1.121 |  Val. Acc: 29.46%\n",
      "\tTrain Loss: 0.830 | Train Acc: 78.00%\n",
      "\t Val. Loss: 1.153 |  Val. Acc: 33.04%\n",
      "\tTrain Loss: 0.723 | Train Acc: 89.75%\n",
      "\t Val. Loss: 1.157 |  Val. Acc: 33.93%\n",
      "\tTrain Loss: 0.637 | Train Acc: 96.75%\n",
      "\t Val. Loss: 1.123 |  Val. Acc: 43.75%\n",
      "\tTrain Loss: 0.592 | Train Acc: 98.00%\n",
      "\t Val. Loss: 1.115 |  Val. Acc: 48.21%\n",
      "\tTrain Loss: 0.575 | Train Acc: 99.25%\n",
      "\t Val. Loss: 1.121 |  Val. Acc: 50.00%\n",
      "\tTrain Loss: 0.569 | Train Acc: 99.25%\n",
      "\t Val. Loss: 1.167 |  Val. Acc: 50.89%\n",
      "\tTrain Loss: 0.574 | Train Acc: 98.50%\n",
      "\t Val. Loss: 1.088 |  Val. Acc: 51.79%\n",
      "\tTrain Loss: 0.563 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.172 |  Val. Acc: 42.86%\n",
      "\tTrain Loss: 0.559 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.154 |  Val. Acc: 49.11%\n",
      "\tTrain Loss: 0.557 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.164 |  Val. Acc: 48.21%\n",
      "\tTrain Loss: 0.556 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.167 |  Val. Acc: 46.43%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.162 |  Val. Acc: 46.43%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.159 |  Val. Acc: 46.43%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.158 |  Val. Acc: 46.43%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.150 |  Val. Acc: 47.32%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.149 |  Val. Acc: 47.32%\n",
      "\tTrain Loss: 1.099 | Train Acc: 18.75%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 19.64%\n",
      "\tTrain Loss: 1.094 | Train Acc: 52.25%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 51.79%\n",
      "\tTrain Loss: 1.081 | Train Acc: 62.25%\n",
      "\t Val. Loss: 1.102 |  Val. Acc: 25.89%\n",
      "\tTrain Loss: 0.991 | Train Acc: 73.25%\n",
      "\t Val. Loss: 1.106 |  Val. Acc: 41.07%\n",
      "\tTrain Loss: 0.809 | Train Acc: 84.50%\n",
      "\t Val. Loss: 1.131 |  Val. Acc: 50.89%\n",
      "\tTrain Loss: 0.711 | Train Acc: 94.00%\n",
      "\t Val. Loss: 1.154 |  Val. Acc: 41.07%\n",
      "\tTrain Loss: 0.645 | Train Acc: 94.00%\n",
      "\t Val. Loss: 1.128 |  Val. Acc: 34.82%\n",
      "\tTrain Loss: 0.597 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.143 |  Val. Acc: 43.75%\n",
      "\tTrain Loss: 0.575 | Train Acc: 99.50%\n",
      "\t Val. Loss: 1.153 |  Val. Acc: 41.96%\n",
      "\tTrain Loss: 0.565 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.165 |  Val. Acc: 40.18%\n",
      "\tTrain Loss: 0.561 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.170 |  Val. Acc: 41.07%\n",
      "\tTrain Loss: 0.560 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.173 |  Val. Acc: 41.07%\n",
      "\tTrain Loss: 0.558 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.168 |  Val. Acc: 43.75%\n",
      "\tTrain Loss: 0.557 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.185 |  Val. Acc: 36.61%\n",
      "\tTrain Loss: 0.557 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.183 |  Val. Acc: 39.29%\n",
      "\tTrain Loss: 0.556 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.185 |  Val. Acc: 38.39%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.187 |  Val. Acc: 39.29%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.190 |  Val. Acc: 40.18%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.192 |  Val. Acc: 39.29%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.185 |  Val. Acc: 40.18%\n",
      "\tTrain Loss: 1.099 | Train Acc: 56.00%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 58.04%\n",
      "\tTrain Loss: 1.093 | Train Acc: 56.25%\n",
      "\t Val. Loss: 1.096 |  Val. Acc: 58.04%\n",
      "\tTrain Loss: 1.076 | Train Acc: 62.50%\n",
      "\t Val. Loss: 1.094 |  Val. Acc: 21.43%\n",
      "\tTrain Loss: 0.993 | Train Acc: 68.75%\n",
      "\t Val. Loss: 1.096 |  Val. Acc: 32.14%\n",
      "\tTrain Loss: 0.838 | Train Acc: 83.75%\n",
      "\t Val. Loss: 1.087 |  Val. Acc: 54.46%\n",
      "\tTrain Loss: 0.713 | Train Acc: 94.25%\n",
      "\t Val. Loss: 1.131 |  Val. Acc: 33.04%\n",
      "\tTrain Loss: 0.650 | Train Acc: 96.00%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 49.11%\n",
      "\tTrain Loss: 0.593 | Train Acc: 99.25%\n",
      "\t Val. Loss: 1.070 |  Val. Acc: 58.04%\n",
      "\tTrain Loss: 0.581 | Train Acc: 99.50%\n",
      "\t Val. Loss: 1.147 |  Val. Acc: 41.96%\n",
      "\tTrain Loss: 0.584 | Train Acc: 99.00%\n",
      "\t Val. Loss: 1.136 |  Val. Acc: 50.89%\n",
      "\tTrain Loss: 0.571 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.110 |  Val. Acc: 44.64%\n",
      "\tTrain Loss: 0.569 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.086 |  Val. Acc: 49.11%\n",
      "\tTrain Loss: 0.565 | Train Acc: 99.50%\n",
      "\t Val. Loss: 1.110 |  Val. Acc: 55.36%\n",
      "\tTrain Loss: 0.560 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.130 |  Val. Acc: 54.46%\n",
      "\tTrain Loss: 0.558 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.139 |  Val. Acc: 51.79%\n",
      "\tTrain Loss: 0.557 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.141 |  Val. Acc: 51.79%\n",
      "\tTrain Loss: 0.556 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.143 |  Val. Acc: 49.11%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.146 |  Val. Acc: 50.00%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.151 |  Val. Acc: 50.00%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.153 |  Val. Acc: 50.89%\n",
      "\tTrain Loss: 1.100 | Train Acc: 19.75%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 29.46%\n",
      "\tTrain Loss: 1.093 | Train Acc: 56.00%\n",
      "\t Val. Loss: 1.096 |  Val. Acc: 57.14%\n",
      "\tTrain Loss: 1.073 | Train Acc: 63.50%\n",
      "\t Val. Loss: 1.094 |  Val. Acc: 34.82%\n",
      "\tTrain Loss: 0.974 | Train Acc: 69.25%\n",
      "\t Val. Loss: 1.105 |  Val. Acc: 44.64%\n",
      "\tTrain Loss: 0.835 | Train Acc: 78.25%\n",
      "\t Val. Loss: 1.105 |  Val. Acc: 28.57%\n",
      "\tTrain Loss: 0.753 | Train Acc: 85.25%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 39.29%\n",
      "\tTrain Loss: 0.651 | Train Acc: 95.25%\n",
      "\t Val. Loss: 1.148 |  Val. Acc: 49.11%\n",
      "\tTrain Loss: 0.609 | Train Acc: 96.50%\n",
      "\t Val. Loss: 1.148 |  Val. Acc: 47.32%\n",
      "\tTrain Loss: 0.579 | Train Acc: 99.00%\n",
      "\t Val. Loss: 1.126 |  Val. Acc: 39.29%\n",
      "\tTrain Loss: 0.569 | Train Acc: 99.25%\n",
      "\t Val. Loss: 1.133 |  Val. Acc: 41.96%\n",
      "\tTrain Loss: 0.566 | Train Acc: 99.25%\n",
      "\t Val. Loss: 1.141 |  Val. Acc: 43.75%\n",
      "\tTrain Loss: 0.563 | Train Acc: 99.25%\n",
      "\t Val. Loss: 1.133 |  Val. Acc: 40.18%\n",
      "\tTrain Loss: 0.560 | Train Acc: 99.50%\n",
      "\t Val. Loss: 1.141 |  Val. Acc: 41.07%\n",
      "\tTrain Loss: 0.559 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.138 |  Val. Acc: 41.07%\n",
      "\tTrain Loss: 0.558 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.135 |  Val. Acc: 41.07%\n",
      "\tTrain Loss: 0.558 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.132 |  Val. Acc: 41.96%\n",
      "\tTrain Loss: 0.557 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.128 |  Val. Acc: 43.75%\n",
      "\tTrain Loss: 0.556 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.130 |  Val. Acc: 43.75%\n",
      "\tTrain Loss: 0.556 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.129 |  Val. Acc: 44.64%\n",
      "\tTrain Loss: 0.556 | Train Acc: 99.75%\n",
      "\t Val. Loss: 1.128 |  Val. Acc: 43.75%\n",
      "\tTrain Loss: 1.100 | Train Acc: 32.50%\n",
      "\t Val. Loss: 1.099 |  Val. Acc: 46.43%\n",
      "\tTrain Loss: 1.094 | Train Acc: 58.50%\n",
      "\t Val. Loss: 1.098 |  Val. Acc: 54.46%\n",
      "\tTrain Loss: 1.080 | Train Acc: 60.75%\n",
      "\t Val. Loss: 1.097 |  Val. Acc: 43.75%\n",
      "\tTrain Loss: 1.006 | Train Acc: 71.00%\n",
      "\t Val. Loss: 1.071 |  Val. Acc: 58.04%\n",
      "\tTrain Loss: 0.848 | Train Acc: 84.75%\n",
      "\t Val. Loss: 1.059 |  Val. Acc: 56.25%\n",
      "\tTrain Loss: 0.712 | Train Acc: 94.50%\n",
      "\t Val. Loss: 1.089 |  Val. Acc: 55.36%\n",
      "\tTrain Loss: 0.640 | Train Acc: 94.75%\n",
      "\t Val. Loss: 1.043 |  Val. Acc: 57.14%\n",
      "\tTrain Loss: 0.597 | Train Acc: 98.75%\n",
      "\t Val. Loss: 1.062 |  Val. Acc: 51.79%\n",
      "\tTrain Loss: 0.573 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.078 |  Val. Acc: 58.04%\n",
      "\tTrain Loss: 0.567 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.101 |  Val. Acc: 53.57%\n",
      "\tTrain Loss: 0.561 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.064 |  Val. Acc: 59.82%\n",
      "\tTrain Loss: 0.562 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.123 |  Val. Acc: 55.36%\n",
      "\tTrain Loss: 0.559 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.051 |  Val. Acc: 58.93%\n",
      "\tTrain Loss: 0.557 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.049 |  Val. Acc: 58.93%\n",
      "\tTrain Loss: 0.556 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.044 |  Val. Acc: 58.93%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.066 |  Val. Acc: 56.25%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.055 |  Val. Acc: 56.25%\n",
      "\tTrain Loss: 0.555 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.067 |  Val. Acc: 56.25%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.049 |  Val. Acc: 56.25%\n",
      "\tTrain Loss: 0.554 | Train Acc: 100.00%\n",
      "\t Val. Loss: 1.052 |  Val. Acc: 56.25%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 20\n",
    "runs = 50\n",
    "accuracies = []\n",
    "\n",
    "for run in range(runs):\n",
    "    train_iterator, valid_iterator, neg_train, neut_train, pos_train = random_train_test_split(0.8)\n",
    "\n",
    "    #instantiate the model\n",
    "    model = classifier(size_of_vocab, embedding_dim, num_hidden_nodes,num_output_nodes, num_layers,\n",
    "                   bidirectional = True, dropout = dropout)\n",
    "    #Initialize the pretrained embedding\n",
    "    pretrained_embeddings = TEXT.vocab.vectors\n",
    "    model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "    #define optimizer and loss\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    # determine the count of the class with the samples training set\n",
    "    max_count = max([neg_train, neut_train, pos_train])\n",
    "    criterion = nn.CrossEntropyLoss(weight=torch.FloatTensor([max_count/float(neg_train), max_count/float(neut_train), max_count/float(pos_train)]))\n",
    "    #push to cuda if available\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "\n",
    "    best_valid_loss = float('inf')\n",
    "    best_valid_acc = 0.0\n",
    "    for epoch in range(N_EPOCHS):\n",
    "\n",
    "        #train the model\n",
    "        train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "\n",
    "        #evaluate the model\n",
    "        valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "        #save the best model\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            best_valid_acc = valid_acc\n",
    "            torch.save(model.state_dict(), 'saved_weights/deep_classifier_04.pt')\n",
    "\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "    accuracies.append(best_valid_acc)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot accuracies\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(sorted(accuracies), label='Model accuracies')\n",
    "plt.axhline(y=0.33, color='r', linestyle='-', label='Random guess')\n",
    "plt.axhline(y=sum(accuracies)/runs, color='orange', linestyle='-', label='Mean accuracy')\n",
    "plt.ylabel('Model Accuracies')\n",
    "plt.title(f\"Accuracies of {runs} individual train-evaluation runs\")\n",
    "plt.legend(loc=\"lower right\", borderaxespad=0)\n",
    "plt.savefig('plots/accuracies_deep04.png')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "\n",
    "#load weights\n",
    "path='saved_weights/deep_classifier_04.pt'\n",
    "model.load_state_dict(torch.load(path));\n",
    "model.eval();\n",
    "\n",
    "#inference"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "def predict_sentence(model, sentence):\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]  #tokenize the sentence\n",
    "    return predict(model, tokenized)\n",
    "\n",
    "\n",
    "def predict(model, tokenized):\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]          #convert to integer sequence\n",
    "    length = [len(indexed)]                                    #compute no. of words\n",
    "    tensor = torch.LongTensor(indexed).to(device)              #convert to tensor\n",
    "    tensor = tensor.unsqueeze(1).T                             #reshape in form of batch,no. of words\n",
    "    length_tensor = torch.LongTensor(length)                   #convert to tensor\n",
    "    prediction = model(tensor, length_tensor)                  #prediction\n",
    "    return prediction\n",
    "\n",
    "tens = predict_sentence(model, \"Ich bin ein Lockdown.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#make predictions\n",
    "sentence = \"Ich bin ein Lockdown.\"\n",
    "probs = predict_sentence(model, sentence)\n",
    "label = LABEL.vocab.itos[torch.argmax(probs)]\n",
    "print(sentence + \" => \" + label)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# determine validation accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "for e in valid_data:\n",
    "    # print(getattr(e,'label'))\n",
    "    # print(getattr(e, 'text'))\n",
    "    target_label = getattr(e, 'label')\n",
    "    tokenized_sentence = getattr(e, 'text')\n",
    "    preds = predict(model, tokenized_sentence)\n",
    "    pred_label = LABEL.vocab.itos[torch.argmax(preds)]\n",
    "    if pred_label == target_label:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "\n",
    "accuracy = correct / total"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "print(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}