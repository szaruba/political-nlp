{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Multinomial Naive Bayes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of each class:\n",
      "1: 0.04259472890229834\n",
      "2: 0.05155736977549028\n",
      "3: 0.05075871860857219\n",
      "4: 0.05208980388676901\n",
      "5: 0.051024935664211554\n",
      "6: 0.052533498979501284\n",
      "7: 0.051646108794036735\n",
      "8: 0.052533498979501284\n",
      "9: 0.052888455053687104\n",
      "10: 0.0527109770165942\n",
      "11: 0.05306593309078002\n",
      "12: 0.0527109770165942\n",
      "13: 0.05244475996095483\n",
      "14: 0.0527109770165942\n",
      "15: 0.052622237998047744\n",
      "16: 0.05315467210932647\n",
      "17: 0.04836276510781791\n",
      "18: 0.05004880646020055\n",
      "19: 0.04117490460555506\n",
      "20: 0.033365870973467035\n"
     ]
    }
   ],
   "source": [
    "#Training label\n",
    "train_label = open('datasets/20news-bydate/matlab/train.label')\n",
    "\n",
    "#pi is the fraction of each class\n",
    "pi = {}\n",
    "\n",
    "#Set a class index for each document as key\n",
    "for i in range(1,21):\n",
    "    pi[i] = 0\n",
    "\n",
    "#Extract values from training labels\n",
    "lines = train_label.readlines()\n",
    "\n",
    "#Get total number of documents\n",
    "total = len(lines)\n",
    "\n",
    "#Count the occurence of each class\n",
    "for line in lines:\n",
    "    val = int(line.split()[0])\n",
    "    pi[val] += 1\n",
    "\n",
    "#Divide the count of each class by total documents\n",
    "for key in pi:\n",
    "    pi[key] /= total\n",
    "\n",
    "print(\"Probability of each class:\")\n",
    "print(\"\\n\".join(\"{}: {}\".format(k, v) for k, v in pi.items()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "   docIdx  wordIdx  count  classIdx\n0       1        1      4         1\n1       1        2      2         1\n2       1        3     10         1\n3       1        4      4         1\n4       1        5      2         1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>docIdx</th>\n      <th>wordIdx</th>\n      <th>count</th>\n      <th>classIdx</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>4</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>3</td>\n      <td>10</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>4</td>\n      <td>4</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>5</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training data\n",
    "train_data = open('datasets/20news-bydate/matlab/train.data')\n",
    "df = pd.read_csv(train_data, delimiter=' ', names=['docIdx', 'wordIdx', 'count'])\n",
    "\n",
    "#Training label\n",
    "label = []\n",
    "train_label = open('datasets/20news-bydate/matlab/train.label')\n",
    "lines = train_label.readlines()\n",
    "for line in lines:\n",
    "    label.append(int(line.split()[0]))\n",
    "\n",
    "#Increase label length to match docIdx\n",
    "docIdx = df['docIdx'].values\n",
    "i = 0\n",
    "new_label = []\n",
    "for index in range(len(docIdx)-1):\n",
    "    new_label.append(label[i])\n",
    "    if docIdx[index] != docIdx[index+1]:\n",
    "        i += 1\n",
    "new_label.append(label[i]) #for-loop ignores last value\n",
    "\n",
    "#Add label column\n",
    "df['classIdx'] = new_label\n",
    "\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "wordIdx          1         2             3             4         5      \\\nclassIdx                                                                 \n1         7.855542e-05  0.000381  1.661627e-03  5.438638e-05  0.000495   \n2         4.722740e-04  0.000464  7.871103e-09  1.338166e-04  0.000110   \n3         1.023768e-04  0.000642  9.306135e-09  1.582136e-04  0.000195   \n4         6.907239e-05  0.000268  8.632969e-09  8.632969e-09  0.000086   \n5         5.833066e-05  0.000321  9.720157e-09  9.729877e-06  0.000010   \n6         2.772348e-04  0.001309  5.898487e-09  4.659864e-04  0.000088   \n7         1.285628e-08  0.000360  1.285628e-08  2.572542e-05  0.000026   \n8         6.881972e-05  0.000413  7.645786e-09  7.645786e-09  0.000099   \n9         1.173399e-04  0.000562  8.380825e-09  3.353168e-05  0.000034   \n10        8.034546e-06  0.000265  8.026520e-09  1.606107e-05  0.000008   \n11        6.337208e-06  0.000424  6.330877e-09  6.330877e-09  0.000006   \n12        2.394759e-04  0.000414  4.605218e-09  5.066200e-05  0.000272   \n13        2.503713e-05  0.000275  8.342928e-09  1.669420e-05  0.000042   \n14        8.720143e-05  0.000227  5.813041e-09  7.557535e-05  0.000116   \n15        2.816911e-04  0.000481  5.868441e-09  1.291116e-04  0.000070   \n16        4.588082e-09  0.000564  7.341390e-05  3.212116e-05  0.000064   \n17        9.865371e-05  0.000171  5.192027e-09  3.115735e-05  0.000057   \n18        3.683691e-05  0.000567  3.683323e-09  3.315359e-05  0.000007   \n19        4.923319e-09  0.000192  4.923319e-09  1.132413e-04  0.000084   \n20        7.364584e-09  0.000331  6.628862e-05  1.473653e-05  0.000169   \n\nwordIdx      6             7             8         9             10     ...  \\\nclassIdx                                                                ...   \n1         0.000248  3.625960e-05  6.048302e-06  0.000205  8.459224e-04  ...   \n2         0.000457  7.871890e-05  4.723449e-05  0.001354  2.362118e-05  ...   \n3         0.000316  1.862158e-05  1.862158e-05  0.001340  9.306135e-09  ...   \n4         0.000414  1.727457e-05  8.641602e-06  0.000414  8.632969e-09  ...   \n5         0.000457  9.729877e-06  9.720157e-09  0.000457  9.720157e-09  ...   \n6         0.000307  1.238741e-04  1.770136e-05  0.001398  5.898487e-09  ...   \n7         0.000411  1.285628e-08  1.285628e-08  0.000360  3.858170e-05  ...   \n8         0.000658  5.352815e-05  2.294500e-05  0.000138  7.645786e-09  ...   \n9         0.000696  2.515085e-05  8.389205e-06  0.000034  8.380825e-09  ...   \n10        0.002424  8.034546e-06  8.026520e-09  0.000016  8.026520e-09  ...   \n11        0.001323  1.266808e-05  5.065335e-05  0.000025  6.330877e-09  ...   \n12        0.000373  1.151350e-04  3.224113e-05  0.000511  4.605218e-09  ...   \n13        0.000250  4.172298e-05  1.669420e-05  0.000242  8.342928e-09  ...   \n14        0.000401  5.818854e-06  5.232318e-05  0.000145  5.813041e-09  ...   \n15        0.000599  1.349800e-04  5.282184e-05  0.000141  5.868441e-09  ...   \n16        0.000463  4.588082e-09  4.588082e-09  0.000078  2.569372e-04  ...   \n17        0.000550  1.038925e-05  4.154141e-05  0.000099  5.192027e-09  ...   \n18        0.000575  2.947026e-05  1.105034e-04  0.000037  3.687006e-06  ...   \n19        0.000743  4.928243e-06  8.370135e-05  0.000049  4.923319e-09  ...   \n20        0.000324  3.683028e-05  7.371948e-06  0.000140  3.683028e-05  ...   \n\nwordIdx          53966         53967         53968         53969  \\\nclassIdx                                                           \n1         6.042260e-09  6.042260e-09  6.042260e-09  6.042260e-09   \n2         7.871103e-09  7.871103e-09  7.871103e-09  7.871103e-09   \n3         9.306135e-09  9.306135e-09  9.306135e-09  9.306135e-09   \n4         8.632969e-09  8.632969e-09  8.632969e-09  8.632969e-09   \n5         9.720157e-09  9.720157e-09  9.720157e-09  9.720157e-09   \n6         5.898487e-09  5.898487e-09  5.898487e-09  5.898487e-09   \n7         1.285628e-08  1.285628e-08  1.285628e-08  1.285628e-08   \n8         7.645786e-09  7.645786e-09  7.645786e-09  7.645786e-09   \n9         8.380825e-09  8.380825e-09  8.380825e-09  8.380825e-09   \n10        8.026520e-09  8.026520e-09  8.026520e-09  8.026520e-09   \n11        6.330877e-09  6.330877e-09  6.330877e-09  6.330877e-09   \n12        4.605218e-09  4.605218e-09  4.605218e-09  4.605218e-09   \n13        8.342928e-09  8.342928e-09  8.342928e-09  8.342928e-09   \n14        5.813041e-09  5.813041e-09  5.813041e-09  5.813041e-09   \n15        5.868441e-09  5.868441e-09  5.868441e-09  5.868441e-09   \n16        4.588082e-09  4.588082e-09  4.588082e-09  4.588082e-09   \n17        5.192027e-09  5.192027e-09  5.192027e-09  5.192027e-09   \n18        3.683323e-09  3.683323e-09  3.683323e-09  3.683323e-09   \n19        4.923319e-09  4.923319e-09  4.923319e-09  4.923319e-09   \n20        7.371948e-06  7.371948e-06  1.473653e-05  7.371948e-06   \n\nwordIdx          53970         53971         53972         53973  \\\nclassIdx                                                           \n1         6.042260e-09  6.042260e-09  6.042260e-09  6.042260e-09   \n2         7.871103e-09  7.871103e-09  7.871103e-09  7.871103e-09   \n3         9.306135e-09  9.306135e-09  9.306135e-09  9.306135e-09   \n4         8.632969e-09  8.632969e-09  8.632969e-09  8.632969e-09   \n5         9.720157e-09  9.720157e-09  9.720157e-09  9.720157e-09   \n6         5.898487e-09  5.898487e-09  5.898487e-09  5.898487e-09   \n7         1.285628e-08  1.285628e-08  1.285628e-08  1.285628e-08   \n8         7.645786e-09  7.645786e-09  7.645786e-09  7.645786e-09   \n9         8.380825e-09  8.380825e-09  8.380825e-09  8.380825e-09   \n10        8.026520e-09  8.026520e-09  8.026520e-09  8.026520e-09   \n11        6.330877e-09  6.330877e-09  6.330877e-09  6.330877e-09   \n12        4.605218e-09  4.605218e-09  4.605218e-09  4.605218e-09   \n13        8.342928e-09  8.342928e-09  8.342928e-09  8.342928e-09   \n14        5.813041e-09  5.813041e-09  5.813041e-09  5.813041e-09   \n15        5.868441e-09  5.868441e-09  5.868441e-09  5.868441e-09   \n16        4.588082e-09  4.588082e-09  4.588082e-09  4.588082e-09   \n17        5.192027e-09  5.192027e-09  5.192027e-09  5.192027e-09   \n18        3.683323e-09  3.683323e-09  3.683323e-09  3.683323e-09   \n19        4.923319e-09  4.923319e-09  4.923319e-09  4.923319e-09   \n20        7.371948e-06  7.371948e-06  7.371948e-06  7.371948e-06   \n\nwordIdx          53974         53975  \nclassIdx                              \n1         6.042260e-09  6.042260e-09  \n2         7.871103e-09  7.871103e-09  \n3         9.306135e-09  9.306135e-09  \n4         8.632969e-09  8.632969e-09  \n5         9.720157e-09  9.720157e-09  \n6         5.898487e-09  5.898487e-09  \n7         1.285628e-08  1.285628e-08  \n8         7.645786e-09  7.645786e-09  \n9         8.380825e-09  8.380825e-09  \n10        8.026520e-09  8.026520e-09  \n11        6.330877e-09  6.330877e-09  \n12        4.605218e-09  4.605218e-09  \n13        8.342928e-09  8.342928e-09  \n14        5.813041e-09  5.813041e-09  \n15        5.868441e-09  5.868441e-09  \n16        4.588082e-09  4.588082e-09  \n17        5.192027e-09  5.192027e-09  \n18        3.683323e-09  3.683323e-09  \n19        4.923319e-09  4.923319e-09  \n20        7.371948e-06  7.371948e-06  \n\n[20 rows x 53975 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>wordIdx</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>...</th>\n      <th>53966</th>\n      <th>53967</th>\n      <th>53968</th>\n      <th>53969</th>\n      <th>53970</th>\n      <th>53971</th>\n      <th>53972</th>\n      <th>53973</th>\n      <th>53974</th>\n      <th>53975</th>\n    </tr>\n    <tr>\n      <th>classIdx</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>7.855542e-05</td>\n      <td>0.000381</td>\n      <td>1.661627e-03</td>\n      <td>5.438638e-05</td>\n      <td>0.000495</td>\n      <td>0.000248</td>\n      <td>3.625960e-05</td>\n      <td>6.048302e-06</td>\n      <td>0.000205</td>\n      <td>8.459224e-04</td>\n      <td>...</td>\n      <td>6.042260e-09</td>\n      <td>6.042260e-09</td>\n      <td>6.042260e-09</td>\n      <td>6.042260e-09</td>\n      <td>6.042260e-09</td>\n      <td>6.042260e-09</td>\n      <td>6.042260e-09</td>\n      <td>6.042260e-09</td>\n      <td>6.042260e-09</td>\n      <td>6.042260e-09</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4.722740e-04</td>\n      <td>0.000464</td>\n      <td>7.871103e-09</td>\n      <td>1.338166e-04</td>\n      <td>0.000110</td>\n      <td>0.000457</td>\n      <td>7.871890e-05</td>\n      <td>4.723449e-05</td>\n      <td>0.001354</td>\n      <td>2.362118e-05</td>\n      <td>...</td>\n      <td>7.871103e-09</td>\n      <td>7.871103e-09</td>\n      <td>7.871103e-09</td>\n      <td>7.871103e-09</td>\n      <td>7.871103e-09</td>\n      <td>7.871103e-09</td>\n      <td>7.871103e-09</td>\n      <td>7.871103e-09</td>\n      <td>7.871103e-09</td>\n      <td>7.871103e-09</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.023768e-04</td>\n      <td>0.000642</td>\n      <td>9.306135e-09</td>\n      <td>1.582136e-04</td>\n      <td>0.000195</td>\n      <td>0.000316</td>\n      <td>1.862158e-05</td>\n      <td>1.862158e-05</td>\n      <td>0.001340</td>\n      <td>9.306135e-09</td>\n      <td>...</td>\n      <td>9.306135e-09</td>\n      <td>9.306135e-09</td>\n      <td>9.306135e-09</td>\n      <td>9.306135e-09</td>\n      <td>9.306135e-09</td>\n      <td>9.306135e-09</td>\n      <td>9.306135e-09</td>\n      <td>9.306135e-09</td>\n      <td>9.306135e-09</td>\n      <td>9.306135e-09</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>6.907239e-05</td>\n      <td>0.000268</td>\n      <td>8.632969e-09</td>\n      <td>8.632969e-09</td>\n      <td>0.000086</td>\n      <td>0.000414</td>\n      <td>1.727457e-05</td>\n      <td>8.641602e-06</td>\n      <td>0.000414</td>\n      <td>8.632969e-09</td>\n      <td>...</td>\n      <td>8.632969e-09</td>\n      <td>8.632969e-09</td>\n      <td>8.632969e-09</td>\n      <td>8.632969e-09</td>\n      <td>8.632969e-09</td>\n      <td>8.632969e-09</td>\n      <td>8.632969e-09</td>\n      <td>8.632969e-09</td>\n      <td>8.632969e-09</td>\n      <td>8.632969e-09</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5.833066e-05</td>\n      <td>0.000321</td>\n      <td>9.720157e-09</td>\n      <td>9.729877e-06</td>\n      <td>0.000010</td>\n      <td>0.000457</td>\n      <td>9.729877e-06</td>\n      <td>9.720157e-09</td>\n      <td>0.000457</td>\n      <td>9.720157e-09</td>\n      <td>...</td>\n      <td>9.720157e-09</td>\n      <td>9.720157e-09</td>\n      <td>9.720157e-09</td>\n      <td>9.720157e-09</td>\n      <td>9.720157e-09</td>\n      <td>9.720157e-09</td>\n      <td>9.720157e-09</td>\n      <td>9.720157e-09</td>\n      <td>9.720157e-09</td>\n      <td>9.720157e-09</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>2.772348e-04</td>\n      <td>0.001309</td>\n      <td>5.898487e-09</td>\n      <td>4.659864e-04</td>\n      <td>0.000088</td>\n      <td>0.000307</td>\n      <td>1.238741e-04</td>\n      <td>1.770136e-05</td>\n      <td>0.001398</td>\n      <td>5.898487e-09</td>\n      <td>...</td>\n      <td>5.898487e-09</td>\n      <td>5.898487e-09</td>\n      <td>5.898487e-09</td>\n      <td>5.898487e-09</td>\n      <td>5.898487e-09</td>\n      <td>5.898487e-09</td>\n      <td>5.898487e-09</td>\n      <td>5.898487e-09</td>\n      <td>5.898487e-09</td>\n      <td>5.898487e-09</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1.285628e-08</td>\n      <td>0.000360</td>\n      <td>1.285628e-08</td>\n      <td>2.572542e-05</td>\n      <td>0.000026</td>\n      <td>0.000411</td>\n      <td>1.285628e-08</td>\n      <td>1.285628e-08</td>\n      <td>0.000360</td>\n      <td>3.858170e-05</td>\n      <td>...</td>\n      <td>1.285628e-08</td>\n      <td>1.285628e-08</td>\n      <td>1.285628e-08</td>\n      <td>1.285628e-08</td>\n      <td>1.285628e-08</td>\n      <td>1.285628e-08</td>\n      <td>1.285628e-08</td>\n      <td>1.285628e-08</td>\n      <td>1.285628e-08</td>\n      <td>1.285628e-08</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>6.881972e-05</td>\n      <td>0.000413</td>\n      <td>7.645786e-09</td>\n      <td>7.645786e-09</td>\n      <td>0.000099</td>\n      <td>0.000658</td>\n      <td>5.352815e-05</td>\n      <td>2.294500e-05</td>\n      <td>0.000138</td>\n      <td>7.645786e-09</td>\n      <td>...</td>\n      <td>7.645786e-09</td>\n      <td>7.645786e-09</td>\n      <td>7.645786e-09</td>\n      <td>7.645786e-09</td>\n      <td>7.645786e-09</td>\n      <td>7.645786e-09</td>\n      <td>7.645786e-09</td>\n      <td>7.645786e-09</td>\n      <td>7.645786e-09</td>\n      <td>7.645786e-09</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>1.173399e-04</td>\n      <td>0.000562</td>\n      <td>8.380825e-09</td>\n      <td>3.353168e-05</td>\n      <td>0.000034</td>\n      <td>0.000696</td>\n      <td>2.515085e-05</td>\n      <td>8.389205e-06</td>\n      <td>0.000034</td>\n      <td>8.380825e-09</td>\n      <td>...</td>\n      <td>8.380825e-09</td>\n      <td>8.380825e-09</td>\n      <td>8.380825e-09</td>\n      <td>8.380825e-09</td>\n      <td>8.380825e-09</td>\n      <td>8.380825e-09</td>\n      <td>8.380825e-09</td>\n      <td>8.380825e-09</td>\n      <td>8.380825e-09</td>\n      <td>8.380825e-09</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>8.034546e-06</td>\n      <td>0.000265</td>\n      <td>8.026520e-09</td>\n      <td>1.606107e-05</td>\n      <td>0.000008</td>\n      <td>0.002424</td>\n      <td>8.034546e-06</td>\n      <td>8.026520e-09</td>\n      <td>0.000016</td>\n      <td>8.026520e-09</td>\n      <td>...</td>\n      <td>8.026520e-09</td>\n      <td>8.026520e-09</td>\n      <td>8.026520e-09</td>\n      <td>8.026520e-09</td>\n      <td>8.026520e-09</td>\n      <td>8.026520e-09</td>\n      <td>8.026520e-09</td>\n      <td>8.026520e-09</td>\n      <td>8.026520e-09</td>\n      <td>8.026520e-09</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>6.337208e-06</td>\n      <td>0.000424</td>\n      <td>6.330877e-09</td>\n      <td>6.330877e-09</td>\n      <td>0.000006</td>\n      <td>0.001323</td>\n      <td>1.266808e-05</td>\n      <td>5.065335e-05</td>\n      <td>0.000025</td>\n      <td>6.330877e-09</td>\n      <td>...</td>\n      <td>6.330877e-09</td>\n      <td>6.330877e-09</td>\n      <td>6.330877e-09</td>\n      <td>6.330877e-09</td>\n      <td>6.330877e-09</td>\n      <td>6.330877e-09</td>\n      <td>6.330877e-09</td>\n      <td>6.330877e-09</td>\n      <td>6.330877e-09</td>\n      <td>6.330877e-09</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>2.394759e-04</td>\n      <td>0.000414</td>\n      <td>4.605218e-09</td>\n      <td>5.066200e-05</td>\n      <td>0.000272</td>\n      <td>0.000373</td>\n      <td>1.151350e-04</td>\n      <td>3.224113e-05</td>\n      <td>0.000511</td>\n      <td>4.605218e-09</td>\n      <td>...</td>\n      <td>4.605218e-09</td>\n      <td>4.605218e-09</td>\n      <td>4.605218e-09</td>\n      <td>4.605218e-09</td>\n      <td>4.605218e-09</td>\n      <td>4.605218e-09</td>\n      <td>4.605218e-09</td>\n      <td>4.605218e-09</td>\n      <td>4.605218e-09</td>\n      <td>4.605218e-09</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>2.503713e-05</td>\n      <td>0.000275</td>\n      <td>8.342928e-09</td>\n      <td>1.669420e-05</td>\n      <td>0.000042</td>\n      <td>0.000250</td>\n      <td>4.172298e-05</td>\n      <td>1.669420e-05</td>\n      <td>0.000242</td>\n      <td>8.342928e-09</td>\n      <td>...</td>\n      <td>8.342928e-09</td>\n      <td>8.342928e-09</td>\n      <td>8.342928e-09</td>\n      <td>8.342928e-09</td>\n      <td>8.342928e-09</td>\n      <td>8.342928e-09</td>\n      <td>8.342928e-09</td>\n      <td>8.342928e-09</td>\n      <td>8.342928e-09</td>\n      <td>8.342928e-09</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>8.720143e-05</td>\n      <td>0.000227</td>\n      <td>5.813041e-09</td>\n      <td>7.557535e-05</td>\n      <td>0.000116</td>\n      <td>0.000401</td>\n      <td>5.818854e-06</td>\n      <td>5.232318e-05</td>\n      <td>0.000145</td>\n      <td>5.813041e-09</td>\n      <td>...</td>\n      <td>5.813041e-09</td>\n      <td>5.813041e-09</td>\n      <td>5.813041e-09</td>\n      <td>5.813041e-09</td>\n      <td>5.813041e-09</td>\n      <td>5.813041e-09</td>\n      <td>5.813041e-09</td>\n      <td>5.813041e-09</td>\n      <td>5.813041e-09</td>\n      <td>5.813041e-09</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>2.816911e-04</td>\n      <td>0.000481</td>\n      <td>5.868441e-09</td>\n      <td>1.291116e-04</td>\n      <td>0.000070</td>\n      <td>0.000599</td>\n      <td>1.349800e-04</td>\n      <td>5.282184e-05</td>\n      <td>0.000141</td>\n      <td>5.868441e-09</td>\n      <td>...</td>\n      <td>5.868441e-09</td>\n      <td>5.868441e-09</td>\n      <td>5.868441e-09</td>\n      <td>5.868441e-09</td>\n      <td>5.868441e-09</td>\n      <td>5.868441e-09</td>\n      <td>5.868441e-09</td>\n      <td>5.868441e-09</td>\n      <td>5.868441e-09</td>\n      <td>5.868441e-09</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>4.588082e-09</td>\n      <td>0.000564</td>\n      <td>7.341390e-05</td>\n      <td>3.212116e-05</td>\n      <td>0.000064</td>\n      <td>0.000463</td>\n      <td>4.588082e-09</td>\n      <td>4.588082e-09</td>\n      <td>0.000078</td>\n      <td>2.569372e-04</td>\n      <td>...</td>\n      <td>4.588082e-09</td>\n      <td>4.588082e-09</td>\n      <td>4.588082e-09</td>\n      <td>4.588082e-09</td>\n      <td>4.588082e-09</td>\n      <td>4.588082e-09</td>\n      <td>4.588082e-09</td>\n      <td>4.588082e-09</td>\n      <td>4.588082e-09</td>\n      <td>4.588082e-09</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>9.865371e-05</td>\n      <td>0.000171</td>\n      <td>5.192027e-09</td>\n      <td>3.115735e-05</td>\n      <td>0.000057</td>\n      <td>0.000550</td>\n      <td>1.038925e-05</td>\n      <td>4.154141e-05</td>\n      <td>0.000099</td>\n      <td>5.192027e-09</td>\n      <td>...</td>\n      <td>5.192027e-09</td>\n      <td>5.192027e-09</td>\n      <td>5.192027e-09</td>\n      <td>5.192027e-09</td>\n      <td>5.192027e-09</td>\n      <td>5.192027e-09</td>\n      <td>5.192027e-09</td>\n      <td>5.192027e-09</td>\n      <td>5.192027e-09</td>\n      <td>5.192027e-09</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>3.683691e-05</td>\n      <td>0.000567</td>\n      <td>3.683323e-09</td>\n      <td>3.315359e-05</td>\n      <td>0.000007</td>\n      <td>0.000575</td>\n      <td>2.947026e-05</td>\n      <td>1.105034e-04</td>\n      <td>0.000037</td>\n      <td>3.687006e-06</td>\n      <td>...</td>\n      <td>3.683323e-09</td>\n      <td>3.683323e-09</td>\n      <td>3.683323e-09</td>\n      <td>3.683323e-09</td>\n      <td>3.683323e-09</td>\n      <td>3.683323e-09</td>\n      <td>3.683323e-09</td>\n      <td>3.683323e-09</td>\n      <td>3.683323e-09</td>\n      <td>3.683323e-09</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>4.923319e-09</td>\n      <td>0.000192</td>\n      <td>4.923319e-09</td>\n      <td>1.132413e-04</td>\n      <td>0.000084</td>\n      <td>0.000743</td>\n      <td>4.928243e-06</td>\n      <td>8.370135e-05</td>\n      <td>0.000049</td>\n      <td>4.923319e-09</td>\n      <td>...</td>\n      <td>4.923319e-09</td>\n      <td>4.923319e-09</td>\n      <td>4.923319e-09</td>\n      <td>4.923319e-09</td>\n      <td>4.923319e-09</td>\n      <td>4.923319e-09</td>\n      <td>4.923319e-09</td>\n      <td>4.923319e-09</td>\n      <td>4.923319e-09</td>\n      <td>4.923319e-09</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>7.364584e-09</td>\n      <td>0.000331</td>\n      <td>6.628862e-05</td>\n      <td>1.473653e-05</td>\n      <td>0.000169</td>\n      <td>0.000324</td>\n      <td>3.683028e-05</td>\n      <td>7.371948e-06</td>\n      <td>0.000140</td>\n      <td>3.683028e-05</td>\n      <td>...</td>\n      <td>7.371948e-06</td>\n      <td>7.371948e-06</td>\n      <td>1.473653e-05</td>\n      <td>7.371948e-06</td>\n      <td>7.371948e-06</td>\n      <td>7.371948e-06</td>\n      <td>7.371948e-06</td>\n      <td>7.371948e-06</td>\n      <td>7.371948e-06</td>\n      <td>7.371948e-06</td>\n    </tr>\n  </tbody>\n</table>\n<p>20 rows Ã— 53975 columns</p>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Alpha value for smoothing\n",
    "a = 0.001\n",
    "\n",
    "#Calculate probability of each word based on class\n",
    "pb_ij = df.groupby(['classIdx','wordIdx'])\n",
    "pb_j = df.groupby(['classIdx'])\n",
    "Pr =  (pb_ij['count'].sum() + a) / (pb_j['count'].sum() + 16689)\n",
    "\n",
    "#Unstack series\n",
    "Pr = Pr.unstack()\n",
    "\n",
    "#Replace NaN or columns with 0 as word count with a/(count+|V|+1)\n",
    "for c in range(1,21):\n",
    "    Pr.loc[c,:] = Pr.loc[c,:].fillna(a/(pb_j['count'].sum()[c] + 16689))\n",
    "\n",
    "#Convert to dictionary for greater speed\n",
    "Pr_dict = Pr.to_dict()\n",
    "\n",
    "Pr\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "#Common stop words from online\n",
    "stop_words = [\n",
    "\"a\", \"about\", \"above\", \"across\", \"after\", \"afterwards\",\n",
    "\"again\", \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\",\n",
    "\"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"another\", \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\", \"as\", \"at\", \"be\", \"became\", \"because\", \"become\",\"becomes\", \"becoming\", \"been\", \"before\", \"behind\", \"being\", \"beside\", \"besides\", \"between\", \"beyond\", \"both\", \"but\", \"by\",\"can\", \"cannot\", \"cant\", \"could\", \"couldnt\", \"de\", \"describe\", \"do\", \"done\", \"each\", \"eg\", \"either\", \"else\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\", \"everything\", \"everywhere\", \"except\", \"few\", \"find\",\"for\",\"found\", \"four\", \"from\", \"further\", \"get\", \"give\", \"go\", \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"however\", \"i\", \"ie\", \"if\", \"in\", \"indeed\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\", \"meanwhile\", \"might\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\", \"never\", \"nevertheless\", \"next\",\"no\", \"nobody\", \"none\", \"noone\", \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\", \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"part\",\"perhaps\", \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"she\", \"should\",\"since\", \"sincere\",\"so\", \"some\", \"somehow\", \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\", \"take\",\"than\", \"that\", \"the\", \"their\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\",\n",
    "\"this\", \"those\", \"though\", \"through\", \"throughout\",\n",
    "\"thru\", \"thus\", \"to\", \"together\", \"too\", \"toward\", \"towards\",\n",
    "\"under\", \"until\", \"up\", \"upon\", \"us\",\n",
    "\"very\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\",\n",
    "\"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\",\n",
    "\"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\",\n",
    "\"who\", \"whoever\", \"whom\", \"whose\", \"why\", \"will\", \"with\",\n",
    "\"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\"\n",
    "]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "   index       word\n0      1    archive\n1      2       name\n2      3    atheism\n3      4  resources\n4      5        alt",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>word</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>archive</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>name</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>atheism</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>resources</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>alt</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = open('datasets/20news-bydate/vocabulary.txt')\n",
    "vocab_df = pd.read_csv(vocab, names = ['word'])\n",
    "vocab_df = vocab_df.reset_index()\n",
    "vocab_df['index'] = vocab_df['index'].apply(lambda x: x+1)\n",
    "vocab_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "#Index of all words\n",
    "tot_list = set(vocab_df['index'])\n",
    "\n",
    "#Index of good words\n",
    "vocab_df = vocab_df[~vocab_df['word'].isin(stop_words)]\n",
    "good_list = vocab_df['index'].tolist()\n",
    "good_list = set(good_list)\n",
    "\n",
    "#Index of stop words\n",
    "bad_list = tot_list - good_list\n",
    "\n",
    "#Set all stop words to 0\n",
    "for bad in bad_list:\n",
    "    for j in range(1,21):\n",
    "        Pr_dict[j][bad] = a/(pb_j['count'].sum()[j] + 16689)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "#Calculate IDF\n",
    "tot = len(df['docIdx'].unique())\n",
    "pb_ij = df.groupby(['wordIdx'])\n",
    "IDF = np.log(tot/pb_ij['docIdx'].count())\n",
    "IDF_dict = IDF.to_dict()\n",
    "def MNB(df, smooth = False, IDF = False):\n",
    "    '''\n",
    "    Multinomial Naive Bayes classifier\n",
    "    :param df [Pandas Dataframe]: Dataframe of data\n",
    "    :param smooth [bool]: Apply Smoothing if True\n",
    "    :param IDF [bool]: Apply Inverse Document Frequency if True\n",
    "    :return predict [list]: Predicted class ID\n",
    "    '''\n",
    "    #Using dictionaries for greater speed\n",
    "    df_dict = df.to_dict()\n",
    "    new_dict = {}\n",
    "    prediction = []\n",
    "\n",
    "    #new_dict = {docIdx : {wordIdx: count},....}\n",
    "    for idx in range(len(df_dict['docIdx'])):\n",
    "        docIdx = df_dict['docIdx'][idx]\n",
    "        wordIdx = df_dict['wordIdx'][idx]\n",
    "        count = df_dict['count'][idx]\n",
    "        try:\n",
    "            new_dict[docIdx][wordIdx] = count\n",
    "        except:\n",
    "            new_dict[df_dict['docIdx'][idx]] = {}\n",
    "            new_dict[docIdx][wordIdx] = count\n",
    "\n",
    "    #Calculating the scores for each doc\n",
    "    for docIdx in range(1, len(new_dict)+1):\n",
    "        score_dict = {}\n",
    "        #Creating a probability row for each class\n",
    "        for classIdx in range(1,21):\n",
    "            score_dict[classIdx] = 1\n",
    "            #For each word:\n",
    "            for wordIdx in new_dict[docIdx]:\n",
    "                #Check for frequency smoothing\n",
    "                #log(1+f)*log(Pr(i|j))\n",
    "                if smooth:\n",
    "                    try:\n",
    "                        probability=Pr_dict[wordIdx][classIdx]\n",
    "                        power = np.log(1+ new_dict[docIdx][wordIdx])\n",
    "                        #Check for IDF\n",
    "                        if IDF:\n",
    "                            score_dict[classIdx]+=(\n",
    "                               power*np.log(\n",
    "                               probability*IDF_dict[wordIdx]))\n",
    "                        else:\n",
    "                            score_dict[classIdx]+=power*np.log(\n",
    "                                                   probability)\n",
    "                    except:\n",
    "                        #Missing V will have log(1+0)*log(a/16689)=0\n",
    "                        score_dict[classIdx] += 0\n",
    "                #f*log(Pr(i|j))\n",
    "                else:\n",
    "                    try:\n",
    "                        probability = Pr_dict[wordIdx][classIdx]\n",
    "                        power = new_dict[docIdx][wordIdx]\n",
    "                        score_dict[classIdx]+=power*np.log(\n",
    "                                           probability)\n",
    "                        #Check for IDF\n",
    "                        if IDF:\n",
    "                            score_dict[classIdx]+= power*np.log(\n",
    "                                   probability*IDF_dict[wordIdx])\n",
    "                    except:\n",
    "                        #Missing V will have 0*log(a/16689) = 0\n",
    "                        score_dict[classIdx] += 0\n",
    "            #Multiply final with pi\n",
    "            score_dict[classIdx] +=  np.log(pi[classIdx])\n",
    "\n",
    "        #Get class with max probabilty for the given docIdx\n",
    "        max_score = max(score_dict, key=score_dict.get)\n",
    "        prediction.append(max_score)\n",
    "\n",
    "    return prediction"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regular Error:\t\t 3.4608217233117404 %\n",
      "Smooth Error:\t\t 1.8546454876209069 %\n",
      "IDF Error:\t\t 3.478569527021031 %\n",
      "Both Error:\t\t 1.8546454876209069 %\n"
     ]
    }
   ],
   "source": [
    "regular_predict = MNB(df, smooth=False, IDF=False)\n",
    "smooth_predict  = MNB(df, smooth=True, IDF=False)\n",
    "tfidf_predict   = MNB(df, smooth=False, IDF=True)\n",
    "all_predict     = MNB(df, smooth=True, IDF=True)#Get list of labels\n",
    "train_label = pd.read_csv('datasets/20news-bydate/matlab/train.label', names=['t'])\n",
    "train_label = train_label['t'].tolist()\n",
    "total = len(train_label)\n",
    "models = [regular_predict, smooth_predict,\n",
    "          tfidf_predict, all_predict]\n",
    "strings = ['Regular', 'Smooth', 'IDF', 'Both']\n",
    "\n",
    "for m,s in zip(models,strings):\n",
    "    val = 0\n",
    "    for i,j in zip(m, train_label):\n",
    "        if i != j:\n",
    "            val +=1\n",
    "        else:\n",
    "            pass\n",
    "    print(s,\"Error:\\t\\t\",val/total * 100, \"%\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\t 77.42838107928048 %\n"
     ]
    }
   ],
   "source": [
    "#Get test data\n",
    "test_data = open('datasets/20news-bydate/matlab/test.data')\n",
    "df = pd.read_csv(test_data, delimiter=' ', names=['docIdx', 'wordIdx', 'count'])\n",
    "\n",
    "#Get list of labels\n",
    "test_label = pd.read_csv('datasets/20news-bydate/matlab/test.label', names=['t'])\n",
    "test_label= test_label['t'].tolist()\n",
    "\n",
    "#MNB Calculation\n",
    "predict = MNB(df, smooth = True, IDF = False)\n",
    "\n",
    "total = len(test_label)\n",
    "val = 0\n",
    "for i,j in zip(predict, test_label):\n",
    "    if i == j:\n",
    "        val +=1\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "print(\"Accuracy:\\t\",val/total * 100, \"%\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}